{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TRANSFORMER_NOSTE_SILLABE.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVR7XSsWKvXh"
      },
      "source": [
        "# TRANSFORMER WITH GENERATED SYLLAB DATASET"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cBwNplTX4QR"
      },
      "source": [
        "# utils import\r\n",
        "\r\n",
        "import syllabification as syll\r\n",
        "import copy\r\n",
        "import tensorflow as tf\r\n",
        "import time\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJuLeZFsK3Cl"
      },
      "source": [
        "## Dataset exploitation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fb8lVqkGX9VU",
        "outputId": "e120eaac-fde9-4eac-ac94-53757971cb94"
      },
      "source": [
        "with open('divine_comedy.txt','r', encoding='ISO-8859-1') as f:\r\n",
        "  divine_comedy = f.read()\r\n",
        "\r\n",
        "print(divine_comedy[:250])\r\n",
        "print('\\n\\n[...]\\n\\n')\r\n",
        "print(divine_comedy[-280:])"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFERNO\n",
            "\n",
            "- Canto I\n",
            "\n",
            "Nel mezzo del cammin di nostra vita\n",
            "mi ritrovai per una selva oscura,\n",
            "ché la diritta via era smarrita.\n",
            "\n",
            "Ahi quanto a dir qual era è cosa dura\n",
            "esta selva selvaggia e aspra e forte\n",
            "che nel pensier rinova la paura!\n",
            "\n",
            "Tant' è amara che\n",
            "\n",
            "\n",
            "[...]\n",
            "\n",
            "\n",
            "vi s'indova;\n",
            "\n",
            "ma non eran da ciò le proprie penne:\n",
            "se non che la mia mente fu percossa\n",
            "da un fulgore in che sua voglia venne.\n",
            "\n",
            "A l'alta fantasia qui mancò possa;\n",
            "ma già volgeva il mio disio e 'l velle,\n",
            "sì come rota ch'igualmente è mossa,\n",
            "\n",
            "l'amor che move il sole e l'altre stelle.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsNJxDKo8lGY"
      },
      "source": [
        "import re\r\n",
        "\r\n",
        "divine_comedy = divine_comedy.replace(\"ä\", \"a\")\r\n",
        "divine_comedy = divine_comedy.replace(\"é\", \"è\")\r\n",
        "divine_comedy = divine_comedy.replace(\"ë\", \"è\")\r\n",
        "divine_comedy = divine_comedy.replace(\"Ë\", \"E\")\r\n",
        "divine_comedy = divine_comedy.replace(\"ï\", \"i\")\r\n",
        "divine_comedy = divine_comedy.replace(\"Ï\", \"I\")\r\n",
        "divine_comedy = divine_comedy.replace(\"ó\", \"ò\")\r\n",
        "divine_comedy = divine_comedy.replace(\"ö\", \"o\")\r\n",
        "divine_comedy = divine_comedy.replace(\"ü\", \"u\")\r\n",
        "divine_comedy = divine_comedy.replace(\"(\", \"-\")\r\n",
        "divine_comedy = divine_comedy.replace(\")\", \"-\")\r\n",
        "divine_comedy = divine_comedy.replace(\"[\", \"\")\r\n",
        "divine_comedy = divine_comedy.replace(\"]\", \"\")\r\n",
        "divine_comedy = re.sub(r'[0-9]+', '', divine_comedy)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jeBpQfJmkNDY"
      },
      "source": [
        "divine_comedy = re.sub(\r\n",
        "  f'\\n- Canto.*\\n\\n',\r\n",
        "  '',\r\n",
        "  divine_comedy\r\n",
        ")\r\n",
        "\r\n",
        "for name in ['INFERNO', 'PURGATORIO', 'PARADISO']:\r\n",
        "  divine_comedy = re.sub(\r\n",
        "    f'{name}',\r\n",
        "    \"\",\r\n",
        "    divine_comedy\r\n",
        "  )"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yfingfah8lDE",
        "outputId": "a539118f-d4cc-4f4c-c5f9-99399f1225bf"
      },
      "source": [
        "divine_comedy = re.sub('\\n\\n\\n','\\n\\n',divine_comedy)\r\n",
        "\r\n",
        "divine_comedy = syll._strip_punctuaction(divine_comedy.lower())\r\n",
        "divine_comedy = syll.syllabify_block(divine_comedy)\r\n",
        "divine_comedy_lines = divine_comedy.split('\\n')\r\n",
        "\r\n",
        "print(\"divine_comedy_lines:\")\r\n",
        "print(divine_comedy_lines[0:10])\r\n",
        "print()\r\n",
        "\r\n",
        "#Generate the dataset of lines\r\n",
        "lines_dataset = []\r\n",
        "for el in divine_comedy_lines:\r\n",
        "    el = el.strip()\r\n",
        "    el = el.split('#')[0:]\r\n",
        "    if el != ['']:\r\n",
        "       lines_dataset.append(el)\r\n",
        "    else:\r\n",
        "       lines_dataset.append(['=end_terzine='])\r\n",
        "\r\n",
        "print(\"lines_dataset:\")\r\n",
        "print(lines_dataset[:10])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "divine_comedy_lines:\n",
            "['', '#nel# mez#zo# del# cam#min# di# no#stra# vi#ta#', 'mi# ri#tro#vai# per# u#na# sel#va# o#scu#ra#', 'chè# la# di#rit#ta# via# e#ra# smar#ri#ta#', '', 'ahi# quan#to# a# dir# qual# e#ra# è# co#sa# du#ra#', 'e#sta# sel#va# sel#vag#gia# e# a#spra# e# for#te#', 'che# nel# pen#sier# ri#no#va# la# pau#ra#', '', 'tan#t# è# a#ma#ra# che# po#co# è# più# mor#te#']\n",
            "\n",
            "lines_dataset:\n",
            "[['=end_terzine='], ['', 'nel', ' mez', 'zo', ' del', ' cam', 'min', ' di', ' no', 'stra', ' vi', 'ta', ''], ['mi', ' ri', 'tro', 'vai', ' per', ' u', 'na', ' sel', 'va', ' o', 'scu', 'ra', ''], ['chè', ' la', ' di', 'rit', 'ta', ' via', ' e', 'ra', ' smar', 'ri', 'ta', ''], ['=end_terzine='], ['ahi', ' quan', 'to', ' a', ' dir', ' qual', ' e', 'ra', ' è', ' co', 'sa', ' du', 'ra', ''], ['e', 'sta', ' sel', 'va', ' sel', 'vag', 'gia', ' e', ' a', 'spra', ' e', ' for', 'te', ''], ['che', ' nel', ' pen', 'sier', ' ri', 'no', 'va', ' la', ' pau', 'ra', ''], ['=end_terzine='], ['tan', 't', ' è', ' a', 'ma', 'ra', ' che', ' po', 'co', ' è', ' più', ' mor', 'te', '']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8v-IfzqYMVp0",
        "outputId": "653e4c48-2e81-4c64-dbc0-e20cf5db5151"
      },
      "source": [
        "# Remove extra spaces\r\n",
        "lines_dataset = copy.deepcopy(lines_dataset)\r\n",
        "\r\n",
        "def extract_spaces(list):\r\n",
        "    test = []\r\n",
        "    for el in list:\r\n",
        "      if el[0]==' ':\r\n",
        "        test.append(' ')\r\n",
        "        test.append(str(el[1:]))\r\n",
        "      else:\r\n",
        "        test.append(el)\r\n",
        "    return test\r\n",
        "\r\n",
        "for i in range(len(lines_dataset)):\r\n",
        "  lines_dataset[i] = list(filter(lambda a: a != '', lines_dataset[i]))\r\n",
        "  lines_dataset[i] = extract_spaces(lines_dataset[i])\r\n",
        "\r\n",
        "for i in range(10):\r\n",
        "  print(lines_dataset[i])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['=end_terzine=']\n",
            "['nel', ' ', 'mez', 'zo', ' ', 'del', ' ', 'cam', 'min', ' ', 'di', ' ', 'no', 'stra', ' ', 'vi', 'ta']\n",
            "['mi', ' ', 'ri', 'tro', 'vai', ' ', 'per', ' ', 'u', 'na', ' ', 'sel', 'va', ' ', 'o', 'scu', 'ra']\n",
            "['chè', ' ', 'la', ' ', 'di', 'rit', 'ta', ' ', 'via', ' ', 'e', 'ra', ' ', 'smar', 'ri', 'ta']\n",
            "['=end_terzine=']\n",
            "['ahi', ' ', 'quan', 'to', ' ', 'a', ' ', 'dir', ' ', 'qual', ' ', 'e', 'ra', ' ', 'è', ' ', 'co', 'sa', ' ', 'du', 'ra']\n",
            "['e', 'sta', ' ', 'sel', 'va', ' ', 'sel', 'vag', 'gia', ' ', 'e', ' ', 'a', 'spra', ' ', 'e', ' ', 'for', 'te']\n",
            "['che', ' ', 'nel', ' ', 'pen', 'sier', ' ', 'ri', 'no', 'va', ' ', 'la', ' ', 'pau', 'ra']\n",
            "['=end_terzine=']\n",
            "['tan', 't', ' ', 'è', ' ', 'a', 'ma', 'ra', ' ', 'che', ' ', 'po', 'co', ' ', 'è', ' ', 'più', ' ', 'mor', 'te']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAW_UyiWM4K0"
      },
      "source": [
        "#Add placeholders\r\n",
        "syllab_dataset = []\r\n",
        "flag = 0\r\n",
        "\r\n",
        "lines_dataset.reverse()\r\n",
        "\r\n",
        "for el in lines_dataset:\r\n",
        "  if flag == 0:\r\n",
        "    syllab_dataset.append(el)\r\n",
        "    if el == ['=end_terzine=']:\r\n",
        "      flag = 1\r\n",
        "\r\n",
        "  elif flag == 1:\r\n",
        "    if el == ['=end_terzine=']:\r\n",
        "      flag = 2\r\n",
        "    else:\r\n",
        "      flag = 0\r\n",
        "      syllab_dataset.append(el)\r\n",
        "\r\n",
        "  elif flag == 2: \r\n",
        "     if el == ['=end_terzine=']:\r\n",
        "       flag = 2\r\n",
        "     else:\r\n",
        "       flag = 4\r\n",
        "\r\n",
        "  elif flag == 4:\r\n",
        "     flag = 0  \r\n",
        "\r\n",
        "lines_dataset.reverse()\r\n",
        "syllab_dataset.reverse()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhAfZa-3Neo0",
        "outputId": "4ccbd970-146f-4d3d-ceb7-1a1a200e283f"
      },
      "source": [
        "syllab_dataset = copy.deepcopy(syllab_dataset)\r\n",
        "\r\n",
        "for i in range(len(syllab_dataset)):\r\n",
        "  if len(syllab_dataset[i]) > 1:\r\n",
        "    syllab_dataset[i].insert(0, '-start-')\r\n",
        "    syllab_dataset[i].append('-end-')\r\n",
        "\r\n",
        "for i in range(10):\r\n",
        "  print(syllab_dataset[i])\r\n",
        "\r\n",
        "one_list_data = [item for sublist in syllab_dataset for item in sublist]\r\n",
        "one_list_tokenized = [item for sublist in syllab_dataset for item in sublist]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['=end_terzine=']\n",
            "['-start-', 'nel', ' ', 'mez', 'zo', ' ', 'del', ' ', 'cam', 'min', ' ', 'di', ' ', 'no', 'stra', ' ', 'vi', 'ta', '-end-']\n",
            "['-start-', 'mi', ' ', 'ri', 'tro', 'vai', ' ', 'per', ' ', 'u', 'na', ' ', 'sel', 'va', ' ', 'o', 'scu', 'ra', '-end-']\n",
            "['-start-', 'chè', ' ', 'la', ' ', 'di', 'rit', 'ta', ' ', 'via', ' ', 'e', 'ra', ' ', 'smar', 'ri', 'ta', '-end-']\n",
            "['=end_terzine=']\n",
            "['-start-', 'ahi', ' ', 'quan', 'to', ' ', 'a', ' ', 'dir', ' ', 'qual', ' ', 'e', 'ra', ' ', 'è', ' ', 'co', 'sa', ' ', 'du', 'ra', '-end-']\n",
            "['-start-', 'e', 'sta', ' ', 'sel', 'va', ' ', 'sel', 'vag', 'gia', ' ', 'e', ' ', 'a', 'spra', ' ', 'e', ' ', 'for', 'te', '-end-']\n",
            "['-start-', 'che', ' ', 'nel', ' ', 'pen', 'sier', ' ', 'ri', 'no', 'va', ' ', 'la', ' ', 'pau', 'ra', '-end-']\n",
            "['=end_terzine=']\n",
            "['-start-', 'tan', 't', ' ', 'è', ' ', 'a', 'ma', 'ra', ' ', 'che', ' ', 'po', 'co', ' ', 'è', ' ', 'più', ' ', 'mor', 'te', '-end-']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjIN5glaObeP",
        "outputId": "4a8c5132-98d1-44d8-ee1c-babd36872b6b"
      },
      "source": [
        "syll_set = set([ item for elem in syllab_dataset for item in elem])\r\n",
        "vocabulary_size = len(syll_set)\r\n",
        "print('nr. of different syllabes: ',vocabulary_size)\r\n",
        "print('syllabes set: ',syll_set)\r\n",
        "\r\n",
        "syl2int = {el:pos for pos,el in enumerate(syll_set)}\r\n",
        "print('syl2int: ',syl2int)\r\n",
        "\r\n",
        "int2syl = {pos:el for pos,el in enumerate(syll_set)}\r\n",
        "print('int2syl: ',int2syl)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nr. of different syllabes:  1921\n",
            "syllabes set:  {'bran', 'fischio', 'om', 'truo', 'mos', 'sgher', 'gras', 'sul', 'mì', 'in', 'scuo', 'som', 'zan', 'sf', 'grem', 'stiò', 'les', 'toschi', 'cot', 'fio', 'spia', 'gib', 'tav', 'piog', 'gnen', 'pit', 'cop', 'spuo', 'glau', 'nen', 'cil', 'for', 'tri', 'pluo', 'smon', ' le', 'd', 'scher', 'sug', 'mol', 'smi', 'stran', 'mab', 'do-', 'spro', 'rez', 'chad', 'gnon', 'mio', 'tier', 'sion', 'los', 'biche', 'lup', 'cle', 'pec', 'bue', 'viem', 'nè', 'id', 'fes', 'dì', 'pun', 'sgo', 'strat', 'vam', ' trat', 'stian', 'zaf', 'chu', 'iu', 'map', 'quod', 'dub', 'tum', 'ciel', 'lav', 'te', 'gie', 'fa', 'scir', 'rim', 'can', ' so', 'sal', 'quil', 'caschi', 'masche', 'dei', 'tut', 'chab', 'noi', 'sbri', 'puos', 'grot', 'naut', 'stul', 'puot', 'lul', 'prun', 'del', 'sfam', 'e', 'tren', 'pia', 'clis', 'ef', 'dug', 'rog', 'nua', 'sbi', 'ben', 'guar', 'bris', 'pien', 'deus', 'sier', 'man', 'git', 'zai', 'stis', 'pen', 'cru', 'ìs', 'duo', 'ta-', 'prim', 'pue', 'ochi', 'lur', 'tol', 'le', 'pot', 'strà', 'leg', 'len', 'vil', 'glior', 'cov', 'gòr', 'nap', 'ril', 'sfor', 'rof', 'tau', 'chor', 'sfac', 'mim', 'sias', 'schie', 'ad', 'fuochi', 'cal', 'mhai', 'dur', 'do', 'chel', 'spal', 'cla', 'ziar', 'gnin', 's', 'guz', 'nef', 'spol', 'prez', 'nur', 'cag', 'chin', 'zul', 'eu', 'tos', 'dom', 'lion', 'cez', 'sgra', 'via', 'ob', 'luc', 'ghes', 'gnum', 'teg', 'gor', 'g', 'bri', 'neg', 'pro', 'nias', 'nei', 'con', 'veg', 'vo', 'joi', 'veschi', 'tà', 'puol', 'cef', 'nache', 'tò', 'clo', 'num', 'tat', 'se-or', 'nuo', 'vez', 'gnuo', 'mar', 'spic', 'stei', 'scos', 'to-', 'stor', 'coz', 'scim', 'siam', 'rua', 'pic', 'nel', 'cer', 'dèch', 'res', ' fos', 'tr', 'chog', 'stre', 'strò', 'soc', 'gir', 'nas', 'muel', ' buo', 'ghien', 'sbe', 'cor', 'vha', 'èv', 'stri', 'r', 'sciom', 'bli', 'el', 'dichia', 'se-e', ' pol', 'chì', 'moz', 'strai', 'sim', 'bec', 'lail', 'richiu', 'nag', 'guaschi', 'dic', 'gom', 'mad', 'nòs', 'quel', 'bev', 'glie', 'laghi', 'lir', 'fiche', 'sup', 'ciar', 'àm', 'sgan', 'tio', 'sol', 'scar', ' vi', 'ca', 'bin', 'scis', 'tad', 'spre', 'spaz', 'scal', 'scem', 'tac', 'noschi', 'but', 'sos', 'lichin', 'maf', 'gliel', 'boschi', 'bru', 'guel', 'fan', 'cu', 'lun', 'dul', 'scet', 'rache', 'giche', 'gliò', 'chag', 'gia', 'tes', 'gòn', 'stal', 'rob', 'af', 'dache', 'sav', 'diz', ' vol', 'quar', 'vèn', 'ploi', 'deh', 'paz', 'cras', 'lien', 'son', 'veb', 'ghiac', 'pol', 'dot', 'può', 'fàc', 'piè', 'bis', 'vostr', 'meb', 'lac', 'fol', 'cion', 'dichi', 'scran', 'fei', 'vei', 'al', 'sciam', 'venha', 'vac', 'suol', 'rau', 'mai', 'cap', 'treb', 'f', 'sit', 'fog', 'mèc', 'sua', 'gan', 'rachel', 'pie', 'ret', 'èl', 'sfa', 'lal', 'spran', 'richie', 'cì', '-e', 'chan', 'vè', 'trop', 'vof', 'vir', 'gual', 'chas', 'preghie', 'spra', 'tua', 've', 'chun', 'scì', 'er', 'en', 'tès', 'viz', 'scen', 'stò', 'tal', 'sue', 'schiet', 'sgri', 'men', 'gliu', 'ùl', 'poche', 'mur', 'lèu', 'det', 'noz', 'dier', 'ciò', 'chi', 'loc', 'ram', 'scot', 'lha', 'sai', 'scun', 'stiz', 'ler', 'squar', 'giàs', 'un', 'raz', 'qui', 'stram', 'feb', ' mar', 'pier', 'let', 'plor', 'rif', 'gam', 'pan', 'vun', 'cian', 'oi', 'daz', 'guc', 'bre', 'ba', 'pris', 'av', 'nig', 'tim', 'toi', 'ser', 'ed', 'gnaz', ' ', 'pio', 'su', 'ver', 'pian', 'dab', 'te-', 'tar', 'des', 'chai', 'disf', 'noc', 'ruo', 'spi', 'tec', 'giar', 'ren', 'sun', 'mhan', 'nan', 'scic', 'chiar', 'bef', 'spon', 'cei', 'piac', 'viò', 'tab', 'liòs', 'spiar', 'm', 'sfio', 'feg', 'ròn', 'achil', 'smaghi', 'bun', 'vaghez', 'rug', 'dio', 'bot', 'bas', 'nai', 'daf', 'nuc', 'pei', 'stav', 'brul', 'quin', 'dam', 'règ', 'guò', 'ho', 'cit', 'stel', 'sban', 'rem', 'ec', 'ne', 'scial', 'mum', 'gau', 'prai', 'ta', 'gè', 'zar', 'poi', 'rech', 'còth', 'giam', 'gian', 'iat', 'chen', 'rev', 'est', 'dal', '-end-', 'lu', 'drit', 'sad', 'sap', 'go', 'nail', 'paschi', 'sud', 'sno', 'gri', 'fum', 'ghian', 'ghio', 'sei', 'svo', 'sman', 'qual', 'p', 'ri', 'ier', 'eghe', 'po', 'ol', 'voi', 'tàn', 'squi', ' ca', 'gran', 'gim', 'spes', 'vem', 'dà', 'fèl', 'stem', 'roc', 'gnun', 'sab', 'glia', 'pochi', 'mes', 'fis', 'smor', 'cral', 'laf', 'schiu', 'rec', 'ghin', 'zechiel', 'ì', 'boz', 'bal', 'giot', 'gnar', 'gh', 'ur', 'chiet', 'stas', 'dru', 'not', 'pel', 'san', 'nuò', 'guon', 'sper', 'mez', 'ei', 'stru', 'proc', 'spres', 'suon', 'plì', 'sil', 'pì', 'fò', 'em', 'lov', 'pa', 'sè', 'fier', 'gliet', 'pros', 'nal', 'tin', 'cun', ' buon', 'tap', 'nostr', 'pir', 'ghiot', 'schem', 'cher', 'rum', 'scom', 'tur', 'cep', 'ir', 'bròt', 'dra', 'stil', 'zòn', 'vaghi', 'brot', 'gluol', 'sver', 'rac', 'cur', 're', 'pren', 'fen', 'pàr', 'ap', 'stial', 'ref', 'sleghe', 'ciom', 'clau', 'sra', ' tro', 'val', 'sgor', 'nì', 'og', 'dial', 'sa', 'chav', 'than', 'scoc', 'fiu', 'meus', 'coi', 'goz', 'draghi', 'pru', 'sec', 'mam', 'scias', 'nion', 'giug', 'neb', 'cen', 'ric', 'ru', 'l', 'fiot', 'quat', 'muor', 'scan', 'trul', 'quiv', 'tue', 'viar', 'thai', 'schian', 'neun', 'ton', 'lia', 'ghir', 'net', 'cò', 'sco', 'co', 'lem', 'diam', 'crol', 'gnan', 'mèz', 'ghe', 'vos', 'maschio', 'a', 'pau', 'duom', 'sce', 'voil', 'vuo', 'mac', 'slac', 'spai', 'sfe', ' qua', 'dec', 'oh', 'vi-', 'tis', 'si-', 'gat', 'cia', 'dus', 'pos', 'liòn', 'deu', 'sfan', 'puz', 'lom', 'dob', 'tai', 'cuc', 'guan', 'clas', ' car', 'chal', 'strom', 'tù', 'gis', 'spit', 'ahi', 'spuos', 'pieghi', 'àr', 'crin', 'prin', 'v', 'puom', 'ceschi', 'dìt', 'an', 'pin', 'sia', 'tror', 'mischiar', 'ciac', 'cum', 'coc', 'bè', 'soz', 'lian', 'pi', 'chiu', 'lep', 'dran', 'spraz', ' sar', 'fi', 'cruc', 'cre', 'gnor', 'lei', 'vom', 'cuir', 'stam', 'stes', 'bor', 'sual', ' f', 'ciasche', 'zò', 'chion', 'rà', 'fos', 'trec', 'pem', 'sam', 'gni', 'fuor', 'maz', 'stai', 'cau', 'ses', 'so', 'lie', 'stop', 'niz', 'tui', 'guer', 'tle', 'brut', 'grep', 'fin', 'fiar', 'cric', 'gì', 'splen', 'dau', 'due', 'pò', 'smal', 'grap', 'fial', 'ghil', 'lèm', 'nà', 'dem', 'sciar', 'no-', 'chuom', 'gher', 'chom', '   non', 'tiem', 'qua', 'nul', 'piu', 'sopr', 'mu', 'sve', 'sev', 'buen', 'pes', 'chier', 'sur', 'glien', 'pri', 'bu', 'chei', 'mav', 'vul', 'vas', 'tuo', 'gel', 'sfat', 'chèi', 'rasche', 'sag', 'scre', 'lap', 'pev', 'stren', 'gil', 'uo', 'lin', 'cro', 'to', 'fet', 'ros', 'lasc', 'dor', 'spa', 'pli', 'ai', 'cain', 'ciol', 'sfo', 'scuf', 'tra', 'reschi', 'cim', 'cli', 'sciàn', 'peg', 'voche', 'sci', 'tet', 'fie', 'mat', 'ior', 'dog', 'preghi', 'gliam', 'suè', 'gio', 'que', ' fa', 'blian', 'grom', 'gias', 'por', 'sma', 'cha', 'zi', 'pop', 'cat', 'lut', 'bon', 'buon', 'riche', 'gad', 'sber', 'zac', 'chiasche', 'sep', 'gnes', 'se-be', 'stec', 'sof', 'non', 'na', 'biò', 'smar', 'lo', 'sin', 'zur', 'chat', 'sla', 'fac', 'pa-', 'dro', 'lor', 'scer', 'cho', 'op', 'vaghe', 'viv', 'puesc', 'piom', 'pièr', 'lum', 'scia', 'mian', 'pai', 'bì', '-lan', 'pal', 'ma', 'mel', 'sna', 'piaghe', 'grez', 'troi', 'sbra', 'vier', 'tho', 'n', 'be', 'vion', 'mi', 'nia', 'tòr', 'pil', 'chis', 'nor', 'guir', 'nòi', '    e', 'stà', 'èn', 'rischio', 'fat', 'glion', 'et', 'ten', 'fòn', 'gre', 'ac', 'strel', 'va', 'gui', 'mischia', 'ghel', 'stì', 'tuf', 'cai', 'vi', 'zian', 'bus', 'spri', 'qu', 'trom', 'fàt', 'fiam', 'brol', 'car', 'cui', 'lier', 'pur', 'plau', 'tris', 'giò', 'rei', 'stol', 'giù', 'fu-quan', 'dras', 'chia', 'squa', 'es', 'dar', 'sem', 'dien', 'nun', 'ròl', 'rav', 'bar', 'stuc', 'nim', 'trap', 'fuc', 'gliar', 'dat', 'reghi', 'ple', 'scor', 'dif', 'la', 'nes', 'dail', 'ne-', 'ghiar', 'das', 'rag', 'nien', 'vuol', 'mof', 'fiac', 'mo', 'tual', 'smen', 'buo', 'brar', 'lau', 'di', 'drap', 'bi', 'da', 'ster', 'de', ' mo', 'dischia', 'hai', 'las', 'ral', 'bir', 'ran', 'nio', 'chac', 'troc', 'ber', 'crà', 'richia', 'na-', 'gra', 'od', 'trò', 'drir', 'chiap', 'um', 'greg', 'lad', 'taf', 'mun', 'on', 'trà', 'red', 'fàs', 'chio', 'rol', 'che', 'gue', 'stet', 'rit', 'sel', 'stra', 'drò', 'sbar', ' fior', 'luz', 'chiun', 'bac', 'cie', 'scol', 'dis', 'suc', 'nual', 'chap', ' roc', 'den', 'grat', 'bren', 'rin', 'grin', 'sien', 'nol', 'blan', 'cul', 'spec', 'richi', 'smu', 'dischio', 'spet', 'boc', 'zial', 'brei', 'scel', 'lol', 'ar', 'chet', 'cel', 'tro', 'nec', 'più', 'vin', 'seb', 'guef', 'bò', 'puc', 'gnus', 'svi', 'nier', 'gnel', 'rì', 'gò', 'guit', 'ches', 'ghi', 'guì', 'sob', 'duol', 'scam', 'scoi', 'lho', 'gnai', 'spu', 'meg', 'tì', 'lop', 'scie', 'dò', 'sot', 'fau', ' gran', ' cen', 'doc', 'nar', 'rot', 'dè', 'dischiu', 'scio', 'peschi', 'gru', 'giac', 'scrit', 'sfin', 'guo', 'creu', 'zel', 'ner', 'sov', 'sdru', 'gior', 'no', 'se', 'fal', 'diche', 'tom', 'stic', 'stier', 'ras', 'col', 'chiò', 'zuf', 'glier', 'fon', 'trei', 'cet', 'gon', 'deg', 'svia', 'pra', 'scris', 'gol', 'viam', 'strin', 'van', 'exi', 'get', 'prir', 'stie', 'bur', 'sau', 'il', 'don', 'gur', 'suo', 'sis', ' mor', 'quis', 'quam', 'giun', 'ci', 'pior', 'dèi', 'resche', 'nau', 'ror', 'zia', 'paghe', 'ù', 'scaz', 'sir', 'stras', 'vit', 'z', 'han', 'echi', 'stòr', 'star', ' po', 'vag', 'sfer', 'mi-', 'lec', 'rischia', 'pet', 'var', '=end_terzine=', 'cad', 'fel', 'got', 'tei', 'strac', 'tu', 'tie', 'ciot', 'tiv', 'dre', ' due', 'fun', 'ful', 'spen', 'cin', 'pieghe', 'hon', 'quest', 'gno', 'laz', 'lim', 'mul', 'pat', 'tron', 'toc', 'zio', 'fian', 'sui', 'vio', 'jau', 'spo', 'sta', 'piez', 'nin', 'cham', 'nam', 'drà', 'snel', 'stro', 'sus', 'cir', 'az', 'raphèl', 'cob', 'sà', 'siche', 'scev', 'sciò', 'strar', 'tòm', 'ste', 'met', ' com', 'preghe', 'sca', 'lel', 'creb', 'mer', 'vis', 'nav', 'guet', 'teschio', 'tichi', 'fà', 'piche', 'gei', 'ciu', 'at', 'stien', 'stret', 'scion', 'spor', 'vec', 'ch', 'chian', 'vid', 'scian', 'suf', 'adha', 'fab', 'tez', 'deun', 'per', 'sog', ' re', 'nil', 'cio', 'mag', 'cheb', ' lu', 'or', 'rechi', 'os', 'nac', 'poc', 'cis', 'ò', 'spe', 'zion', 'char', 'vat', 'ghie', 'dachil', 'put', 'stù', 'mem', 'casche', 'of', 'du', 'graf', 'clug', 'sor', 'ghiò', 'pig', 'piov', 'luen', 'vòl', 'lic', 'die', 'sen', 'dram', 'pla', 'gres', 'rab', 'pub', 'iv', 'smo', 'sciu', 'run', 'desche', 'vu', 'luo', 'bul', 'scu', 'loi', 'grue', 'ples', 'dron', '-quan', 'tre', 'nob', 'sven', 'nis', 'pric', 'sù', 'pruo', 'gab', 'scrip', 'mha', 'stan', 'fioc', 'tiche', 'ag', 'rìb', ' mie', 'lag', 'fai', 'tuc', 'rai', 'niel', 'quor', 'lòn', 'pis', 'bui', 'piag', 'plu', 'ces', 'raschet', 'fia', 'bat', 'schi', 'sio', ' di', 'euf', 'gli', 'sde', 'michel', 'dai', 'fe', 'dac', 'ruf', 'buf', 'vial', 'seg', 'ot', 'vau', 'uom', 'gros', 'quache', 'von', 'còl', 'sfoghi', 'fien', 'tam', 'plan', 'scon', 'mosche', 'luoghi', 'test', 'cuo', 'ghet', 'vò', 'strut', 'gnie', 'din', 'lot', 'gnia', 'tiun', 'ciechi', 'sguar', 'min', 'scien', 'rel', 'ha', ' de', 'ler-co', 'plo', 'set', 'rid', 'fo', 'cam', 'bai', 'mon', 'dian', 'rè', 'deum', 'rò', 'glo', 'gu', 'fuo', 'lef', 'pon', 'scin', 'tiò', 'trot', 'sciut', 'i', 'giu', 'prei', 'lis', 'si', 'mir', 'richeg', 'tien', 'gob', 'ra', 'oc', 'scri', 'lhan', 'rer', 'gial', 'brug', 'quan', 'fioche', 'tè', 'tòn', 'dos', 'croi', 'sme', 'pòs', 'scheg', 'pe-', 'nic', 'fiò', 'roz', 'stroz', 'lof', 'brac', 'ohi', 'tic', 'ched', 'cos', 'tel', 'tian', 'mò', 'trac', 'gna', 'schiat', 'stuo', 'drai', 'maschi', 'cra', 'dop', 'cias', 'ris', 'mie', 'excel', 'ciòl', 'stèr', '-start-', 'mug', 'dies', 'nian', 'svel', 'pion', 'iep', 'gner', 'fue', 'sdeg', 'ni', 'b', 'stia', 'til', 'bel', 'di-e', 'cem', 'ciam', 'giz', 'nir', 'ul', 'gram', 'vet', 'nuan', 'trat', 'lam', 'as', 'stral', 'eghi', 'achi', 'guaz', 'muc', 'vien', 'lò', 'rup', 'meschi', 'stin', 'mic', 'cioc', 'riz', 'ves', 'guag', 'svian', 'stroc', 'là', 'driz', 'mischio', 'saf', 'mè', 'vab', 'ov', 'vap', 'tir', 'ger', 'stoi', 'sfar', 'pre', 'chias', 'glet', 'tan', 'nat', 'lit', 'rìs', 'cri', 'tru', 'bom', 'ghia', 'bù', 't', 'spau', 'òth', 'shai', 'drel', 'quen', 'sòn', 'ab', 'chez', 'sar', 'far', 'sub', 'gar', 'siòn', 'rar', 'no-in', 'ion', 'quo', 'dit', 'guic', 'glian', 'ven', 'par', 'pran', 'span', 'reche', 'prà', 'spun', 'scop', 'drì', 'sneb', 'mot', 'scov', 'prec', 'miche', 'trar', 'fug', 'sian', 'com', 'tràs', 'pres', 'vexil', 'piat', 'lip', 'nab', 'dan', 'ge', 'me', ' lie', 'vian', 'è', 'vìd', 'au', 'pap', 'viag', 'nie', 'gem', 'glies', 'pul', 'stric', 'tag', 'puo', 'vol', 'zie', '   tal', 'vel', 'spez', 'fib', 'guis', 'spir', 'moi', 'nieghi', 'nom', 'ciuf', 'nez', 'pac', 'reb', 'cè', 'iar', 'rut', 'guen', 'spin', 'dil', 'lì', 'cred', 'dez', 'bo', 'cut', 'deb', 'guiz', 'sop', 'gnas', 'ges', 'bron', 'gal', 'lez', 'nha', 'der', 'uhi', 'ie', 'tas', 'gi', 'sat', 'mìs', 'cac', 'rui', ' tuo', 'sgom', 'tor', 'sga', 'bol', 'nhan', 'poz', 'vagheg', ' suo', 'stiam', 'bet', ' bi', ' fi', 'mei', 'liev', 'lat', 'lichi', 'grop', 'dua', 'gai', 'ughi', 'am', 'strug', 'tran', 'tha', 'piz', 'gien', 'lio', 'pron', 'strav', 'io', 'tras', 'o', 'nès', 'spie', 'rir', 'stichi', 'cèr', 'gron', 'tun', 'trai', 'nem', 'sti', 'ro', 'gnu', 'bug', 'sac', 'dol', 'dap', 'fas', 'spieghi', 'mil', 'sba', 'blio', 'ruc', 'ter', 'sle', ' cor', 'fior', 'blia', 'mas', 'is', 'sden', 'plar', ' rei', 'gen', 'li', 'mor', 'sie', 'cien', 'sod', 'vai-e', 'nò', 'tret', 'nu', 'clès', 'leb', 'cuor', 'stio', 'bro', 'ciul', 'sas', 'cliò', 'rom', 'schia', 'niam', 'tuas', 'diè', 'zien', 'streg', 'za', 'top', 'guil', 'stig', 'squil', 'gne', 'nev', 'sto', 'sten', 'scab', 'luom', 'uc', 'vho', 'diur', 'zuc', 'pas', 'deschi', 'gag', 'fec', 'egyp', 'càn', 'tia', 'svuol', 'broc', 'ron', 'eb', 'lè', 'zo', 'u', 'ti', 'vor', 'sez', 'giochi', 'cheg', 'còb', 'trui', 'pr', 'fom', 'dri', 'schic', 'lab', 'trag', 'peschie', 'pom', 'pachi', 'mal', 'bei', 'fui', 'vì', 'rap', 'raf', 'gier', 'siar', 'dui', 'stion', 'vai', 'ga', 'scac', 'fu', 'chioc', 'smoz', 'cic', 'fig', 'lai', 'tif', 'bra', 'sic', 'caf', 'bàl', 'sdet', 'sciol', 'chè', 'già', 'bèl', 'fit', 'buc', 'im', 'spar', 'clu', 'lhai', ' ma', 'ce', 'stui', 'bil', 'fil', 'scioc', 'gro', 'quer', 'chie', 'fam', 'ban', 'lan', 'stè', 'cas', 'ache', 'pe', 'fè', 'fem', 'rig', 'cù', 'gnem', 'eschi', 'lil', 'lon', 'sha', 'vie', 'lui', 'spel', 'tem', 'dof', 'stu', 'muo', 'dia', 'muf', 'ciap', 'mia', 'gua', 'reg', 'vogl', 'tuon', 'prì', 'lus', 'gaz', 'pog', 'tram', 'tiam', 'dov', 'bab', 'liche', 'paghi', 'sò', 'sgiun', 'òs', 'biz', 'ghez', 'gin', 'lar', 'lig', 'piar', 'sì', 'spiac', 'sleghi', 'pu', 'zon', 'gas', 'dir', 'làche', 'dun', 'fer', 'gion', 'ze', 'goc', 'dim', 'sum', 'fur', 'tus', 'dut', 'glio', 'ia', 'mus', 'fic', 'scro', 'rat', 'pè', 'làn', 'brez', 'nìt', 'c'}\n",
            "syl2int:  {'bran': 0, 'fischio': 1, 'om': 2, 'truo': 3, 'mos': 4, 'sgher': 5, 'gras': 6, 'sul': 7, 'mì': 8, 'in': 9, 'scuo': 10, 'som': 11, 'zan': 12, 'sf': 13, 'grem': 14, 'stiò': 15, 'les': 16, 'toschi': 17, 'cot': 18, 'fio': 19, 'spia': 20, 'gib': 21, 'tav': 22, 'piog': 23, 'gnen': 24, 'pit': 25, 'cop': 26, 'spuo': 27, 'glau': 28, 'nen': 29, 'cil': 30, 'for': 31, 'tri': 32, 'pluo': 33, 'smon': 34, ' le': 35, 'd': 36, 'scher': 37, 'sug': 38, 'mol': 39, 'smi': 40, 'stran': 41, 'mab': 42, 'do-': 43, 'spro': 44, 'rez': 45, 'chad': 46, 'gnon': 47, 'mio': 48, 'tier': 49, 'sion': 50, 'los': 51, 'biche': 52, 'lup': 53, 'cle': 54, 'pec': 55, 'bue': 56, 'viem': 57, 'nè': 58, 'id': 59, 'fes': 60, 'dì': 61, 'pun': 62, 'sgo': 63, 'strat': 64, 'vam': 65, ' trat': 66, 'stian': 67, 'zaf': 68, 'chu': 69, 'iu': 70, 'map': 71, 'quod': 72, 'dub': 73, 'tum': 74, 'ciel': 75, 'lav': 76, 'te': 77, 'gie': 78, 'fa': 79, 'scir': 80, 'rim': 81, 'can': 82, ' so': 83, 'sal': 84, 'quil': 85, 'caschi': 86, 'masche': 87, 'dei': 88, 'tut': 89, 'chab': 90, 'noi': 91, 'sbri': 92, 'puos': 93, 'grot': 94, 'naut': 95, 'stul': 96, 'puot': 97, 'lul': 98, 'prun': 99, 'del': 100, 'sfam': 101, 'e': 102, 'tren': 103, 'pia': 104, 'clis': 105, 'ef': 106, 'dug': 107, 'rog': 108, 'nua': 109, 'sbi': 110, 'ben': 111, 'guar': 112, 'bris': 113, 'pien': 114, 'deus': 115, 'sier': 116, 'man': 117, 'git': 118, 'zai': 119, 'stis': 120, 'pen': 121, 'cru': 122, 'ìs': 123, 'duo': 124, 'ta-': 125, 'prim': 126, 'pue': 127, 'ochi': 128, 'lur': 129, 'tol': 130, 'le': 131, 'pot': 132, 'strà': 133, 'leg': 134, 'len': 135, 'vil': 136, 'glior': 137, 'cov': 138, 'gòr': 139, 'nap': 140, 'ril': 141, 'sfor': 142, 'rof': 143, 'tau': 144, 'chor': 145, 'sfac': 146, 'mim': 147, 'sias': 148, 'schie': 149, 'ad': 150, 'fuochi': 151, 'cal': 152, 'mhai': 153, 'dur': 154, 'do': 155, 'chel': 156, 'spal': 157, 'cla': 158, 'ziar': 159, 'gnin': 160, 's': 161, 'guz': 162, 'nef': 163, 'spol': 164, 'prez': 165, 'nur': 166, 'cag': 167, 'chin': 168, 'zul': 169, 'eu': 170, 'tos': 171, 'dom': 172, 'lion': 173, 'cez': 174, 'sgra': 175, 'via': 176, 'ob': 177, 'luc': 178, 'ghes': 179, 'gnum': 180, 'teg': 181, 'gor': 182, 'g': 183, 'bri': 184, 'neg': 185, 'pro': 186, 'nias': 187, 'nei': 188, 'con': 189, 'veg': 190, 'vo': 191, 'joi': 192, 'veschi': 193, 'tà': 194, 'puol': 195, 'cef': 196, 'nache': 197, 'tò': 198, 'clo': 199, 'num': 200, 'tat': 201, 'se-or': 202, 'nuo': 203, 'vez': 204, 'gnuo': 205, 'mar': 206, 'spic': 207, 'stei': 208, 'scos': 209, 'to-': 210, 'stor': 211, 'coz': 212, 'scim': 213, 'siam': 214, 'rua': 215, 'pic': 216, 'nel': 217, 'cer': 218, 'dèch': 219, 'res': 220, ' fos': 221, 'tr': 222, 'chog': 223, 'stre': 224, 'strò': 225, 'soc': 226, 'gir': 227, 'nas': 228, 'muel': 229, ' buo': 230, 'ghien': 231, 'sbe': 232, 'cor': 233, 'vha': 234, 'èv': 235, 'stri': 236, 'r': 237, 'sciom': 238, 'bli': 239, 'el': 240, 'dichia': 241, 'se-e': 242, ' pol': 243, 'chì': 244, 'moz': 245, 'strai': 246, 'sim': 247, 'bec': 248, 'lail': 249, 'richiu': 250, 'nag': 251, 'guaschi': 252, 'dic': 253, 'gom': 254, 'mad': 255, 'nòs': 256, 'quel': 257, 'bev': 258, 'glie': 259, 'laghi': 260, 'lir': 261, 'fiche': 262, 'sup': 263, 'ciar': 264, 'àm': 265, 'sgan': 266, 'tio': 267, 'sol': 268, 'scar': 269, ' vi': 270, 'ca': 271, 'bin': 272, 'scis': 273, 'tad': 274, 'spre': 275, 'spaz': 276, 'scal': 277, 'scem': 278, 'tac': 279, 'noschi': 280, 'but': 281, 'sos': 282, 'lichin': 283, 'maf': 284, 'gliel': 285, 'boschi': 286, 'bru': 287, 'guel': 288, 'fan': 289, 'cu': 290, 'lun': 291, 'dul': 292, 'scet': 293, 'rache': 294, 'giche': 295, 'gliò': 296, 'chag': 297, 'gia': 298, 'tes': 299, 'gòn': 300, 'stal': 301, 'rob': 302, 'af': 303, 'dache': 304, 'sav': 305, 'diz': 306, ' vol': 307, 'quar': 308, 'vèn': 309, 'ploi': 310, 'deh': 311, 'paz': 312, 'cras': 313, 'lien': 314, 'son': 315, 'veb': 316, 'ghiac': 317, 'pol': 318, 'dot': 319, 'può': 320, 'fàc': 321, 'piè': 322, 'bis': 323, 'vostr': 324, 'meb': 325, 'lac': 326, 'fol': 327, 'cion': 328, 'dichi': 329, 'scran': 330, 'fei': 331, 'vei': 332, 'al': 333, 'sciam': 334, 'venha': 335, 'vac': 336, 'suol': 337, 'rau': 338, 'mai': 339, 'cap': 340, 'treb': 341, 'f': 342, 'sit': 343, 'fog': 344, 'mèc': 345, 'sua': 346, 'gan': 347, 'rachel': 348, 'pie': 349, 'ret': 350, 'èl': 351, 'sfa': 352, 'lal': 353, 'spran': 354, 'richie': 355, 'cì': 356, '-e': 357, 'chan': 358, 'vè': 359, 'trop': 360, 'vof': 361, 'vir': 362, 'gual': 363, 'chas': 364, 'preghie': 365, 'spra': 366, 'tua': 367, 've': 368, 'chun': 369, 'scì': 370, 'er': 371, 'en': 372, 'tès': 373, 'viz': 374, 'scen': 375, 'stò': 376, 'tal': 377, 'sue': 378, 'schiet': 379, 'sgri': 380, 'men': 381, 'gliu': 382, 'ùl': 383, 'poche': 384, 'mur': 385, 'lèu': 386, 'det': 387, 'noz': 388, 'dier': 389, 'ciò': 390, 'chi': 391, 'loc': 392, 'ram': 393, 'scot': 394, 'lha': 395, 'sai': 396, 'scun': 397, 'stiz': 398, 'ler': 399, 'squar': 400, 'giàs': 401, 'un': 402, 'raz': 403, 'qui': 404, 'stram': 405, 'feb': 406, ' mar': 407, 'pier': 408, 'let': 409, 'plor': 410, 'rif': 411, 'gam': 412, 'pan': 413, 'vun': 414, 'cian': 415, 'oi': 416, 'daz': 417, 'guc': 418, 'bre': 419, 'ba': 420, 'pris': 421, 'av': 422, 'nig': 423, 'tim': 424, 'toi': 425, 'ser': 426, 'ed': 427, 'gnaz': 428, ' ': 429, 'pio': 430, 'su': 431, 'ver': 432, 'pian': 433, 'dab': 434, 'te-': 435, 'tar': 436, 'des': 437, 'chai': 438, 'disf': 439, 'noc': 440, 'ruo': 441, 'spi': 442, 'tec': 443, 'giar': 444, 'ren': 445, 'sun': 446, 'mhan': 447, 'nan': 448, 'scic': 449, 'chiar': 450, 'bef': 451, 'spon': 452, 'cei': 453, 'piac': 454, 'viò': 455, 'tab': 456, 'liòs': 457, 'spiar': 458, 'm': 459, 'sfio': 460, 'feg': 461, 'ròn': 462, 'achil': 463, 'smaghi': 464, 'bun': 465, 'vaghez': 466, 'rug': 467, 'dio': 468, 'bot': 469, 'bas': 470, 'nai': 471, 'daf': 472, 'nuc': 473, 'pei': 474, 'stav': 475, 'brul': 476, 'quin': 477, 'dam': 478, 'règ': 479, 'guò': 480, 'ho': 481, 'cit': 482, 'stel': 483, 'sban': 484, 'rem': 485, 'ec': 486, 'ne': 487, 'scial': 488, 'mum': 489, 'gau': 490, 'prai': 491, 'ta': 492, 'gè': 493, 'zar': 494, 'poi': 495, 'rech': 496, 'còth': 497, 'giam': 498, 'gian': 499, 'iat': 500, 'chen': 501, 'rev': 502, 'est': 503, 'dal': 504, '-end-': 505, 'lu': 506, 'drit': 507, 'sad': 508, 'sap': 509, 'go': 510, 'nail': 511, 'paschi': 512, 'sud': 513, 'sno': 514, 'gri': 515, 'fum': 516, 'ghian': 517, 'ghio': 518, 'sei': 519, 'svo': 520, 'sman': 521, 'qual': 522, 'p': 523, 'ri': 524, 'ier': 525, 'eghe': 526, 'po': 527, 'ol': 528, 'voi': 529, 'tàn': 530, 'squi': 531, ' ca': 532, 'gran': 533, 'gim': 534, 'spes': 535, 'vem': 536, 'dà': 537, 'fèl': 538, 'stem': 539, 'roc': 540, 'gnun': 541, 'sab': 542, 'glia': 543, 'pochi': 544, 'mes': 545, 'fis': 546, 'smor': 547, 'cral': 548, 'laf': 549, 'schiu': 550, 'rec': 551, 'ghin': 552, 'zechiel': 553, 'ì': 554, 'boz': 555, 'bal': 556, 'giot': 557, 'gnar': 558, 'gh': 559, 'ur': 560, 'chiet': 561, 'stas': 562, 'dru': 563, 'not': 564, 'pel': 565, 'san': 566, 'nuò': 567, 'guon': 568, 'sper': 569, 'mez': 570, 'ei': 571, 'stru': 572, 'proc': 573, 'spres': 574, 'suon': 575, 'plì': 576, 'sil': 577, 'pì': 578, 'fò': 579, 'em': 580, 'lov': 581, 'pa': 582, 'sè': 583, 'fier': 584, 'gliet': 585, 'pros': 586, 'nal': 587, 'tin': 588, 'cun': 589, ' buon': 590, 'tap': 591, 'nostr': 592, 'pir': 593, 'ghiot': 594, 'schem': 595, 'cher': 596, 'rum': 597, 'scom': 598, 'tur': 599, 'cep': 600, 'ir': 601, 'bròt': 602, 'dra': 603, 'stil': 604, 'zòn': 605, 'vaghi': 606, 'brot': 607, 'gluol': 608, 'sver': 609, 'rac': 610, 'cur': 611, 're': 612, 'pren': 613, 'fen': 614, 'pàr': 615, 'ap': 616, 'stial': 617, 'ref': 618, 'sleghe': 619, 'ciom': 620, 'clau': 621, 'sra': 622, ' tro': 623, 'val': 624, 'sgor': 625, 'nì': 626, 'og': 627, 'dial': 628, 'sa': 629, 'chav': 630, 'than': 631, 'scoc': 632, 'fiu': 633, 'meus': 634, 'coi': 635, 'goz': 636, 'draghi': 637, 'pru': 638, 'sec': 639, 'mam': 640, 'scias': 641, 'nion': 642, 'giug': 643, 'neb': 644, 'cen': 645, 'ric': 646, 'ru': 647, 'l': 648, 'fiot': 649, 'quat': 650, 'muor': 651, 'scan': 652, 'trul': 653, 'quiv': 654, 'tue': 655, 'viar': 656, 'thai': 657, 'schian': 658, 'neun': 659, 'ton': 660, 'lia': 661, 'ghir': 662, 'net': 663, 'cò': 664, 'sco': 665, 'co': 666, 'lem': 667, 'diam': 668, 'crol': 669, 'gnan': 670, 'mèz': 671, 'ghe': 672, 'vos': 673, 'maschio': 674, 'a': 675, 'pau': 676, 'duom': 677, 'sce': 678, 'voil': 679, 'vuo': 680, 'mac': 681, 'slac': 682, 'spai': 683, 'sfe': 684, ' qua': 685, 'dec': 686, 'oh': 687, 'vi-': 688, 'tis': 689, 'si-': 690, 'gat': 691, 'cia': 692, 'dus': 693, 'pos': 694, 'liòn': 695, 'deu': 696, 'sfan': 697, 'puz': 698, 'lom': 699, 'dob': 700, 'tai': 701, 'cuc': 702, 'guan': 703, 'clas': 704, ' car': 705, 'chal': 706, 'strom': 707, 'tù': 708, 'gis': 709, 'spit': 710, 'ahi': 711, 'spuos': 712, 'pieghi': 713, 'àr': 714, 'crin': 715, 'prin': 716, 'v': 717, 'puom': 718, 'ceschi': 719, 'dìt': 720, 'an': 721, 'pin': 722, 'sia': 723, 'tror': 724, 'mischiar': 725, 'ciac': 726, 'cum': 727, 'coc': 728, 'bè': 729, 'soz': 730, 'lian': 731, 'pi': 732, 'chiu': 733, 'lep': 734, 'dran': 735, 'spraz': 736, ' sar': 737, 'fi': 738, 'cruc': 739, 'cre': 740, 'gnor': 741, 'lei': 742, 'vom': 743, 'cuir': 744, 'stam': 745, 'stes': 746, 'bor': 747, 'sual': 748, ' f': 749, 'ciasche': 750, 'zò': 751, 'chion': 752, 'rà': 753, 'fos': 754, 'trec': 755, 'pem': 756, 'sam': 757, 'gni': 758, 'fuor': 759, 'maz': 760, 'stai': 761, 'cau': 762, 'ses': 763, 'so': 764, 'lie': 765, 'stop': 766, 'niz': 767, 'tui': 768, 'guer': 769, 'tle': 770, 'brut': 771, 'grep': 772, 'fin': 773, 'fiar': 774, 'cric': 775, 'gì': 776, 'splen': 777, 'dau': 778, 'due': 779, 'pò': 780, 'smal': 781, 'grap': 782, 'fial': 783, 'ghil': 784, 'lèm': 785, 'nà': 786, 'dem': 787, 'sciar': 788, 'no-': 789, 'chuom': 790, 'gher': 791, 'chom': 792, '   non': 793, 'tiem': 794, 'qua': 795, 'nul': 796, 'piu': 797, 'sopr': 798, 'mu': 799, 'sve': 800, 'sev': 801, 'buen': 802, 'pes': 803, 'chier': 804, 'sur': 805, 'glien': 806, 'pri': 807, 'bu': 808, 'chei': 809, 'mav': 810, 'vul': 811, 'vas': 812, 'tuo': 813, 'gel': 814, 'sfat': 815, 'chèi': 816, 'rasche': 817, 'sag': 818, 'scre': 819, 'lap': 820, 'pev': 821, 'stren': 822, 'gil': 823, 'uo': 824, 'lin': 825, 'cro': 826, 'to': 827, 'fet': 828, 'ros': 829, 'lasc': 830, 'dor': 831, 'spa': 832, 'pli': 833, 'ai': 834, 'cain': 835, 'ciol': 836, 'sfo': 837, 'scuf': 838, 'tra': 839, 'reschi': 840, 'cim': 841, 'cli': 842, 'sciàn': 843, 'peg': 844, 'voche': 845, 'sci': 846, 'tet': 847, 'fie': 848, 'mat': 849, 'ior': 850, 'dog': 851, 'preghi': 852, 'gliam': 853, 'suè': 854, 'gio': 855, 'que': 856, ' fa': 857, 'blian': 858, 'grom': 859, 'gias': 860, 'por': 861, 'sma': 862, 'cha': 863, 'zi': 864, 'pop': 865, 'cat': 866, 'lut': 867, 'bon': 868, 'buon': 869, 'riche': 870, 'gad': 871, 'sber': 872, 'zac': 873, 'chiasche': 874, 'sep': 875, 'gnes': 876, 'se-be': 877, 'stec': 878, 'sof': 879, 'non': 880, 'na': 881, 'biò': 882, 'smar': 883, 'lo': 884, 'sin': 885, 'zur': 886, 'chat': 887, 'sla': 888, 'fac': 889, 'pa-': 890, 'dro': 891, 'lor': 892, 'scer': 893, 'cho': 894, 'op': 895, 'vaghe': 896, 'viv': 897, 'puesc': 898, 'piom': 899, 'pièr': 900, 'lum': 901, 'scia': 902, 'mian': 903, 'pai': 904, 'bì': 905, '-lan': 906, 'pal': 907, 'ma': 908, 'mel': 909, 'sna': 910, 'piaghe': 911, 'grez': 912, 'troi': 913, 'sbra': 914, 'vier': 915, 'tho': 916, 'n': 917, 'be': 918, 'vion': 919, 'mi': 920, 'nia': 921, 'tòr': 922, 'pil': 923, 'chis': 924, 'nor': 925, 'guir': 926, 'nòi': 927, '    e': 928, 'stà': 929, 'èn': 930, 'rischio': 931, 'fat': 932, 'glion': 933, 'et': 934, 'ten': 935, 'fòn': 936, 'gre': 937, 'ac': 938, 'strel': 939, 'va': 940, 'gui': 941, 'mischia': 942, 'ghel': 943, 'stì': 944, 'tuf': 945, 'cai': 946, 'vi': 947, 'zian': 948, 'bus': 949, 'spri': 950, 'qu': 951, 'trom': 952, 'fàt': 953, 'fiam': 954, 'brol': 955, 'car': 956, 'cui': 957, 'lier': 958, 'pur': 959, 'plau': 960, 'tris': 961, 'giò': 962, 'rei': 963, 'stol': 964, 'giù': 965, 'fu-quan': 966, 'dras': 967, 'chia': 968, 'squa': 969, 'es': 970, 'dar': 971, 'sem': 972, 'dien': 973, 'nun': 974, 'ròl': 975, 'rav': 976, 'bar': 977, 'stuc': 978, 'nim': 979, 'trap': 980, 'fuc': 981, 'gliar': 982, 'dat': 983, 'reghi': 984, 'ple': 985, 'scor': 986, 'dif': 987, 'la': 988, 'nes': 989, 'dail': 990, 'ne-': 991, 'ghiar': 992, 'das': 993, 'rag': 994, 'nien': 995, 'vuol': 996, 'mof': 997, 'fiac': 998, 'mo': 999, 'tual': 1000, 'smen': 1001, 'buo': 1002, 'brar': 1003, 'lau': 1004, 'di': 1005, 'drap': 1006, 'bi': 1007, 'da': 1008, 'ster': 1009, 'de': 1010, ' mo': 1011, 'dischia': 1012, 'hai': 1013, 'las': 1014, 'ral': 1015, 'bir': 1016, 'ran': 1017, 'nio': 1018, 'chac': 1019, 'troc': 1020, 'ber': 1021, 'crà': 1022, 'richia': 1023, 'na-': 1024, 'gra': 1025, 'od': 1026, 'trò': 1027, 'drir': 1028, 'chiap': 1029, 'um': 1030, 'greg': 1031, 'lad': 1032, 'taf': 1033, 'mun': 1034, 'on': 1035, 'trà': 1036, 'red': 1037, 'fàs': 1038, 'chio': 1039, 'rol': 1040, 'che': 1041, 'gue': 1042, 'stet': 1043, 'rit': 1044, 'sel': 1045, 'stra': 1046, 'drò': 1047, 'sbar': 1048, ' fior': 1049, 'luz': 1050, 'chiun': 1051, 'bac': 1052, 'cie': 1053, 'scol': 1054, 'dis': 1055, 'suc': 1056, 'nual': 1057, 'chap': 1058, ' roc': 1059, 'den': 1060, 'grat': 1061, 'bren': 1062, 'rin': 1063, 'grin': 1064, 'sien': 1065, 'nol': 1066, 'blan': 1067, 'cul': 1068, 'spec': 1069, 'richi': 1070, 'smu': 1071, 'dischio': 1072, 'spet': 1073, 'boc': 1074, 'zial': 1075, 'brei': 1076, 'scel': 1077, 'lol': 1078, 'ar': 1079, 'chet': 1080, 'cel': 1081, 'tro': 1082, 'nec': 1083, 'più': 1084, 'vin': 1085, 'seb': 1086, 'guef': 1087, 'bò': 1088, 'puc': 1089, 'gnus': 1090, 'svi': 1091, 'nier': 1092, 'gnel': 1093, 'rì': 1094, 'gò': 1095, 'guit': 1096, 'ches': 1097, 'ghi': 1098, 'guì': 1099, 'sob': 1100, 'duol': 1101, 'scam': 1102, 'scoi': 1103, 'lho': 1104, 'gnai': 1105, 'spu': 1106, 'meg': 1107, 'tì': 1108, 'lop': 1109, 'scie': 1110, 'dò': 1111, 'sot': 1112, 'fau': 1113, ' gran': 1114, ' cen': 1115, 'doc': 1116, 'nar': 1117, 'rot': 1118, 'dè': 1119, 'dischiu': 1120, 'scio': 1121, 'peschi': 1122, 'gru': 1123, 'giac': 1124, 'scrit': 1125, 'sfin': 1126, 'guo': 1127, 'creu': 1128, 'zel': 1129, 'ner': 1130, 'sov': 1131, 'sdru': 1132, 'gior': 1133, 'no': 1134, 'se': 1135, 'fal': 1136, 'diche': 1137, 'tom': 1138, 'stic': 1139, 'stier': 1140, 'ras': 1141, 'col': 1142, 'chiò': 1143, 'zuf': 1144, 'glier': 1145, 'fon': 1146, 'trei': 1147, 'cet': 1148, 'gon': 1149, 'deg': 1150, 'svia': 1151, 'pra': 1152, 'scris': 1153, 'gol': 1154, 'viam': 1155, 'strin': 1156, 'van': 1157, 'exi': 1158, 'get': 1159, 'prir': 1160, 'stie': 1161, 'bur': 1162, 'sau': 1163, 'il': 1164, 'don': 1165, 'gur': 1166, 'suo': 1167, 'sis': 1168, ' mor': 1169, 'quis': 1170, 'quam': 1171, 'giun': 1172, 'ci': 1173, 'pior': 1174, 'dèi': 1175, 'resche': 1176, 'nau': 1177, 'ror': 1178, 'zia': 1179, 'paghe': 1180, 'ù': 1181, 'scaz': 1182, 'sir': 1183, 'stras': 1184, 'vit': 1185, 'z': 1186, 'han': 1187, 'echi': 1188, 'stòr': 1189, 'star': 1190, ' po': 1191, 'vag': 1192, 'sfer': 1193, 'mi-': 1194, 'lec': 1195, 'rischia': 1196, 'pet': 1197, 'var': 1198, '=end_terzine=': 1199, 'cad': 1200, 'fel': 1201, 'got': 1202, 'tei': 1203, 'strac': 1204, 'tu': 1205, 'tie': 1206, 'ciot': 1207, 'tiv': 1208, 'dre': 1209, ' due': 1210, 'fun': 1211, 'ful': 1212, 'spen': 1213, 'cin': 1214, 'pieghe': 1215, 'hon': 1216, 'quest': 1217, 'gno': 1218, 'laz': 1219, 'lim': 1220, 'mul': 1221, 'pat': 1222, 'tron': 1223, 'toc': 1224, 'zio': 1225, 'fian': 1226, 'sui': 1227, 'vio': 1228, 'jau': 1229, 'spo': 1230, 'sta': 1231, 'piez': 1232, 'nin': 1233, 'cham': 1234, 'nam': 1235, 'drà': 1236, 'snel': 1237, 'stro': 1238, 'sus': 1239, 'cir': 1240, 'az': 1241, 'raphèl': 1242, 'cob': 1243, 'sà': 1244, 'siche': 1245, 'scev': 1246, 'sciò': 1247, 'strar': 1248, 'tòm': 1249, 'ste': 1250, 'met': 1251, ' com': 1252, 'preghe': 1253, 'sca': 1254, 'lel': 1255, 'creb': 1256, 'mer': 1257, 'vis': 1258, 'nav': 1259, 'guet': 1260, 'teschio': 1261, 'tichi': 1262, 'fà': 1263, 'piche': 1264, 'gei': 1265, 'ciu': 1266, 'at': 1267, 'stien': 1268, 'stret': 1269, 'scion': 1270, 'spor': 1271, 'vec': 1272, 'ch': 1273, 'chian': 1274, 'vid': 1275, 'scian': 1276, 'suf': 1277, 'adha': 1278, 'fab': 1279, 'tez': 1280, 'deun': 1281, 'per': 1282, 'sog': 1283, ' re': 1284, 'nil': 1285, 'cio': 1286, 'mag': 1287, 'cheb': 1288, ' lu': 1289, 'or': 1290, 'rechi': 1291, 'os': 1292, 'nac': 1293, 'poc': 1294, 'cis': 1295, 'ò': 1296, 'spe': 1297, 'zion': 1298, 'char': 1299, 'vat': 1300, 'ghie': 1301, 'dachil': 1302, 'put': 1303, 'stù': 1304, 'mem': 1305, 'casche': 1306, 'of': 1307, 'du': 1308, 'graf': 1309, 'clug': 1310, 'sor': 1311, 'ghiò': 1312, 'pig': 1313, 'piov': 1314, 'luen': 1315, 'vòl': 1316, 'lic': 1317, 'die': 1318, 'sen': 1319, 'dram': 1320, 'pla': 1321, 'gres': 1322, 'rab': 1323, 'pub': 1324, 'iv': 1325, 'smo': 1326, 'sciu': 1327, 'run': 1328, 'desche': 1329, 'vu': 1330, 'luo': 1331, 'bul': 1332, 'scu': 1333, 'loi': 1334, 'grue': 1335, 'ples': 1336, 'dron': 1337, '-quan': 1338, 'tre': 1339, 'nob': 1340, 'sven': 1341, 'nis': 1342, 'pric': 1343, 'sù': 1344, 'pruo': 1345, 'gab': 1346, 'scrip': 1347, 'mha': 1348, 'stan': 1349, 'fioc': 1350, 'tiche': 1351, 'ag': 1352, 'rìb': 1353, ' mie': 1354, 'lag': 1355, 'fai': 1356, 'tuc': 1357, 'rai': 1358, 'niel': 1359, 'quor': 1360, 'lòn': 1361, 'pis': 1362, 'bui': 1363, 'piag': 1364, 'plu': 1365, 'ces': 1366, 'raschet': 1367, 'fia': 1368, 'bat': 1369, 'schi': 1370, 'sio': 1371, ' di': 1372, 'euf': 1373, 'gli': 1374, 'sde': 1375, 'michel': 1376, 'dai': 1377, 'fe': 1378, 'dac': 1379, 'ruf': 1380, 'buf': 1381, 'vial': 1382, 'seg': 1383, 'ot': 1384, 'vau': 1385, 'uom': 1386, 'gros': 1387, 'quache': 1388, 'von': 1389, 'còl': 1390, 'sfoghi': 1391, 'fien': 1392, 'tam': 1393, 'plan': 1394, 'scon': 1395, 'mosche': 1396, 'luoghi': 1397, 'test': 1398, 'cuo': 1399, 'ghet': 1400, 'vò': 1401, 'strut': 1402, 'gnie': 1403, 'din': 1404, 'lot': 1405, 'gnia': 1406, 'tiun': 1407, 'ciechi': 1408, 'sguar': 1409, 'min': 1410, 'scien': 1411, 'rel': 1412, 'ha': 1413, ' de': 1414, 'ler-co': 1415, 'plo': 1416, 'set': 1417, 'rid': 1418, 'fo': 1419, 'cam': 1420, 'bai': 1421, 'mon': 1422, 'dian': 1423, 'rè': 1424, 'deum': 1425, 'rò': 1426, 'glo': 1427, 'gu': 1428, 'fuo': 1429, 'lef': 1430, 'pon': 1431, 'scin': 1432, 'tiò': 1433, 'trot': 1434, 'sciut': 1435, 'i': 1436, 'giu': 1437, 'prei': 1438, 'lis': 1439, 'si': 1440, 'mir': 1441, 'richeg': 1442, 'tien': 1443, 'gob': 1444, 'ra': 1445, 'oc': 1446, 'scri': 1447, 'lhan': 1448, 'rer': 1449, 'gial': 1450, 'brug': 1451, 'quan': 1452, 'fioche': 1453, 'tè': 1454, 'tòn': 1455, 'dos': 1456, 'croi': 1457, 'sme': 1458, 'pòs': 1459, 'scheg': 1460, 'pe-': 1461, 'nic': 1462, 'fiò': 1463, 'roz': 1464, 'stroz': 1465, 'lof': 1466, 'brac': 1467, 'ohi': 1468, 'tic': 1469, 'ched': 1470, 'cos': 1471, 'tel': 1472, 'tian': 1473, 'mò': 1474, 'trac': 1475, 'gna': 1476, 'schiat': 1477, 'stuo': 1478, 'drai': 1479, 'maschi': 1480, 'cra': 1481, 'dop': 1482, 'cias': 1483, 'ris': 1484, 'mie': 1485, 'excel': 1486, 'ciòl': 1487, 'stèr': 1488, '-start-': 1489, 'mug': 1490, 'dies': 1491, 'nian': 1492, 'svel': 1493, 'pion': 1494, 'iep': 1495, 'gner': 1496, 'fue': 1497, 'sdeg': 1498, 'ni': 1499, 'b': 1500, 'stia': 1501, 'til': 1502, 'bel': 1503, 'di-e': 1504, 'cem': 1505, 'ciam': 1506, 'giz': 1507, 'nir': 1508, 'ul': 1509, 'gram': 1510, 'vet': 1511, 'nuan': 1512, 'trat': 1513, 'lam': 1514, 'as': 1515, 'stral': 1516, 'eghi': 1517, 'achi': 1518, 'guaz': 1519, 'muc': 1520, 'vien': 1521, 'lò': 1522, 'rup': 1523, 'meschi': 1524, 'stin': 1525, 'mic': 1526, 'cioc': 1527, 'riz': 1528, 'ves': 1529, 'guag': 1530, 'svian': 1531, 'stroc': 1532, 'là': 1533, 'driz': 1534, 'mischio': 1535, 'saf': 1536, 'mè': 1537, 'vab': 1538, 'ov': 1539, 'vap': 1540, 'tir': 1541, 'ger': 1542, 'stoi': 1543, 'sfar': 1544, 'pre': 1545, 'chias': 1546, 'glet': 1547, 'tan': 1548, 'nat': 1549, 'lit': 1550, 'rìs': 1551, 'cri': 1552, 'tru': 1553, 'bom': 1554, 'ghia': 1555, 'bù': 1556, 't': 1557, 'spau': 1558, 'òth': 1559, 'shai': 1560, 'drel': 1561, 'quen': 1562, 'sòn': 1563, 'ab': 1564, 'chez': 1565, 'sar': 1566, 'far': 1567, 'sub': 1568, 'gar': 1569, 'siòn': 1570, 'rar': 1571, 'no-in': 1572, 'ion': 1573, 'quo': 1574, 'dit': 1575, 'guic': 1576, 'glian': 1577, 'ven': 1578, 'par': 1579, 'pran': 1580, 'span': 1581, 'reche': 1582, 'prà': 1583, 'spun': 1584, 'scop': 1585, 'drì': 1586, 'sneb': 1587, 'mot': 1588, 'scov': 1589, 'prec': 1590, 'miche': 1591, 'trar': 1592, 'fug': 1593, 'sian': 1594, 'com': 1595, 'tràs': 1596, 'pres': 1597, 'vexil': 1598, 'piat': 1599, 'lip': 1600, 'nab': 1601, 'dan': 1602, 'ge': 1603, 'me': 1604, ' lie': 1605, 'vian': 1606, 'è': 1607, 'vìd': 1608, 'au': 1609, 'pap': 1610, 'viag': 1611, 'nie': 1612, 'gem': 1613, 'glies': 1614, 'pul': 1615, 'stric': 1616, 'tag': 1617, 'puo': 1618, 'vol': 1619, 'zie': 1620, '   tal': 1621, 'vel': 1622, 'spez': 1623, 'fib': 1624, 'guis': 1625, 'spir': 1626, 'moi': 1627, 'nieghi': 1628, 'nom': 1629, 'ciuf': 1630, 'nez': 1631, 'pac': 1632, 'reb': 1633, 'cè': 1634, 'iar': 1635, 'rut': 1636, 'guen': 1637, 'spin': 1638, 'dil': 1639, 'lì': 1640, 'cred': 1641, 'dez': 1642, 'bo': 1643, 'cut': 1644, 'deb': 1645, 'guiz': 1646, 'sop': 1647, 'gnas': 1648, 'ges': 1649, 'bron': 1650, 'gal': 1651, 'lez': 1652, 'nha': 1653, 'der': 1654, 'uhi': 1655, 'ie': 1656, 'tas': 1657, 'gi': 1658, 'sat': 1659, 'mìs': 1660, 'cac': 1661, 'rui': 1662, ' tuo': 1663, 'sgom': 1664, 'tor': 1665, 'sga': 1666, 'bol': 1667, 'nhan': 1668, 'poz': 1669, 'vagheg': 1670, ' suo': 1671, 'stiam': 1672, 'bet': 1673, ' bi': 1674, ' fi': 1675, 'mei': 1676, 'liev': 1677, 'lat': 1678, 'lichi': 1679, 'grop': 1680, 'dua': 1681, 'gai': 1682, 'ughi': 1683, 'am': 1684, 'strug': 1685, 'tran': 1686, 'tha': 1687, 'piz': 1688, 'gien': 1689, 'lio': 1690, 'pron': 1691, 'strav': 1692, 'io': 1693, 'tras': 1694, 'o': 1695, 'nès': 1696, 'spie': 1697, 'rir': 1698, 'stichi': 1699, 'cèr': 1700, 'gron': 1701, 'tun': 1702, 'trai': 1703, 'nem': 1704, 'sti': 1705, 'ro': 1706, 'gnu': 1707, 'bug': 1708, 'sac': 1709, 'dol': 1710, 'dap': 1711, 'fas': 1712, 'spieghi': 1713, 'mil': 1714, 'sba': 1715, 'blio': 1716, 'ruc': 1717, 'ter': 1718, 'sle': 1719, ' cor': 1720, 'fior': 1721, 'blia': 1722, 'mas': 1723, 'is': 1724, 'sden': 1725, 'plar': 1726, ' rei': 1727, 'gen': 1728, 'li': 1729, 'mor': 1730, 'sie': 1731, 'cien': 1732, 'sod': 1733, 'vai-e': 1734, 'nò': 1735, 'tret': 1736, 'nu': 1737, 'clès': 1738, 'leb': 1739, 'cuor': 1740, 'stio': 1741, 'bro': 1742, 'ciul': 1743, 'sas': 1744, 'cliò': 1745, 'rom': 1746, 'schia': 1747, 'niam': 1748, 'tuas': 1749, 'diè': 1750, 'zien': 1751, 'streg': 1752, 'za': 1753, 'top': 1754, 'guil': 1755, 'stig': 1756, 'squil': 1757, 'gne': 1758, 'nev': 1759, 'sto': 1760, 'sten': 1761, 'scab': 1762, 'luom': 1763, 'uc': 1764, 'vho': 1765, 'diur': 1766, 'zuc': 1767, 'pas': 1768, 'deschi': 1769, 'gag': 1770, 'fec': 1771, 'egyp': 1772, 'càn': 1773, 'tia': 1774, 'svuol': 1775, 'broc': 1776, 'ron': 1777, 'eb': 1778, 'lè': 1779, 'zo': 1780, 'u': 1781, 'ti': 1782, 'vor': 1783, 'sez': 1784, 'giochi': 1785, 'cheg': 1786, 'còb': 1787, 'trui': 1788, 'pr': 1789, 'fom': 1790, 'dri': 1791, 'schic': 1792, 'lab': 1793, 'trag': 1794, 'peschie': 1795, 'pom': 1796, 'pachi': 1797, 'mal': 1798, 'bei': 1799, 'fui': 1800, 'vì': 1801, 'rap': 1802, 'raf': 1803, 'gier': 1804, 'siar': 1805, 'dui': 1806, 'stion': 1807, 'vai': 1808, 'ga': 1809, 'scac': 1810, 'fu': 1811, 'chioc': 1812, 'smoz': 1813, 'cic': 1814, 'fig': 1815, 'lai': 1816, 'tif': 1817, 'bra': 1818, 'sic': 1819, 'caf': 1820, 'bàl': 1821, 'sdet': 1822, 'sciol': 1823, 'chè': 1824, 'già': 1825, 'bèl': 1826, 'fit': 1827, 'buc': 1828, 'im': 1829, 'spar': 1830, 'clu': 1831, 'lhai': 1832, ' ma': 1833, 'ce': 1834, 'stui': 1835, 'bil': 1836, 'fil': 1837, 'scioc': 1838, 'gro': 1839, 'quer': 1840, 'chie': 1841, 'fam': 1842, 'ban': 1843, 'lan': 1844, 'stè': 1845, 'cas': 1846, 'ache': 1847, 'pe': 1848, 'fè': 1849, 'fem': 1850, 'rig': 1851, 'cù': 1852, 'gnem': 1853, 'eschi': 1854, 'lil': 1855, 'lon': 1856, 'sha': 1857, 'vie': 1858, 'lui': 1859, 'spel': 1860, 'tem': 1861, 'dof': 1862, 'stu': 1863, 'muo': 1864, 'dia': 1865, 'muf': 1866, 'ciap': 1867, 'mia': 1868, 'gua': 1869, 'reg': 1870, 'vogl': 1871, 'tuon': 1872, 'prì': 1873, 'lus': 1874, 'gaz': 1875, 'pog': 1876, 'tram': 1877, 'tiam': 1878, 'dov': 1879, 'bab': 1880, 'liche': 1881, 'paghi': 1882, 'sò': 1883, 'sgiun': 1884, 'òs': 1885, 'biz': 1886, 'ghez': 1887, 'gin': 1888, 'lar': 1889, 'lig': 1890, 'piar': 1891, 'sì': 1892, 'spiac': 1893, 'sleghi': 1894, 'pu': 1895, 'zon': 1896, 'gas': 1897, 'dir': 1898, 'làche': 1899, 'dun': 1900, 'fer': 1901, 'gion': 1902, 'ze': 1903, 'goc': 1904, 'dim': 1905, 'sum': 1906, 'fur': 1907, 'tus': 1908, 'dut': 1909, 'glio': 1910, 'ia': 1911, 'mus': 1912, 'fic': 1913, 'scro': 1914, 'rat': 1915, 'pè': 1916, 'làn': 1917, 'brez': 1918, 'nìt': 1919, 'c': 1920}\n",
            "int2syl:  {0: 'bran', 1: 'fischio', 2: 'om', 3: 'truo', 4: 'mos', 5: 'sgher', 6: 'gras', 7: 'sul', 8: 'mì', 9: 'in', 10: 'scuo', 11: 'som', 12: 'zan', 13: 'sf', 14: 'grem', 15: 'stiò', 16: 'les', 17: 'toschi', 18: 'cot', 19: 'fio', 20: 'spia', 21: 'gib', 22: 'tav', 23: 'piog', 24: 'gnen', 25: 'pit', 26: 'cop', 27: 'spuo', 28: 'glau', 29: 'nen', 30: 'cil', 31: 'for', 32: 'tri', 33: 'pluo', 34: 'smon', 35: ' le', 36: 'd', 37: 'scher', 38: 'sug', 39: 'mol', 40: 'smi', 41: 'stran', 42: 'mab', 43: 'do-', 44: 'spro', 45: 'rez', 46: 'chad', 47: 'gnon', 48: 'mio', 49: 'tier', 50: 'sion', 51: 'los', 52: 'biche', 53: 'lup', 54: 'cle', 55: 'pec', 56: 'bue', 57: 'viem', 58: 'nè', 59: 'id', 60: 'fes', 61: 'dì', 62: 'pun', 63: 'sgo', 64: 'strat', 65: 'vam', 66: ' trat', 67: 'stian', 68: 'zaf', 69: 'chu', 70: 'iu', 71: 'map', 72: 'quod', 73: 'dub', 74: 'tum', 75: 'ciel', 76: 'lav', 77: 'te', 78: 'gie', 79: 'fa', 80: 'scir', 81: 'rim', 82: 'can', 83: ' so', 84: 'sal', 85: 'quil', 86: 'caschi', 87: 'masche', 88: 'dei', 89: 'tut', 90: 'chab', 91: 'noi', 92: 'sbri', 93: 'puos', 94: 'grot', 95: 'naut', 96: 'stul', 97: 'puot', 98: 'lul', 99: 'prun', 100: 'del', 101: 'sfam', 102: 'e', 103: 'tren', 104: 'pia', 105: 'clis', 106: 'ef', 107: 'dug', 108: 'rog', 109: 'nua', 110: 'sbi', 111: 'ben', 112: 'guar', 113: 'bris', 114: 'pien', 115: 'deus', 116: 'sier', 117: 'man', 118: 'git', 119: 'zai', 120: 'stis', 121: 'pen', 122: 'cru', 123: 'ìs', 124: 'duo', 125: 'ta-', 126: 'prim', 127: 'pue', 128: 'ochi', 129: 'lur', 130: 'tol', 131: 'le', 132: 'pot', 133: 'strà', 134: 'leg', 135: 'len', 136: 'vil', 137: 'glior', 138: 'cov', 139: 'gòr', 140: 'nap', 141: 'ril', 142: 'sfor', 143: 'rof', 144: 'tau', 145: 'chor', 146: 'sfac', 147: 'mim', 148: 'sias', 149: 'schie', 150: 'ad', 151: 'fuochi', 152: 'cal', 153: 'mhai', 154: 'dur', 155: 'do', 156: 'chel', 157: 'spal', 158: 'cla', 159: 'ziar', 160: 'gnin', 161: 's', 162: 'guz', 163: 'nef', 164: 'spol', 165: 'prez', 166: 'nur', 167: 'cag', 168: 'chin', 169: 'zul', 170: 'eu', 171: 'tos', 172: 'dom', 173: 'lion', 174: 'cez', 175: 'sgra', 176: 'via', 177: 'ob', 178: 'luc', 179: 'ghes', 180: 'gnum', 181: 'teg', 182: 'gor', 183: 'g', 184: 'bri', 185: 'neg', 186: 'pro', 187: 'nias', 188: 'nei', 189: 'con', 190: 'veg', 191: 'vo', 192: 'joi', 193: 'veschi', 194: 'tà', 195: 'puol', 196: 'cef', 197: 'nache', 198: 'tò', 199: 'clo', 200: 'num', 201: 'tat', 202: 'se-or', 203: 'nuo', 204: 'vez', 205: 'gnuo', 206: 'mar', 207: 'spic', 208: 'stei', 209: 'scos', 210: 'to-', 211: 'stor', 212: 'coz', 213: 'scim', 214: 'siam', 215: 'rua', 216: 'pic', 217: 'nel', 218: 'cer', 219: 'dèch', 220: 'res', 221: ' fos', 222: 'tr', 223: 'chog', 224: 'stre', 225: 'strò', 226: 'soc', 227: 'gir', 228: 'nas', 229: 'muel', 230: ' buo', 231: 'ghien', 232: 'sbe', 233: 'cor', 234: 'vha', 235: 'èv', 236: 'stri', 237: 'r', 238: 'sciom', 239: 'bli', 240: 'el', 241: 'dichia', 242: 'se-e', 243: ' pol', 244: 'chì', 245: 'moz', 246: 'strai', 247: 'sim', 248: 'bec', 249: 'lail', 250: 'richiu', 251: 'nag', 252: 'guaschi', 253: 'dic', 254: 'gom', 255: 'mad', 256: 'nòs', 257: 'quel', 258: 'bev', 259: 'glie', 260: 'laghi', 261: 'lir', 262: 'fiche', 263: 'sup', 264: 'ciar', 265: 'àm', 266: 'sgan', 267: 'tio', 268: 'sol', 269: 'scar', 270: ' vi', 271: 'ca', 272: 'bin', 273: 'scis', 274: 'tad', 275: 'spre', 276: 'spaz', 277: 'scal', 278: 'scem', 279: 'tac', 280: 'noschi', 281: 'but', 282: 'sos', 283: 'lichin', 284: 'maf', 285: 'gliel', 286: 'boschi', 287: 'bru', 288: 'guel', 289: 'fan', 290: 'cu', 291: 'lun', 292: 'dul', 293: 'scet', 294: 'rache', 295: 'giche', 296: 'gliò', 297: 'chag', 298: 'gia', 299: 'tes', 300: 'gòn', 301: 'stal', 302: 'rob', 303: 'af', 304: 'dache', 305: 'sav', 306: 'diz', 307: ' vol', 308: 'quar', 309: 'vèn', 310: 'ploi', 311: 'deh', 312: 'paz', 313: 'cras', 314: 'lien', 315: 'son', 316: 'veb', 317: 'ghiac', 318: 'pol', 319: 'dot', 320: 'può', 321: 'fàc', 322: 'piè', 323: 'bis', 324: 'vostr', 325: 'meb', 326: 'lac', 327: 'fol', 328: 'cion', 329: 'dichi', 330: 'scran', 331: 'fei', 332: 'vei', 333: 'al', 334: 'sciam', 335: 'venha', 336: 'vac', 337: 'suol', 338: 'rau', 339: 'mai', 340: 'cap', 341: 'treb', 342: 'f', 343: 'sit', 344: 'fog', 345: 'mèc', 346: 'sua', 347: 'gan', 348: 'rachel', 349: 'pie', 350: 'ret', 351: 'èl', 352: 'sfa', 353: 'lal', 354: 'spran', 355: 'richie', 356: 'cì', 357: '-e', 358: 'chan', 359: 'vè', 360: 'trop', 361: 'vof', 362: 'vir', 363: 'gual', 364: 'chas', 365: 'preghie', 366: 'spra', 367: 'tua', 368: 've', 369: 'chun', 370: 'scì', 371: 'er', 372: 'en', 373: 'tès', 374: 'viz', 375: 'scen', 376: 'stò', 377: 'tal', 378: 'sue', 379: 'schiet', 380: 'sgri', 381: 'men', 382: 'gliu', 383: 'ùl', 384: 'poche', 385: 'mur', 386: 'lèu', 387: 'det', 388: 'noz', 389: 'dier', 390: 'ciò', 391: 'chi', 392: 'loc', 393: 'ram', 394: 'scot', 395: 'lha', 396: 'sai', 397: 'scun', 398: 'stiz', 399: 'ler', 400: 'squar', 401: 'giàs', 402: 'un', 403: 'raz', 404: 'qui', 405: 'stram', 406: 'feb', 407: ' mar', 408: 'pier', 409: 'let', 410: 'plor', 411: 'rif', 412: 'gam', 413: 'pan', 414: 'vun', 415: 'cian', 416: 'oi', 417: 'daz', 418: 'guc', 419: 'bre', 420: 'ba', 421: 'pris', 422: 'av', 423: 'nig', 424: 'tim', 425: 'toi', 426: 'ser', 427: 'ed', 428: 'gnaz', 429: ' ', 430: 'pio', 431: 'su', 432: 'ver', 433: 'pian', 434: 'dab', 435: 'te-', 436: 'tar', 437: 'des', 438: 'chai', 439: 'disf', 440: 'noc', 441: 'ruo', 442: 'spi', 443: 'tec', 444: 'giar', 445: 'ren', 446: 'sun', 447: 'mhan', 448: 'nan', 449: 'scic', 450: 'chiar', 451: 'bef', 452: 'spon', 453: 'cei', 454: 'piac', 455: 'viò', 456: 'tab', 457: 'liòs', 458: 'spiar', 459: 'm', 460: 'sfio', 461: 'feg', 462: 'ròn', 463: 'achil', 464: 'smaghi', 465: 'bun', 466: 'vaghez', 467: 'rug', 468: 'dio', 469: 'bot', 470: 'bas', 471: 'nai', 472: 'daf', 473: 'nuc', 474: 'pei', 475: 'stav', 476: 'brul', 477: 'quin', 478: 'dam', 479: 'règ', 480: 'guò', 481: 'ho', 482: 'cit', 483: 'stel', 484: 'sban', 485: 'rem', 486: 'ec', 487: 'ne', 488: 'scial', 489: 'mum', 490: 'gau', 491: 'prai', 492: 'ta', 493: 'gè', 494: 'zar', 495: 'poi', 496: 'rech', 497: 'còth', 498: 'giam', 499: 'gian', 500: 'iat', 501: 'chen', 502: 'rev', 503: 'est', 504: 'dal', 505: '-end-', 506: 'lu', 507: 'drit', 508: 'sad', 509: 'sap', 510: 'go', 511: 'nail', 512: 'paschi', 513: 'sud', 514: 'sno', 515: 'gri', 516: 'fum', 517: 'ghian', 518: 'ghio', 519: 'sei', 520: 'svo', 521: 'sman', 522: 'qual', 523: 'p', 524: 'ri', 525: 'ier', 526: 'eghe', 527: 'po', 528: 'ol', 529: 'voi', 530: 'tàn', 531: 'squi', 532: ' ca', 533: 'gran', 534: 'gim', 535: 'spes', 536: 'vem', 537: 'dà', 538: 'fèl', 539: 'stem', 540: 'roc', 541: 'gnun', 542: 'sab', 543: 'glia', 544: 'pochi', 545: 'mes', 546: 'fis', 547: 'smor', 548: 'cral', 549: 'laf', 550: 'schiu', 551: 'rec', 552: 'ghin', 553: 'zechiel', 554: 'ì', 555: 'boz', 556: 'bal', 557: 'giot', 558: 'gnar', 559: 'gh', 560: 'ur', 561: 'chiet', 562: 'stas', 563: 'dru', 564: 'not', 565: 'pel', 566: 'san', 567: 'nuò', 568: 'guon', 569: 'sper', 570: 'mez', 571: 'ei', 572: 'stru', 573: 'proc', 574: 'spres', 575: 'suon', 576: 'plì', 577: 'sil', 578: 'pì', 579: 'fò', 580: 'em', 581: 'lov', 582: 'pa', 583: 'sè', 584: 'fier', 585: 'gliet', 586: 'pros', 587: 'nal', 588: 'tin', 589: 'cun', 590: ' buon', 591: 'tap', 592: 'nostr', 593: 'pir', 594: 'ghiot', 595: 'schem', 596: 'cher', 597: 'rum', 598: 'scom', 599: 'tur', 600: 'cep', 601: 'ir', 602: 'bròt', 603: 'dra', 604: 'stil', 605: 'zòn', 606: 'vaghi', 607: 'brot', 608: 'gluol', 609: 'sver', 610: 'rac', 611: 'cur', 612: 're', 613: 'pren', 614: 'fen', 615: 'pàr', 616: 'ap', 617: 'stial', 618: 'ref', 619: 'sleghe', 620: 'ciom', 621: 'clau', 622: 'sra', 623: ' tro', 624: 'val', 625: 'sgor', 626: 'nì', 627: 'og', 628: 'dial', 629: 'sa', 630: 'chav', 631: 'than', 632: 'scoc', 633: 'fiu', 634: 'meus', 635: 'coi', 636: 'goz', 637: 'draghi', 638: 'pru', 639: 'sec', 640: 'mam', 641: 'scias', 642: 'nion', 643: 'giug', 644: 'neb', 645: 'cen', 646: 'ric', 647: 'ru', 648: 'l', 649: 'fiot', 650: 'quat', 651: 'muor', 652: 'scan', 653: 'trul', 654: 'quiv', 655: 'tue', 656: 'viar', 657: 'thai', 658: 'schian', 659: 'neun', 660: 'ton', 661: 'lia', 662: 'ghir', 663: 'net', 664: 'cò', 665: 'sco', 666: 'co', 667: 'lem', 668: 'diam', 669: 'crol', 670: 'gnan', 671: 'mèz', 672: 'ghe', 673: 'vos', 674: 'maschio', 675: 'a', 676: 'pau', 677: 'duom', 678: 'sce', 679: 'voil', 680: 'vuo', 681: 'mac', 682: 'slac', 683: 'spai', 684: 'sfe', 685: ' qua', 686: 'dec', 687: 'oh', 688: 'vi-', 689: 'tis', 690: 'si-', 691: 'gat', 692: 'cia', 693: 'dus', 694: 'pos', 695: 'liòn', 696: 'deu', 697: 'sfan', 698: 'puz', 699: 'lom', 700: 'dob', 701: 'tai', 702: 'cuc', 703: 'guan', 704: 'clas', 705: ' car', 706: 'chal', 707: 'strom', 708: 'tù', 709: 'gis', 710: 'spit', 711: 'ahi', 712: 'spuos', 713: 'pieghi', 714: 'àr', 715: 'crin', 716: 'prin', 717: 'v', 718: 'puom', 719: 'ceschi', 720: 'dìt', 721: 'an', 722: 'pin', 723: 'sia', 724: 'tror', 725: 'mischiar', 726: 'ciac', 727: 'cum', 728: 'coc', 729: 'bè', 730: 'soz', 731: 'lian', 732: 'pi', 733: 'chiu', 734: 'lep', 735: 'dran', 736: 'spraz', 737: ' sar', 738: 'fi', 739: 'cruc', 740: 'cre', 741: 'gnor', 742: 'lei', 743: 'vom', 744: 'cuir', 745: 'stam', 746: 'stes', 747: 'bor', 748: 'sual', 749: ' f', 750: 'ciasche', 751: 'zò', 752: 'chion', 753: 'rà', 754: 'fos', 755: 'trec', 756: 'pem', 757: 'sam', 758: 'gni', 759: 'fuor', 760: 'maz', 761: 'stai', 762: 'cau', 763: 'ses', 764: 'so', 765: 'lie', 766: 'stop', 767: 'niz', 768: 'tui', 769: 'guer', 770: 'tle', 771: 'brut', 772: 'grep', 773: 'fin', 774: 'fiar', 775: 'cric', 776: 'gì', 777: 'splen', 778: 'dau', 779: 'due', 780: 'pò', 781: 'smal', 782: 'grap', 783: 'fial', 784: 'ghil', 785: 'lèm', 786: 'nà', 787: 'dem', 788: 'sciar', 789: 'no-', 790: 'chuom', 791: 'gher', 792: 'chom', 793: '   non', 794: 'tiem', 795: 'qua', 796: 'nul', 797: 'piu', 798: 'sopr', 799: 'mu', 800: 'sve', 801: 'sev', 802: 'buen', 803: 'pes', 804: 'chier', 805: 'sur', 806: 'glien', 807: 'pri', 808: 'bu', 809: 'chei', 810: 'mav', 811: 'vul', 812: 'vas', 813: 'tuo', 814: 'gel', 815: 'sfat', 816: 'chèi', 817: 'rasche', 818: 'sag', 819: 'scre', 820: 'lap', 821: 'pev', 822: 'stren', 823: 'gil', 824: 'uo', 825: 'lin', 826: 'cro', 827: 'to', 828: 'fet', 829: 'ros', 830: 'lasc', 831: 'dor', 832: 'spa', 833: 'pli', 834: 'ai', 835: 'cain', 836: 'ciol', 837: 'sfo', 838: 'scuf', 839: 'tra', 840: 'reschi', 841: 'cim', 842: 'cli', 843: 'sciàn', 844: 'peg', 845: 'voche', 846: 'sci', 847: 'tet', 848: 'fie', 849: 'mat', 850: 'ior', 851: 'dog', 852: 'preghi', 853: 'gliam', 854: 'suè', 855: 'gio', 856: 'que', 857: ' fa', 858: 'blian', 859: 'grom', 860: 'gias', 861: 'por', 862: 'sma', 863: 'cha', 864: 'zi', 865: 'pop', 866: 'cat', 867: 'lut', 868: 'bon', 869: 'buon', 870: 'riche', 871: 'gad', 872: 'sber', 873: 'zac', 874: 'chiasche', 875: 'sep', 876: 'gnes', 877: 'se-be', 878: 'stec', 879: 'sof', 880: 'non', 881: 'na', 882: 'biò', 883: 'smar', 884: 'lo', 885: 'sin', 886: 'zur', 887: 'chat', 888: 'sla', 889: 'fac', 890: 'pa-', 891: 'dro', 892: 'lor', 893: 'scer', 894: 'cho', 895: 'op', 896: 'vaghe', 897: 'viv', 898: 'puesc', 899: 'piom', 900: 'pièr', 901: 'lum', 902: 'scia', 903: 'mian', 904: 'pai', 905: 'bì', 906: '-lan', 907: 'pal', 908: 'ma', 909: 'mel', 910: 'sna', 911: 'piaghe', 912: 'grez', 913: 'troi', 914: 'sbra', 915: 'vier', 916: 'tho', 917: 'n', 918: 'be', 919: 'vion', 920: 'mi', 921: 'nia', 922: 'tòr', 923: 'pil', 924: 'chis', 925: 'nor', 926: 'guir', 927: 'nòi', 928: '    e', 929: 'stà', 930: 'èn', 931: 'rischio', 932: 'fat', 933: 'glion', 934: 'et', 935: 'ten', 936: 'fòn', 937: 'gre', 938: 'ac', 939: 'strel', 940: 'va', 941: 'gui', 942: 'mischia', 943: 'ghel', 944: 'stì', 945: 'tuf', 946: 'cai', 947: 'vi', 948: 'zian', 949: 'bus', 950: 'spri', 951: 'qu', 952: 'trom', 953: 'fàt', 954: 'fiam', 955: 'brol', 956: 'car', 957: 'cui', 958: 'lier', 959: 'pur', 960: 'plau', 961: 'tris', 962: 'giò', 963: 'rei', 964: 'stol', 965: 'giù', 966: 'fu-quan', 967: 'dras', 968: 'chia', 969: 'squa', 970: 'es', 971: 'dar', 972: 'sem', 973: 'dien', 974: 'nun', 975: 'ròl', 976: 'rav', 977: 'bar', 978: 'stuc', 979: 'nim', 980: 'trap', 981: 'fuc', 982: 'gliar', 983: 'dat', 984: 'reghi', 985: 'ple', 986: 'scor', 987: 'dif', 988: 'la', 989: 'nes', 990: 'dail', 991: 'ne-', 992: 'ghiar', 993: 'das', 994: 'rag', 995: 'nien', 996: 'vuol', 997: 'mof', 998: 'fiac', 999: 'mo', 1000: 'tual', 1001: 'smen', 1002: 'buo', 1003: 'brar', 1004: 'lau', 1005: 'di', 1006: 'drap', 1007: 'bi', 1008: 'da', 1009: 'ster', 1010: 'de', 1011: ' mo', 1012: 'dischia', 1013: 'hai', 1014: 'las', 1015: 'ral', 1016: 'bir', 1017: 'ran', 1018: 'nio', 1019: 'chac', 1020: 'troc', 1021: 'ber', 1022: 'crà', 1023: 'richia', 1024: 'na-', 1025: 'gra', 1026: 'od', 1027: 'trò', 1028: 'drir', 1029: 'chiap', 1030: 'um', 1031: 'greg', 1032: 'lad', 1033: 'taf', 1034: 'mun', 1035: 'on', 1036: 'trà', 1037: 'red', 1038: 'fàs', 1039: 'chio', 1040: 'rol', 1041: 'che', 1042: 'gue', 1043: 'stet', 1044: 'rit', 1045: 'sel', 1046: 'stra', 1047: 'drò', 1048: 'sbar', 1049: ' fior', 1050: 'luz', 1051: 'chiun', 1052: 'bac', 1053: 'cie', 1054: 'scol', 1055: 'dis', 1056: 'suc', 1057: 'nual', 1058: 'chap', 1059: ' roc', 1060: 'den', 1061: 'grat', 1062: 'bren', 1063: 'rin', 1064: 'grin', 1065: 'sien', 1066: 'nol', 1067: 'blan', 1068: 'cul', 1069: 'spec', 1070: 'richi', 1071: 'smu', 1072: 'dischio', 1073: 'spet', 1074: 'boc', 1075: 'zial', 1076: 'brei', 1077: 'scel', 1078: 'lol', 1079: 'ar', 1080: 'chet', 1081: 'cel', 1082: 'tro', 1083: 'nec', 1084: 'più', 1085: 'vin', 1086: 'seb', 1087: 'guef', 1088: 'bò', 1089: 'puc', 1090: 'gnus', 1091: 'svi', 1092: 'nier', 1093: 'gnel', 1094: 'rì', 1095: 'gò', 1096: 'guit', 1097: 'ches', 1098: 'ghi', 1099: 'guì', 1100: 'sob', 1101: 'duol', 1102: 'scam', 1103: 'scoi', 1104: 'lho', 1105: 'gnai', 1106: 'spu', 1107: 'meg', 1108: 'tì', 1109: 'lop', 1110: 'scie', 1111: 'dò', 1112: 'sot', 1113: 'fau', 1114: ' gran', 1115: ' cen', 1116: 'doc', 1117: 'nar', 1118: 'rot', 1119: 'dè', 1120: 'dischiu', 1121: 'scio', 1122: 'peschi', 1123: 'gru', 1124: 'giac', 1125: 'scrit', 1126: 'sfin', 1127: 'guo', 1128: 'creu', 1129: 'zel', 1130: 'ner', 1131: 'sov', 1132: 'sdru', 1133: 'gior', 1134: 'no', 1135: 'se', 1136: 'fal', 1137: 'diche', 1138: 'tom', 1139: 'stic', 1140: 'stier', 1141: 'ras', 1142: 'col', 1143: 'chiò', 1144: 'zuf', 1145: 'glier', 1146: 'fon', 1147: 'trei', 1148: 'cet', 1149: 'gon', 1150: 'deg', 1151: 'svia', 1152: 'pra', 1153: 'scris', 1154: 'gol', 1155: 'viam', 1156: 'strin', 1157: 'van', 1158: 'exi', 1159: 'get', 1160: 'prir', 1161: 'stie', 1162: 'bur', 1163: 'sau', 1164: 'il', 1165: 'don', 1166: 'gur', 1167: 'suo', 1168: 'sis', 1169: ' mor', 1170: 'quis', 1171: 'quam', 1172: 'giun', 1173: 'ci', 1174: 'pior', 1175: 'dèi', 1176: 'resche', 1177: 'nau', 1178: 'ror', 1179: 'zia', 1180: 'paghe', 1181: 'ù', 1182: 'scaz', 1183: 'sir', 1184: 'stras', 1185: 'vit', 1186: 'z', 1187: 'han', 1188: 'echi', 1189: 'stòr', 1190: 'star', 1191: ' po', 1192: 'vag', 1193: 'sfer', 1194: 'mi-', 1195: 'lec', 1196: 'rischia', 1197: 'pet', 1198: 'var', 1199: '=end_terzine=', 1200: 'cad', 1201: 'fel', 1202: 'got', 1203: 'tei', 1204: 'strac', 1205: 'tu', 1206: 'tie', 1207: 'ciot', 1208: 'tiv', 1209: 'dre', 1210: ' due', 1211: 'fun', 1212: 'ful', 1213: 'spen', 1214: 'cin', 1215: 'pieghe', 1216: 'hon', 1217: 'quest', 1218: 'gno', 1219: 'laz', 1220: 'lim', 1221: 'mul', 1222: 'pat', 1223: 'tron', 1224: 'toc', 1225: 'zio', 1226: 'fian', 1227: 'sui', 1228: 'vio', 1229: 'jau', 1230: 'spo', 1231: 'sta', 1232: 'piez', 1233: 'nin', 1234: 'cham', 1235: 'nam', 1236: 'drà', 1237: 'snel', 1238: 'stro', 1239: 'sus', 1240: 'cir', 1241: 'az', 1242: 'raphèl', 1243: 'cob', 1244: 'sà', 1245: 'siche', 1246: 'scev', 1247: 'sciò', 1248: 'strar', 1249: 'tòm', 1250: 'ste', 1251: 'met', 1252: ' com', 1253: 'preghe', 1254: 'sca', 1255: 'lel', 1256: 'creb', 1257: 'mer', 1258: 'vis', 1259: 'nav', 1260: 'guet', 1261: 'teschio', 1262: 'tichi', 1263: 'fà', 1264: 'piche', 1265: 'gei', 1266: 'ciu', 1267: 'at', 1268: 'stien', 1269: 'stret', 1270: 'scion', 1271: 'spor', 1272: 'vec', 1273: 'ch', 1274: 'chian', 1275: 'vid', 1276: 'scian', 1277: 'suf', 1278: 'adha', 1279: 'fab', 1280: 'tez', 1281: 'deun', 1282: 'per', 1283: 'sog', 1284: ' re', 1285: 'nil', 1286: 'cio', 1287: 'mag', 1288: 'cheb', 1289: ' lu', 1290: 'or', 1291: 'rechi', 1292: 'os', 1293: 'nac', 1294: 'poc', 1295: 'cis', 1296: 'ò', 1297: 'spe', 1298: 'zion', 1299: 'char', 1300: 'vat', 1301: 'ghie', 1302: 'dachil', 1303: 'put', 1304: 'stù', 1305: 'mem', 1306: 'casche', 1307: 'of', 1308: 'du', 1309: 'graf', 1310: 'clug', 1311: 'sor', 1312: 'ghiò', 1313: 'pig', 1314: 'piov', 1315: 'luen', 1316: 'vòl', 1317: 'lic', 1318: 'die', 1319: 'sen', 1320: 'dram', 1321: 'pla', 1322: 'gres', 1323: 'rab', 1324: 'pub', 1325: 'iv', 1326: 'smo', 1327: 'sciu', 1328: 'run', 1329: 'desche', 1330: 'vu', 1331: 'luo', 1332: 'bul', 1333: 'scu', 1334: 'loi', 1335: 'grue', 1336: 'ples', 1337: 'dron', 1338: '-quan', 1339: 'tre', 1340: 'nob', 1341: 'sven', 1342: 'nis', 1343: 'pric', 1344: 'sù', 1345: 'pruo', 1346: 'gab', 1347: 'scrip', 1348: 'mha', 1349: 'stan', 1350: 'fioc', 1351: 'tiche', 1352: 'ag', 1353: 'rìb', 1354: ' mie', 1355: 'lag', 1356: 'fai', 1357: 'tuc', 1358: 'rai', 1359: 'niel', 1360: 'quor', 1361: 'lòn', 1362: 'pis', 1363: 'bui', 1364: 'piag', 1365: 'plu', 1366: 'ces', 1367: 'raschet', 1368: 'fia', 1369: 'bat', 1370: 'schi', 1371: 'sio', 1372: ' di', 1373: 'euf', 1374: 'gli', 1375: 'sde', 1376: 'michel', 1377: 'dai', 1378: 'fe', 1379: 'dac', 1380: 'ruf', 1381: 'buf', 1382: 'vial', 1383: 'seg', 1384: 'ot', 1385: 'vau', 1386: 'uom', 1387: 'gros', 1388: 'quache', 1389: 'von', 1390: 'còl', 1391: 'sfoghi', 1392: 'fien', 1393: 'tam', 1394: 'plan', 1395: 'scon', 1396: 'mosche', 1397: 'luoghi', 1398: 'test', 1399: 'cuo', 1400: 'ghet', 1401: 'vò', 1402: 'strut', 1403: 'gnie', 1404: 'din', 1405: 'lot', 1406: 'gnia', 1407: 'tiun', 1408: 'ciechi', 1409: 'sguar', 1410: 'min', 1411: 'scien', 1412: 'rel', 1413: 'ha', 1414: ' de', 1415: 'ler-co', 1416: 'plo', 1417: 'set', 1418: 'rid', 1419: 'fo', 1420: 'cam', 1421: 'bai', 1422: 'mon', 1423: 'dian', 1424: 'rè', 1425: 'deum', 1426: 'rò', 1427: 'glo', 1428: 'gu', 1429: 'fuo', 1430: 'lef', 1431: 'pon', 1432: 'scin', 1433: 'tiò', 1434: 'trot', 1435: 'sciut', 1436: 'i', 1437: 'giu', 1438: 'prei', 1439: 'lis', 1440: 'si', 1441: 'mir', 1442: 'richeg', 1443: 'tien', 1444: 'gob', 1445: 'ra', 1446: 'oc', 1447: 'scri', 1448: 'lhan', 1449: 'rer', 1450: 'gial', 1451: 'brug', 1452: 'quan', 1453: 'fioche', 1454: 'tè', 1455: 'tòn', 1456: 'dos', 1457: 'croi', 1458: 'sme', 1459: 'pòs', 1460: 'scheg', 1461: 'pe-', 1462: 'nic', 1463: 'fiò', 1464: 'roz', 1465: 'stroz', 1466: 'lof', 1467: 'brac', 1468: 'ohi', 1469: 'tic', 1470: 'ched', 1471: 'cos', 1472: 'tel', 1473: 'tian', 1474: 'mò', 1475: 'trac', 1476: 'gna', 1477: 'schiat', 1478: 'stuo', 1479: 'drai', 1480: 'maschi', 1481: 'cra', 1482: 'dop', 1483: 'cias', 1484: 'ris', 1485: 'mie', 1486: 'excel', 1487: 'ciòl', 1488: 'stèr', 1489: '-start-', 1490: 'mug', 1491: 'dies', 1492: 'nian', 1493: 'svel', 1494: 'pion', 1495: 'iep', 1496: 'gner', 1497: 'fue', 1498: 'sdeg', 1499: 'ni', 1500: 'b', 1501: 'stia', 1502: 'til', 1503: 'bel', 1504: 'di-e', 1505: 'cem', 1506: 'ciam', 1507: 'giz', 1508: 'nir', 1509: 'ul', 1510: 'gram', 1511: 'vet', 1512: 'nuan', 1513: 'trat', 1514: 'lam', 1515: 'as', 1516: 'stral', 1517: 'eghi', 1518: 'achi', 1519: 'guaz', 1520: 'muc', 1521: 'vien', 1522: 'lò', 1523: 'rup', 1524: 'meschi', 1525: 'stin', 1526: 'mic', 1527: 'cioc', 1528: 'riz', 1529: 'ves', 1530: 'guag', 1531: 'svian', 1532: 'stroc', 1533: 'là', 1534: 'driz', 1535: 'mischio', 1536: 'saf', 1537: 'mè', 1538: 'vab', 1539: 'ov', 1540: 'vap', 1541: 'tir', 1542: 'ger', 1543: 'stoi', 1544: 'sfar', 1545: 'pre', 1546: 'chias', 1547: 'glet', 1548: 'tan', 1549: 'nat', 1550: 'lit', 1551: 'rìs', 1552: 'cri', 1553: 'tru', 1554: 'bom', 1555: 'ghia', 1556: 'bù', 1557: 't', 1558: 'spau', 1559: 'òth', 1560: 'shai', 1561: 'drel', 1562: 'quen', 1563: 'sòn', 1564: 'ab', 1565: 'chez', 1566: 'sar', 1567: 'far', 1568: 'sub', 1569: 'gar', 1570: 'siòn', 1571: 'rar', 1572: 'no-in', 1573: 'ion', 1574: 'quo', 1575: 'dit', 1576: 'guic', 1577: 'glian', 1578: 'ven', 1579: 'par', 1580: 'pran', 1581: 'span', 1582: 'reche', 1583: 'prà', 1584: 'spun', 1585: 'scop', 1586: 'drì', 1587: 'sneb', 1588: 'mot', 1589: 'scov', 1590: 'prec', 1591: 'miche', 1592: 'trar', 1593: 'fug', 1594: 'sian', 1595: 'com', 1596: 'tràs', 1597: 'pres', 1598: 'vexil', 1599: 'piat', 1600: 'lip', 1601: 'nab', 1602: 'dan', 1603: 'ge', 1604: 'me', 1605: ' lie', 1606: 'vian', 1607: 'è', 1608: 'vìd', 1609: 'au', 1610: 'pap', 1611: 'viag', 1612: 'nie', 1613: 'gem', 1614: 'glies', 1615: 'pul', 1616: 'stric', 1617: 'tag', 1618: 'puo', 1619: 'vol', 1620: 'zie', 1621: '   tal', 1622: 'vel', 1623: 'spez', 1624: 'fib', 1625: 'guis', 1626: 'spir', 1627: 'moi', 1628: 'nieghi', 1629: 'nom', 1630: 'ciuf', 1631: 'nez', 1632: 'pac', 1633: 'reb', 1634: 'cè', 1635: 'iar', 1636: 'rut', 1637: 'guen', 1638: 'spin', 1639: 'dil', 1640: 'lì', 1641: 'cred', 1642: 'dez', 1643: 'bo', 1644: 'cut', 1645: 'deb', 1646: 'guiz', 1647: 'sop', 1648: 'gnas', 1649: 'ges', 1650: 'bron', 1651: 'gal', 1652: 'lez', 1653: 'nha', 1654: 'der', 1655: 'uhi', 1656: 'ie', 1657: 'tas', 1658: 'gi', 1659: 'sat', 1660: 'mìs', 1661: 'cac', 1662: 'rui', 1663: ' tuo', 1664: 'sgom', 1665: 'tor', 1666: 'sga', 1667: 'bol', 1668: 'nhan', 1669: 'poz', 1670: 'vagheg', 1671: ' suo', 1672: 'stiam', 1673: 'bet', 1674: ' bi', 1675: ' fi', 1676: 'mei', 1677: 'liev', 1678: 'lat', 1679: 'lichi', 1680: 'grop', 1681: 'dua', 1682: 'gai', 1683: 'ughi', 1684: 'am', 1685: 'strug', 1686: 'tran', 1687: 'tha', 1688: 'piz', 1689: 'gien', 1690: 'lio', 1691: 'pron', 1692: 'strav', 1693: 'io', 1694: 'tras', 1695: 'o', 1696: 'nès', 1697: 'spie', 1698: 'rir', 1699: 'stichi', 1700: 'cèr', 1701: 'gron', 1702: 'tun', 1703: 'trai', 1704: 'nem', 1705: 'sti', 1706: 'ro', 1707: 'gnu', 1708: 'bug', 1709: 'sac', 1710: 'dol', 1711: 'dap', 1712: 'fas', 1713: 'spieghi', 1714: 'mil', 1715: 'sba', 1716: 'blio', 1717: 'ruc', 1718: 'ter', 1719: 'sle', 1720: ' cor', 1721: 'fior', 1722: 'blia', 1723: 'mas', 1724: 'is', 1725: 'sden', 1726: 'plar', 1727: ' rei', 1728: 'gen', 1729: 'li', 1730: 'mor', 1731: 'sie', 1732: 'cien', 1733: 'sod', 1734: 'vai-e', 1735: 'nò', 1736: 'tret', 1737: 'nu', 1738: 'clès', 1739: 'leb', 1740: 'cuor', 1741: 'stio', 1742: 'bro', 1743: 'ciul', 1744: 'sas', 1745: 'cliò', 1746: 'rom', 1747: 'schia', 1748: 'niam', 1749: 'tuas', 1750: 'diè', 1751: 'zien', 1752: 'streg', 1753: 'za', 1754: 'top', 1755: 'guil', 1756: 'stig', 1757: 'squil', 1758: 'gne', 1759: 'nev', 1760: 'sto', 1761: 'sten', 1762: 'scab', 1763: 'luom', 1764: 'uc', 1765: 'vho', 1766: 'diur', 1767: 'zuc', 1768: 'pas', 1769: 'deschi', 1770: 'gag', 1771: 'fec', 1772: 'egyp', 1773: 'càn', 1774: 'tia', 1775: 'svuol', 1776: 'broc', 1777: 'ron', 1778: 'eb', 1779: 'lè', 1780: 'zo', 1781: 'u', 1782: 'ti', 1783: 'vor', 1784: 'sez', 1785: 'giochi', 1786: 'cheg', 1787: 'còb', 1788: 'trui', 1789: 'pr', 1790: 'fom', 1791: 'dri', 1792: 'schic', 1793: 'lab', 1794: 'trag', 1795: 'peschie', 1796: 'pom', 1797: 'pachi', 1798: 'mal', 1799: 'bei', 1800: 'fui', 1801: 'vì', 1802: 'rap', 1803: 'raf', 1804: 'gier', 1805: 'siar', 1806: 'dui', 1807: 'stion', 1808: 'vai', 1809: 'ga', 1810: 'scac', 1811: 'fu', 1812: 'chioc', 1813: 'smoz', 1814: 'cic', 1815: 'fig', 1816: 'lai', 1817: 'tif', 1818: 'bra', 1819: 'sic', 1820: 'caf', 1821: 'bàl', 1822: 'sdet', 1823: 'sciol', 1824: 'chè', 1825: 'già', 1826: 'bèl', 1827: 'fit', 1828: 'buc', 1829: 'im', 1830: 'spar', 1831: 'clu', 1832: 'lhai', 1833: ' ma', 1834: 'ce', 1835: 'stui', 1836: 'bil', 1837: 'fil', 1838: 'scioc', 1839: 'gro', 1840: 'quer', 1841: 'chie', 1842: 'fam', 1843: 'ban', 1844: 'lan', 1845: 'stè', 1846: 'cas', 1847: 'ache', 1848: 'pe', 1849: 'fè', 1850: 'fem', 1851: 'rig', 1852: 'cù', 1853: 'gnem', 1854: 'eschi', 1855: 'lil', 1856: 'lon', 1857: 'sha', 1858: 'vie', 1859: 'lui', 1860: 'spel', 1861: 'tem', 1862: 'dof', 1863: 'stu', 1864: 'muo', 1865: 'dia', 1866: 'muf', 1867: 'ciap', 1868: 'mia', 1869: 'gua', 1870: 'reg', 1871: 'vogl', 1872: 'tuon', 1873: 'prì', 1874: 'lus', 1875: 'gaz', 1876: 'pog', 1877: 'tram', 1878: 'tiam', 1879: 'dov', 1880: 'bab', 1881: 'liche', 1882: 'paghi', 1883: 'sò', 1884: 'sgiun', 1885: 'òs', 1886: 'biz', 1887: 'ghez', 1888: 'gin', 1889: 'lar', 1890: 'lig', 1891: 'piar', 1892: 'sì', 1893: 'spiac', 1894: 'sleghi', 1895: 'pu', 1896: 'zon', 1897: 'gas', 1898: 'dir', 1899: 'làche', 1900: 'dun', 1901: 'fer', 1902: 'gion', 1903: 'ze', 1904: 'goc', 1905: 'dim', 1906: 'sum', 1907: 'fur', 1908: 'tus', 1909: 'dut', 1910: 'glio', 1911: 'ia', 1912: 'mus', 1913: 'fic', 1914: 'scro', 1915: 'rat', 1916: 'pè', 1917: 'làn', 1918: 'brez', 1919: 'nìt', 1920: 'c'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1c3xymeIZdr2",
        "outputId": "49bdf39b-b906-40d6-91b0-79c7ac6245c5"
      },
      "source": [
        "with open('divine_comedy.txt','r', encoding='ISO-8859-1') as f:\r\n",
        "  input_text = f.read()\r\n",
        "\r\n",
        "print(input_text[:250])\r\n",
        "print('\\n\\n[...]\\n\\n')\r\n",
        "print(input_text[-280:])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFERNO\n",
            "\n",
            "- Canto I\n",
            "\n",
            "Nel mezzo del cammin di nostra vita\n",
            "mi ritrovai per una selva oscura,\n",
            "ché la diritta via era smarrita.\n",
            "\n",
            "Ahi quanto a dir qual era è cosa dura\n",
            "esta selva selvaggia e aspra e forte\n",
            "che nel pensier rinova la paura!\n",
            "\n",
            "Tant' è amara che\n",
            "\n",
            "\n",
            "[...]\n",
            "\n",
            "\n",
            "vi s'indova;\n",
            "\n",
            "ma non eran da ciò le proprie penne:\n",
            "se non che la mia mente fu percossa\n",
            "da un fulgore in che sua voglia venne.\n",
            "\n",
            "A l'alta fantasia qui mancò possa;\n",
            "ma già volgeva il mio disio e 'l velle,\n",
            "sì come rota ch'igualmente è mossa,\n",
            "\n",
            "l'amor che move il sole e l'altre stelle.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVGSyiGrazzZ"
      },
      "source": [
        "input_text = input_text.replace(\"ä\", \"a\")\r\n",
        "input_text = input_text.replace(\"é\", \"è\")\r\n",
        "input_text = input_text.replace(\"ë\", \"è\")\r\n",
        "input_text = input_text.replace(\"Ë\", \"E\")\r\n",
        "input_text = input_text.replace(\"ï\", \"i\")\r\n",
        "input_text = input_text.replace(\"Ï\", \"I\")\r\n",
        "input_text = input_text.replace(\"ó\", \"ò\")\r\n",
        "input_text = input_text.replace(\"ö\", \"o\")\r\n",
        "input_text = input_text.replace(\"ü\", \"u\")\r\n",
        "input_text = input_text.replace(\"(\", \"-\")\r\n",
        "input_text = input_text.replace(\")\", \"-\")\r\n",
        "input_text = input_text.replace(\"[\", \"\")\r\n",
        "input_text = input_text.replace(\"]\", \"\")\r\n",
        "input_text = re.sub(r'[0-9]+', '', input_text)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNBVNxarglt7"
      },
      "source": [
        "input_text = re.sub(\r\n",
        "  f'\\n- Canto.*\\n\\n',\r\n",
        "  '',\r\n",
        "  input_text\r\n",
        ")\r\n",
        "\r\n",
        "for name in ['INFERNO', 'PURGATORIO', 'PARADISO']:\r\n",
        "  input_text = re.sub(\r\n",
        "    f'{name}',\r\n",
        "    \"\",\r\n",
        "    input_text\r\n",
        "  )"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3FZ-XKJea0-9",
        "outputId": "bf1c364d-def0-4e69-d576-1c208821a363"
      },
      "source": [
        "input_text = re.sub('\\n\\n\\n','\\n\\n',input_text)\r\n",
        "input_text = syll._strip_punctuaction(input_text.lower())\r\n",
        "input_text_lines = input_text.split('\\n')\r\n",
        "\r\n",
        "print(\"input_text_lines:\")\r\n",
        "print(input_text_lines[0:10])\r\n",
        "print()\r\n",
        "\r\n",
        "input_lines_dataset = []\r\n",
        "for el in input_text_lines:\r\n",
        "  if el != \"\":\r\n",
        "    input_lines_dataset.append(el)\r\n",
        "  else:\r\n",
        "    input_lines_dataset.append('=end_terzine=')\r\n",
        "\r\n",
        "print(\"input_lines_dataset:\")\r\n",
        "print(input_lines_dataset[0:10])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input_text_lines:\n",
            "['', 'nel mezzo del cammin di nostra vita', 'mi ritrovai per una selva oscura', 'chè la diritta via era smarrita', '', 'ahi quanto a dir qual era è cosa dura', 'esta selva selvaggia e aspra e forte', 'che nel pensier rinova la paura', '', 'tant è amara che poco è più morte']\n",
            "\n",
            "input_lines_dataset:\n",
            "['=end_terzine=', 'nel mezzo del cammin di nostra vita', 'mi ritrovai per una selva oscura', 'chè la diritta via era smarrita', '=end_terzine=', 'ahi quanto a dir qual era è cosa dura', 'esta selva selvaggia e aspra e forte', 'che nel pensier rinova la paura', '=end_terzine=', 'tant è amara che poco è più morte']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19pmekCreqbx",
        "outputId": "2889bc53-b26c-491d-fe1b-012eb8c65df8"
      },
      "source": [
        "import copy\r\n",
        "input_data_full = copy.deepcopy(input_lines_dataset)\r\n",
        "for i in range(len(input_lines_dataset)):\r\n",
        "  if len(input_data_full[i]) > 1:\r\n",
        "    input_data_full[i] = '-start-'+ input_data_full[i]\r\n",
        "    input_data_full[i]= input_data_full[i] + '-end-'\r\n",
        "\r\n",
        "for i in range(10):\r\n",
        "  print(input_data_full[i])\r\n",
        "\r\n",
        "one_list_data = [item for sublist in input_data_full for item in sublist]\r\n",
        "one_list_tokenized = [item for sublist in input_data_full for item in sublist]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-start-=end_terzine=-end-\n",
            "-start-nel mezzo del cammin di nostra vita-end-\n",
            "-start-mi ritrovai per una selva oscura-end-\n",
            "-start-chè la diritta via era smarrita-end-\n",
            "-start-=end_terzine=-end-\n",
            "-start-ahi quanto a dir qual era è cosa dura-end-\n",
            "-start-esta selva selvaggia e aspra e forte-end-\n",
            "-start-che nel pensier rinova la paura-end-\n",
            "-start-=end_terzine=-end-\n",
            "-start-tant è amara che poco è più morte-end-\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_p2zN-MQOBB"
      },
      "source": [
        "## MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XfhOp-ZLh7W_",
        "outputId": "a2c685f6-836d-4394-b279-a07357823af6"
      },
      "source": [
        "seq_length = 3\r\n",
        "step_length = 1\r\n",
        "batch_size = 64\r\n",
        "train_val_split = 0.7\r\n",
        "\r\n",
        "tot_samples = int((len(input_data_full) - seq_length) / step_length)\r\n",
        "train_samples = round(tot_samples * train_val_split)\r\n",
        "\r\n",
        "print('Train Samples:', train_samples)\r\n",
        "print('  Val Samples:', tot_samples - train_samples)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Samples: 13261\n",
            "  Val Samples: 5683\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jg6y4X4wY5IN"
      },
      "source": [
        "from tensorflow.data import Dataset\r\n",
        "from tensorflow.strings import reduce_join\r\n",
        "\r\n",
        "def split_input_target(chunk):\r\n",
        "  input_text = reduce_join(chunk[:-1], separator='\\n') + '\\n'\r\n",
        "  target_text = reduce_join(chunk, separator='\\n') + '\\n'\r\n",
        "  return input_text, target_text\r\n",
        "\r\n",
        "dataset = Dataset.from_tensor_slices(input_data_full)\r\n",
        "dataset = dataset.window(seq_length + 1, step_length, drop_remainder=True)\r\n",
        "dataset = dataset.flat_map(lambda window: window.batch(seq_length + 1))\r\n",
        "dataset = dataset.map(split_input_target).shuffle(tot_samples, seed=0)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mr2KUkhdjEQ8",
        "outputId": "ae2be2db-8c8b-401d-e727-30d668541243"
      },
      "source": [
        "import tensorflow_datasets as tfds\r\n",
        "\r\n",
        "tokenizer = tfds.deprecated.text.SubwordTextEncoder(\r\n",
        "    vocab_list=syll_set\r\n",
        ")\r\n",
        "print(tokenizer.vocab_size, 'tokens:')\r\n",
        "print()\r\n",
        "for i, token in enumerate(tokenizer.subwords[:40]):\r\n",
        "  print(\"'{}'\".format('\\\\n' if token == '\\n' else token))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2178 tokens:\n",
            "\n",
            "'bran'\n",
            "'fischio'\n",
            "'om'\n",
            "'truo'\n",
            "'mos'\n",
            "'sgher'\n",
            "'gras'\n",
            "'sul'\n",
            "'mì'\n",
            "'in'\n",
            "'scuo'\n",
            "'som'\n",
            "'zan'\n",
            "'sf'\n",
            "'grem'\n",
            "'stiò'\n",
            "'les'\n",
            "'toschi'\n",
            "'cot'\n",
            "'fio'\n",
            "'spia'\n",
            "'gib'\n",
            "'tav'\n",
            "'piog'\n",
            "'gnen'\n",
            "'pit'\n",
            "'cop'\n",
            "'spuo'\n",
            "'glau'\n",
            "'nen'\n",
            "'cil'\n",
            "'for'\n",
            "'tri'\n",
            "'pluo'\n",
            "'smon'\n",
            "' le'\n",
            "'d'\n",
            "'scher'\n",
            "'sug'\n",
            "'mol'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpYMTsKbhNPk"
      },
      "source": [
        "def encode(input, target):\r\n",
        "  input = [tokenizer.vocab_size] + tokenizer.encode(\r\n",
        "      input.numpy()) + [tokenizer.vocab_size+1]\r\n",
        "\r\n",
        "  target = [tokenizer.vocab_size] + tokenizer.encode(\r\n",
        "      target.numpy()) + [tokenizer.vocab_size+1]\r\n",
        "\r\n",
        "  return input, target"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-SZfa9khNGH"
      },
      "source": [
        "def tf_encode(input, target):\r\n",
        "  result_input, result_target = tf.py_function(encode, [input, target], [tf.int64, tf.int64])\r\n",
        "  result_input.set_shape([None])\r\n",
        "  result_target.set_shape([None])\r\n",
        "\r\n",
        "  return result_input, result_target"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHkMZQBSsar_"
      },
      "source": [
        "def encode_dataset(input_dataset, target_dataset):\r\n",
        "  def encode_sample(input, target):\r\n",
        "    input = [tokenizer.vocab_size] + tokenizer.encode(input.numpy()) + [tokenizer.vocab_size+1]\r\n",
        "    target = [tokenizer.vocab_size] + tokenizer.encode(target.numpy()) + [tokenizer.vocab_size+1]\r\n",
        "    return input, target\r\n",
        "\r\n",
        "  input_dataset, target_dataset = tf.py_function(encode_sample, [input_dataset, target_dataset], [tf.int64, tf.int64])\r\n",
        "  input_dataset.set_shape([None])\r\n",
        "  target_dataset.set_shape([None])\r\n",
        "  return input_dataset, target_dataset\r\n",
        "\r\n",
        "train_dataset = dataset.take(train_samples).map(encode_dataset)\r\n",
        "train_dataset = train_dataset.cache()\r\n",
        "train_dataset = train_dataset.padded_batch(batch_size)\r\n",
        "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\r\n",
        "\r\n",
        "val_dataset = dataset.take(tot_samples - train_samples).map(encode_dataset)\r\n",
        "val_dataset = val_dataset.cache()\r\n",
        "val_dataset = val_dataset.padded_batch(batch_size)\r\n",
        "val_dataset = val_dataset.prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nksudxmOyqPa",
        "outputId": "7af7f737-5be3-4c55-d412-0ddcbe58ac49"
      },
      "source": [
        "for input, target in train_dataset.take(1):\r\n",
        "  input = input.numpy()[0]\r\n",
        "  target = target.numpy()[0]\r\n",
        "\r\n",
        "  print(f'Input  Shape: {input.shape}')\r\n",
        "  print(f'Target Shape: {target.shape}')\r\n",
        "  print()\r\n",
        "  print('INPUT:\\n')\r\n",
        "  print(tokenizer.decode([token for token in input if token < tokenizer.vocab_size]))\r\n",
        "  print('\\n\\n---------------------\\n\\n')\r\n",
        "  print('TARGET:\\n')\r\n",
        "  print(tokenizer.decode([token for token in target if token < tokenizer.vocab_size]))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input  Shape: (79,)\n",
            "Target Shape: (93,)\n",
            "\n",
            "INPUT:\n",
            "\n",
            "-start-non vi dispiaccia se vi lece dirci-end-\n",
            "-start-sa la man destra giace alcuna foce-end-\n",
            "-start-=end_terzine=-end-\n",
            "\n",
            "\n",
            "\n",
            "---------------------\n",
            "\n",
            "\n",
            "TARGET:\n",
            "\n",
            "-start-non vi dispiaccia se vi lece dirci-end-\n",
            "-start-sa la man destra giace alcuna foce-end-\n",
            "-start-=end_terzine=-end-\n",
            "-start-onde noi amendue possiamo uscirci-end-\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2US01OUWr0A"
      },
      "source": [
        "def get_angles(pos, i, d_model):\r\n",
        "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\r\n",
        "  return pos * angle_rates"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGgDROJyXfmL"
      },
      "source": [
        "def positional_encoding(position, d_model):\r\n",
        "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\r\n",
        "                          np.arange(d_model)[np.newaxis, :],\r\n",
        "                          d_model)\r\n",
        "\r\n",
        "  # apply sin to even indices in the array; 2i\r\n",
        "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\r\n",
        "\r\n",
        "  # apply cos to odd indices in the array; 2i+1\r\n",
        "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\r\n",
        "\r\n",
        "  pos_encoding = angle_rads[np.newaxis, ...]\r\n",
        "\r\n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "ME0CswwqXjK2",
        "outputId": "941e304c-46ed-4c05-aacb-0c55bb88b2b6"
      },
      "source": [
        "pos_encoding = positional_encoding(50, 512)\r\n",
        "print (pos_encoding.shape)\r\n",
        "\r\n",
        "plt.pcolormesh(pos_encoding[0], cmap='RdBu')\r\n",
        "plt.xlabel('Depth')\r\n",
        "plt.xlim((0, 512))\r\n",
        "plt.ylabel('Position')\r\n",
        "plt.colorbar()\r\n",
        "plt.show()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 50, 512)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NnSQB-FXwjj"
      },
      "source": [
        "def create_padding_mask(seq):\r\n",
        "  seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\r\n",
        "\r\n",
        "  # add extra dimensions to add the padding\r\n",
        "  # to the attention logits.\r\n",
        "  return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0QKaX6cX2ly",
        "outputId": "a6dc91c0-7004-4c79-edcb-af46f8c265bb"
      },
      "source": [
        "x = tf.constant([[7, 6, 0, 0, 1], [1, 2, 3, 0, 0], [0, 0, 0, 4, 5]])\r\n",
        "create_padding_mask(x)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 1, 1, 5), dtype=float32, numpy=\n",
              "array([[[[0., 0., 1., 1., 0.]]],\n",
              "\n",
              "\n",
              "       [[[0., 0., 0., 1., 1.]]],\n",
              "\n",
              "\n",
              "       [[[1., 1., 1., 0., 0.]]]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdrU3AIJX4EC"
      },
      "source": [
        "def create_look_ahead_mask(size):\r\n",
        "  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\r\n",
        "  return mask  # (seq_len, seq_len)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQYgvqd6X6Qy",
        "outputId": "fb647a5b-39d1-4e40-e593-e6550256021c"
      },
      "source": [
        "x = tf.random.uniform((1, 3))\r\n",
        "temp = create_look_ahead_mask(x.shape[1])\r\n",
        "temp"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
              "array([[0., 1., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 0.]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rg5EiziSX7bP"
      },
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\r\n",
        "  \"\"\"Calculate the attention weights.\r\n",
        "  q, k, v must have matching leading dimensions.\r\n",
        "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\r\n",
        "  The mask has different shapes depending on its type(padding or look ahead) \r\n",
        "  but it must be broadcastable for addition.\r\n",
        "\r\n",
        "  Args:\r\n",
        "    q: query shape == (..., seq_len_q, depth)\r\n",
        "    k: key shape == (..., seq_len_k, depth)\r\n",
        "    v: value shape == (..., seq_len_v, depth_v)\r\n",
        "    mask: Float tensor with shape broadcastable \r\n",
        "          to (..., seq_len_q, seq_len_k). Defaults to None.\r\n",
        "\r\n",
        "  Returns:\r\n",
        "    output, attention_weights\r\n",
        "  \"\"\"\r\n",
        "\r\n",
        "  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\r\n",
        "\r\n",
        "  # scale matmul_qk\r\n",
        "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\r\n",
        "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\r\n",
        "\r\n",
        "  # add the mask to the scaled tensor.\r\n",
        "  if mask is not None:\r\n",
        "    scaled_attention_logits += (mask * -1e9)  \r\n",
        "\r\n",
        "  # softmax is normalized on the last axis (seq_len_k) so that the scores\r\n",
        "  # add up to 1.\r\n",
        "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\r\n",
        "\r\n",
        "  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\r\n",
        "\r\n",
        "  return output, attention_weights"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSJ0L3vuX9xI"
      },
      "source": [
        "def print_out(q, k, v):\r\n",
        "  temp_out, temp_attn = scaled_dot_product_attention(\r\n",
        "      q, k, v, None)\r\n",
        "  print ('Attention weights are:')\r\n",
        "  print (temp_attn)\r\n",
        "  print ('Output is:')\r\n",
        "  print (temp_out)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PuOb8bNLX_MY",
        "outputId": "c35d203a-34a3-4aad-b0d9-4c0f8498049f"
      },
      "source": [
        "np.set_printoptions(suppress=True)\r\n",
        "\r\n",
        "temp_k = tf.constant([[10,0,0],\r\n",
        "                      [0,10,0],\r\n",
        "                      [0,0,10],\r\n",
        "                      [0,0,10]], dtype=tf.float32)  # (4, 3)\r\n",
        "\r\n",
        "temp_v = tf.constant([[   1,0],\r\n",
        "                      [  10,0],\r\n",
        "                      [ 100,5],\r\n",
        "                      [1000,6]], dtype=tf.float32)  # (4, 2)\r\n",
        "\r\n",
        "# This `query` aligns with the second `key`,\r\n",
        "# so the second `value` is returned.\r\n",
        "temp_q = tf.constant([[0, 10, 0]], dtype=tf.float32)  # (1, 3)\r\n",
        "print_out(temp_q, temp_k, temp_v)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention weights are:\n",
            "tf.Tensor([[0. 1. 0. 0.]], shape=(1, 4), dtype=float32)\n",
            "Output is:\n",
            "tf.Tensor([[10.  0.]], shape=(1, 2), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjvAU-f4YBHB",
        "outputId": "72d31cf8-982a-4b9b-fce2-3cecfef65eac"
      },
      "source": [
        "# This query aligns with a repeated key (third and fourth), \r\n",
        "# so all associated values get averaged.\r\n",
        "temp_q = tf.constant([[0, 0, 10]], dtype=tf.float32)  # (1, 3)\r\n",
        "print_out(temp_q, temp_k, temp_v)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention weights are:\n",
            "tf.Tensor([[0.  0.  0.5 0.5]], shape=(1, 4), dtype=float32)\n",
            "Output is:\n",
            "tf.Tensor([[550.    5.5]], shape=(1, 2), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEBcUNMQYC6A",
        "outputId": "debbb58e-6cf2-46e3-ade3-5d7029e8214b"
      },
      "source": [
        "# This query aligns equally with the first and second key, \r\n",
        "# so their values get averaged.\r\n",
        "temp_q = tf.constant([[10, 10, 0]], dtype=tf.float32)  # (1, 3)\r\n",
        "print_out(temp_q, temp_k, temp_v)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention weights are:\n",
            "tf.Tensor([[0.5 0.5 0.  0. ]], shape=(1, 4), dtype=float32)\n",
            "Output is:\n",
            "tf.Tensor([[5.5 0. ]], shape=(1, 2), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIvFB2ANYGx3",
        "outputId": "4c84c0e2-ff80-4b70-99d4-313b24ae1cb7"
      },
      "source": [
        "temp_q = tf.constant([[0, 0, 10], [0, 10, 0], [10, 10, 0]], dtype=tf.float32)  # (3, 3)\r\n",
        "print_out(temp_q, temp_k, temp_v)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention weights are:\n",
            "tf.Tensor(\n",
            "[[0.  0.  0.5 0.5]\n",
            " [0.  1.  0.  0. ]\n",
            " [0.5 0.5 0.  0. ]], shape=(3, 4), dtype=float32)\n",
            "Output is:\n",
            "tf.Tensor(\n",
            "[[550.    5.5]\n",
            " [ 10.    0. ]\n",
            " [  5.5   0. ]], shape=(3, 2), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0mGI0RkYIUX"
      },
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\r\n",
        "  def __init__(self, d_model, num_heads):\r\n",
        "    super(MultiHeadAttention, self).__init__()\r\n",
        "    self.num_heads = num_heads\r\n",
        "    self.d_model = d_model\r\n",
        "\r\n",
        "    assert d_model % self.num_heads == 0\r\n",
        "\r\n",
        "    self.depth = d_model // self.num_heads\r\n",
        "\r\n",
        "    self.wq = tf.keras.layers.Dense(d_model)\r\n",
        "    self.wk = tf.keras.layers.Dense(d_model)\r\n",
        "    self.wv = tf.keras.layers.Dense(d_model)\r\n",
        "\r\n",
        "    self.dense = tf.keras.layers.Dense(d_model)\r\n",
        "\r\n",
        "  def split_heads(self, x, batch_size):\r\n",
        "    \"\"\"Split the last dimension into (num_heads, depth).\r\n",
        "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\r\n",
        "    \"\"\"\r\n",
        "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\r\n",
        "    return tf.transpose(x, perm=[0, 2, 1, 3])\r\n",
        "\r\n",
        "  def call(self, v, k, q, mask):\r\n",
        "    batch_size = tf.shape(q)[0]\r\n",
        "\r\n",
        "    q = self.wq(q)  # (batch_size, seq_len, d_model)\r\n",
        "    k = self.wk(k)  # (batch_size, seq_len, d_model)\r\n",
        "    v = self.wv(v)  # (batch_size, seq_len, d_model)\r\n",
        "\r\n",
        "    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\r\n",
        "    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\r\n",
        "    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\r\n",
        "\r\n",
        "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\r\n",
        "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\r\n",
        "    scaled_attention, attention_weights = scaled_dot_product_attention(\r\n",
        "        q, k, v, mask)\r\n",
        "\r\n",
        "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\r\n",
        "\r\n",
        "    concat_attention = tf.reshape(scaled_attention, \r\n",
        "                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\r\n",
        "\r\n",
        "    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\r\n",
        "\r\n",
        "    return output, attention_weights"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TmfivnlfYKIx",
        "outputId": "705514b8-93ca-4580-ece9-f792f7323595"
      },
      "source": [
        "temp_mha = MultiHeadAttention(d_model=512, num_heads=8)\r\n",
        "y = tf.random.uniform((1, 60, 512))  # (batch_size, encoder_sequence, d_model)\r\n",
        "out, attn = temp_mha(y, k=y, q=y, mask=None)\r\n",
        "out.shape, attn.shape"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([1, 60, 512]), TensorShape([1, 8, 60, 60]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFr-C5pqYLZB"
      },
      "source": [
        "def point_wise_feed_forward_network(d_model, dff):\r\n",
        "  return tf.keras.Sequential([\r\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\r\n",
        "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\r\n",
        "  ])"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqShZx9MYNsQ",
        "outputId": "6f8628c8-60e9-4af5-ffb4-4b4ca9191f6d"
      },
      "source": [
        "sample_ffn = point_wise_feed_forward_network(512, 2048)\r\n",
        "sample_ffn(tf.random.uniform((64, 50, 512))).shape"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([64, 50, 512])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3WKpUpsYOn4"
      },
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\r\n",
        "  def __init__(self, d_model, num_heads, dff, rate=0.1):\r\n",
        "    super(EncoderLayer, self).__init__()\r\n",
        "\r\n",
        "    self.mha = MultiHeadAttention(d_model, num_heads)\r\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\r\n",
        "\r\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\r\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\r\n",
        "\r\n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\r\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\r\n",
        "\r\n",
        "  def call(self, x, training, mask):\r\n",
        "\r\n",
        "    attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\r\n",
        "    attn_output = self.dropout1(attn_output, training=training)\r\n",
        "    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\r\n",
        "\r\n",
        "    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\r\n",
        "    ffn_output = self.dropout2(ffn_output, training=training)\r\n",
        "    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\r\n",
        "\r\n",
        "    return out2"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fd4EkykiYQUY",
        "outputId": "1720869b-8798-486a-e49f-265ecdcfc25d"
      },
      "source": [
        "sample_encoder_layer = EncoderLayer(512, 8, 2048)\r\n",
        "\r\n",
        "sample_encoder_layer_output = sample_encoder_layer(\r\n",
        "    tf.random.uniform((64, 43, 512)), False, None)\r\n",
        "\r\n",
        "sample_encoder_layer_output.shape  # (batch_size, input_seq_len, d_model)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([64, 43, 512])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cmg2G6mYRuI"
      },
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\r\n",
        "  def __init__(self, d_model, num_heads, dff, rate=0.1):\r\n",
        "    super(DecoderLayer, self).__init__()\r\n",
        "\r\n",
        "    self.mha1 = MultiHeadAttention(d_model, num_heads)\r\n",
        "    self.mha2 = MultiHeadAttention(d_model, num_heads)\r\n",
        "\r\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\r\n",
        "\r\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\r\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\r\n",
        "    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\r\n",
        "\r\n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\r\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\r\n",
        "    self.dropout3 = tf.keras.layers.Dropout(rate)\r\n",
        "\r\n",
        "\r\n",
        "  def call(self, x, enc_output, training, \r\n",
        "           look_ahead_mask, padding_mask):\r\n",
        "    # enc_output.shape == (batch_size, input_seq_len, d_model)\r\n",
        "\r\n",
        "    attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\r\n",
        "    attn1 = self.dropout1(attn1, training=training)\r\n",
        "    out1 = self.layernorm1(attn1 + x)\r\n",
        "\r\n",
        "    attn2, attn_weights_block2 = self.mha2(\r\n",
        "        enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\r\n",
        "    attn2 = self.dropout2(attn2, training=training)\r\n",
        "    out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\r\n",
        "\r\n",
        "    ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\r\n",
        "    ffn_output = self.dropout3(ffn_output, training=training)\r\n",
        "    out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\r\n",
        "\r\n",
        "    return out3, attn_weights_block1, attn_weights_block2"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iub_uyzeYTzJ",
        "outputId": "f16cb713-9d65-4a96-f46a-668dd8756c08"
      },
      "source": [
        "sample_decoder_layer = DecoderLayer(512, 8, 2048)\r\n",
        "\r\n",
        "sample_decoder_layer_output, _, _ = sample_decoder_layer(\r\n",
        "    tf.random.uniform((64, 50, 512)), sample_encoder_layer_output, \r\n",
        "    False, None, None)\r\n",
        "\r\n",
        "sample_decoder_layer_output.shape  # (batch_size, target_seq_len, d_model)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([64, 50, 512])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qCK5Z-eYVTp"
      },
      "source": [
        "class Encoder(tf.keras.layers.Layer):\r\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\r\n",
        "               maximum_position_encoding, rate=0.1):\r\n",
        "    super(Encoder, self).__init__()\r\n",
        "\r\n",
        "    self.d_model = d_model\r\n",
        "    self.num_layers = num_layers\r\n",
        "\r\n",
        "    self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\r\n",
        "    self.pos_encoding = positional_encoding(maximum_position_encoding, \r\n",
        "                                            self.d_model)\r\n",
        "\r\n",
        "\r\n",
        "    self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \r\n",
        "                       for _ in range(num_layers)]\r\n",
        "\r\n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\r\n",
        "\r\n",
        "  def call(self, x, training, mask):\r\n",
        "\r\n",
        "    seq_len = tf.shape(x)[1]\r\n",
        "\r\n",
        "    # adding embedding and position encoding.\r\n",
        "    x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\r\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\r\n",
        "    x += self.pos_encoding[:, :seq_len, :]\r\n",
        "\r\n",
        "    x = self.dropout(x, training=training)\r\n",
        "\r\n",
        "    for i in range(self.num_layers):\r\n",
        "      x = self.enc_layers[i](x, training, mask)\r\n",
        "\r\n",
        "    return x  # (batch_size, input_seq_len, d_model)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_gcHBoX7YXPp",
        "outputId": "4dc61e32-996e-4e79-a396-a837aa5f9fc3"
      },
      "source": [
        "sample_encoder = Encoder(num_layers=2, d_model=512, num_heads=8, \r\n",
        "                         dff=2048, input_vocab_size=8500,\r\n",
        "                         maximum_position_encoding=10000)\r\n",
        "temp_input = tf.random.uniform((64, 62), dtype=tf.int64, minval=0, maxval=200)\r\n",
        "\r\n",
        "sample_encoder_output = sample_encoder(temp_input, training=False, mask=None)\r\n",
        "\r\n",
        "print (sample_encoder_output.shape)  # (batch_size, input_seq_len, d_model)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 62, 512)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1VlkhLuYYQo"
      },
      "source": [
        "class Decoder(tf.keras.layers.Layer):\r\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\r\n",
        "               maximum_position_encoding, rate=0.1):\r\n",
        "    super(Decoder, self).__init__()\r\n",
        "\r\n",
        "    self.d_model = d_model\r\n",
        "    self.num_layers = num_layers\r\n",
        "\r\n",
        "    self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\r\n",
        "    self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\r\n",
        "\r\n",
        "    self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \r\n",
        "                       for _ in range(num_layers)]\r\n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\r\n",
        "\r\n",
        "  def call(self, x, enc_output, training, \r\n",
        "           look_ahead_mask, padding_mask):\r\n",
        "\r\n",
        "    seq_len = tf.shape(x)[1]\r\n",
        "    attention_weights = {}\r\n",
        "\r\n",
        "    x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\r\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\r\n",
        "    x += self.pos_encoding[:, :seq_len, :]\r\n",
        "\r\n",
        "    x = self.dropout(x, training=training)\r\n",
        "\r\n",
        "    for i in range(self.num_layers):\r\n",
        "      x, block1, block2 = self.dec_layers[i](x, enc_output, training,\r\n",
        "                                             look_ahead_mask, padding_mask)\r\n",
        "\r\n",
        "      attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\r\n",
        "      attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\r\n",
        "\r\n",
        "    # x.shape == (batch_size, target_seq_len, d_model)\r\n",
        "    return x, attention_weights"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVp1X_FeYaPg",
        "outputId": "e64ebcf5-dd00-49ed-c605-da8e8b83d1ed"
      },
      "source": [
        "sample_decoder = Decoder(num_layers=2, d_model=512, num_heads=8, \r\n",
        "                         dff=2048, target_vocab_size=8000,\r\n",
        "                         maximum_position_encoding=5000)\r\n",
        "temp_input = tf.random.uniform((64, 26), dtype=tf.int64, minval=0, maxval=200)\r\n",
        "\r\n",
        "output, attn = sample_decoder(temp_input, \r\n",
        "                              enc_output=sample_encoder_output, \r\n",
        "                              training=False,\r\n",
        "                              look_ahead_mask=None, \r\n",
        "                              padding_mask=None)\r\n",
        "\r\n",
        "output.shape, attn['decoder_layer2_block2'].shape"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 26, 512]), TensorShape([64, 8, 26, 62]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8MkuTCnYbmp"
      },
      "source": [
        "class Transformer(tf.keras.Model):\r\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \r\n",
        "               target_vocab_size, pe_input, pe_target, rate=0.1):\r\n",
        "    super(Transformer, self).__init__()\r\n",
        "\r\n",
        "    self.encoder = Encoder(num_layers, d_model, num_heads, dff, \r\n",
        "                           input_vocab_size, pe_input, rate)\r\n",
        "\r\n",
        "    self.decoder = Decoder(num_layers, d_model, num_heads, dff, \r\n",
        "                           target_vocab_size, pe_target, rate)\r\n",
        "\r\n",
        "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\r\n",
        "\r\n",
        "  def call(self, inp, tar, training, enc_padding_mask, \r\n",
        "           look_ahead_mask, dec_padding_mask):\r\n",
        "\r\n",
        "    enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\r\n",
        "\r\n",
        "    # dec_output.shape == (batch_size, tar_seq_len, d_model)\r\n",
        "    dec_output, attention_weights = self.decoder(\r\n",
        "        tar, enc_output, training, look_ahead_mask, dec_padding_mask)\r\n",
        "\r\n",
        "    final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\r\n",
        "\r\n",
        "    return final_output, attention_weights"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXBjdJ5QYgow"
      },
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\r\n",
        "  def __init__(self, d_model, warmup_steps=4000):\r\n",
        "    super(CustomSchedule, self).__init__()\r\n",
        "\r\n",
        "    self.d_model = d_model\r\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\r\n",
        "\r\n",
        "    self.warmup_steps = warmup_steps\r\n",
        "\r\n",
        "  def __call__(self, step):\r\n",
        "    arg1 = tf.math.rsqrt(step)\r\n",
        "    arg2 = step * (self.warmup_steps ** -1.5)\r\n",
        "\r\n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgtcYzPLYfIy"
      },
      "source": [
        "from tensorflow.keras.optimizers import Adam\r\n",
        "\r\n",
        "num_layers = 4\r\n",
        "num_heads = 4\r\n",
        "d_model = 256\r\n",
        "dff = 512\r\n",
        "dropout = 0.2\r\n",
        "input_vocab_size = tokenizer.vocab_size + 2\r\n",
        "target_vocab_size = tokenizer.vocab_size + 2\r\n",
        "optimizer = Adam(CustomSchedule(d_model), beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4SMak-dYnq6"
      },
      "source": [
        "learning_rate = CustomSchedule(d_model)\r\n",
        "\r\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \r\n",
        "                                     epsilon=1e-9)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "AP88eppqYpFI",
        "outputId": "13cc850c-3f8b-4317-b5d3-7efd1403fec0"
      },
      "source": [
        "temp_learning_rate_schedule = CustomSchedule(d_model)\r\n",
        "\r\n",
        "plt.plot(temp_learning_rate_schedule(tf.range(40000, dtype=tf.float32)))\r\n",
        "plt.ylabel(\"Learning Rate\")\r\n",
        "plt.xlabel(\"Train Step\")"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'Train Step')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCStVz76YqkI"
      },
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\r\n",
        "    from_logits=True, reduction='none')"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHpX1dUIYsCA"
      },
      "source": [
        "def loss_function(real, pred):\r\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\r\n",
        "  loss_ = loss_object(real, pred)\r\n",
        "\r\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\r\n",
        "  loss_ *= mask\r\n",
        "\r\n",
        "  return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\r\n",
        "\r\n",
        "\r\n",
        "def accuracy_function(real, pred):\r\n",
        "  accuracies = tf.equal(real, tf.argmax(pred, axis=2))\r\n",
        "\r\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\r\n",
        "  accuracies = tf.math.logical_and(mask, accuracies)\r\n",
        "\r\n",
        "  accuracies = tf.cast(accuracies, dtype=tf.float32)\r\n",
        "  mask = tf.cast(mask, dtype=tf.float32)\r\n",
        "  return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tXPChC0YtNI"
      },
      "source": [
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\r\n",
        "train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55rP-kEzYu0g"
      },
      "source": [
        "transformer = Transformer(num_layers, d_model, num_heads, dff,\r\n",
        "                          input_vocab_size, target_vocab_size, \r\n",
        "                          pe_input=input_vocab_size, \r\n",
        "                          pe_target=target_vocab_size,\r\n",
        "                          rate=dropout)\r\n",
        "\r\n"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wjm3fe30Y_16"
      },
      "source": [
        "def create_masks(inp, tar):\r\n",
        "  # Encoder padding mask\r\n",
        "  enc_padding_mask = create_padding_mask(inp)\r\n",
        "\r\n",
        "  # Used in the 2nd attention block in the decoder.\r\n",
        "  # This padding mask is used to mask the encoder outputs.\r\n",
        "  dec_padding_mask = create_padding_mask(inp)\r\n",
        "\r\n",
        "  # Used in the 1st attention block in the decoder.\r\n",
        "  # It is used to pad and mask future tokens in the input received by \r\n",
        "  # the decoder.\r\n",
        "  look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\r\n",
        "  dec_target_padding_mask = create_padding_mask(tar)\r\n",
        "  combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\r\n",
        "\r\n",
        "  return enc_padding_mask, combined_mask, dec_padding_mask"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gjpi_jmNZMoS"
      },
      "source": [
        "checkpoint_path = \"./checkpoints/train\"\r\n",
        "\r\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer,\r\n",
        "                           optimizer=optimizer)\r\n",
        "\r\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\r\n",
        "\r\n",
        "# if a checkpoint exists, restore the latest checkpoint.\r\n",
        "if ckpt_manager.latest_checkpoint:\r\n",
        "  ckpt.restore(ckpt_manager.latest_checkpoint)\r\n",
        "  print ('Latest checkpoint restored!!')"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SURGxB6rZOTS"
      },
      "source": [
        "EPOCHS = 120"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PZkCzOqlF3-"
      },
      "source": [
        "history = {'train loss': [], 'train acc': [], 'val loss': [], 'val acc': []}\r\n",
        "\r\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\r\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\r\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\r\n",
        "val_loss = tf.keras.metrics.Mean(name='val_loss')\r\n",
        "val_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='val_accuracy')\r\n",
        "\r\n",
        "def loss_function(real, pred):\r\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\r\n",
        "    loss_ = loss_object(real, pred)\r\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\r\n",
        "    loss_ *= mask\r\n",
        "    return tf.reduce_sum(loss_) / tf.reduce_sum(mask)\r\n",
        "\r\n",
        "step_signature = [\r\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\r\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\r\n",
        "]\r\n",
        "\r\n",
        "@tf.function(input_signature=step_signature)\r\n",
        "def train_step(inp, tar):\r\n",
        "  tar_inp = tar[:, :-1]\r\n",
        "  tar_real = tar[:, 1:]\r\n",
        "  \r\n",
        "  enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\r\n",
        "  with tf.GradientTape() as tape:\r\n",
        "    predictions, _ = transformer(inp, tar_inp, True, enc_padding_mask, combined_mask, dec_padding_mask)\r\n",
        "    loss = loss_function(tar_real, predictions)\r\n",
        "\r\n",
        "  gradients = tape.gradient(loss, transformer.trainable_variables)    \r\n",
        "  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\r\n",
        "  \r\n",
        "  train_loss(loss)\r\n",
        "  train_accuracy(tar_real, predictions)\r\n",
        "\r\n",
        "@tf.function(input_signature=step_signature)\r\n",
        "def val_step(inp, tar):\r\n",
        "  tar_inp = tar[:, :-1]\r\n",
        "  tar_real = tar[:, 1:]\r\n",
        "  \r\n",
        "  enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\r\n",
        "  with tf.GradientTape() as tape:\r\n",
        "    predictions, _ = transformer(inp, tar_inp, True, enc_padding_mask, combined_mask, dec_padding_mask)\r\n",
        "    loss = loss_function(tar_real, predictions)\r\n",
        "\r\n",
        "  val_loss(loss)\r\n",
        "  val_accuracy(tar_real, predictions)"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U53IfG-cZS_o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61c89c8c-977b-4371-beae-dd67a263c8ac"
      },
      "source": [
        "for epoch in range(EPOCHS):\r\n",
        "  start = time.time()\r\n",
        "\r\n",
        "  train_loss.reset_states()\r\n",
        "  train_accuracy.reset_states()\r\n",
        "\r\n",
        "  val_loss.reset_states()\r\n",
        "  val_accuracy.reset_states()\r\n",
        "  print(f'Starting Epoch {epoch+1}/{EPOCHS}')\r\n",
        "\r\n",
        "  for (batch, (inp, tar)) in enumerate(train_dataset):\r\n",
        "    train_step(inp, tar)\r\n",
        "\r\n",
        "    if (batch + 1) % 50 == 0:\r\n",
        "      print(f'  > Batch {batch+1}', end=' \\t\\t ')\r\n",
        "      print(f'- train_loss: {train_loss.result():.4f} - train_acc: {train_accuracy.result():.4f}')      \r\n",
        "  history['train loss'].append(train_loss.result())\r\n",
        "  history['train acc'].append(train_accuracy.result())\r\n",
        "  \r\n",
        "  for (batch, (inp, tar)) in enumerate(val_dataset):\r\n",
        "    val_step(inp, tar)  \r\n",
        "  history['val loss'].append(val_loss.result())\r\n",
        "  history['val acc'].append(val_accuracy.result())\r\n",
        "\r\n",
        "  elapsed = time.time() - start\r\n",
        "  print(f'Ending Epoch {epoch+1}/{EPOCHS}', end=' \\t ')\r\n",
        "\r\n",
        "  if (epoch + 1) % 5 == 0:\r\n",
        "    ckpt_save_path = ckpt_manager.save()\r\n",
        "    print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\r\n",
        "                                                         ckpt_save_path))\r\n",
        "\r\n",
        "  print(f'- train_loss: {history[\"train loss\"][-1]:.4f} - train_acc: {history[\"train acc\"][-1]:.4f}', end=' ')\r\n",
        "  print(f'- val_loss: {history[\"val loss\"][-1]:.4f} - val_acc: {history[\"val acc\"][-1]:.4f}')\r\n",
        "\r\n",
        "  print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting Epoch 1/100\n",
            "  > Batch 50 \t\t - train_loss: 0.4274 - train_acc: 0.8150\n",
            "  > Batch 100 \t\t - train_loss: 0.4269 - train_acc: 0.8183\n",
            "  > Batch 150 \t\t - train_loss: 0.4279 - train_acc: 0.8195\n",
            "  > Batch 200 \t\t - train_loss: 0.4262 - train_acc: 0.8199\n",
            "Ending Epoch 1/100 \t - train_loss: 0.4256 - train_acc: 0.8201 - val_loss: 0.4395 - val_acc: 0.8196\n",
            "Time taken for 1 epoch: 44.50752782821655 secs\n",
            "\n",
            "Starting Epoch 2/100\n",
            "  > Batch 50 \t\t - train_loss: 0.4178 - train_acc: 0.8158\n",
            "  > Batch 100 \t\t - train_loss: 0.4173 - train_acc: 0.8192\n",
            "  > Batch 150 \t\t - train_loss: 0.4176 - train_acc: 0.8206\n",
            "  > Batch 200 \t\t - train_loss: 0.4162 - train_acc: 0.8211\n",
            "Ending Epoch 2/100 \t - train_loss: 0.4155 - train_acc: 0.8212 - val_loss: 0.4318 - val_acc: 0.8204\n",
            "Time taken for 1 epoch: 35.70273399353027 secs\n",
            "\n",
            "Starting Epoch 3/100\n",
            "  > Batch 50 \t\t - train_loss: 0.4083 - train_acc: 0.8172\n",
            "  > Batch 100 \t\t - train_loss: 0.4082 - train_acc: 0.8205\n",
            "  > Batch 150 \t\t - train_loss: 0.4080 - train_acc: 0.8219\n",
            "  > Batch 200 \t\t - train_loss: 0.4063 - train_acc: 0.8224\n",
            "Ending Epoch 3/100 \t - train_loss: 0.4056 - train_acc: 0.8225 - val_loss: 0.4227 - val_acc: 0.8218\n",
            "Time taken for 1 epoch: 35.83530831336975 secs\n",
            "\n",
            "Starting Epoch 4/100\n",
            "  > Batch 50 \t\t - train_loss: 0.3990 - train_acc: 0.8182\n",
            "  > Batch 100 \t\t - train_loss: 0.3988 - train_acc: 0.8216\n",
            "  > Batch 150 \t\t - train_loss: 0.3991 - train_acc: 0.8230\n",
            "  > Batch 200 \t\t - train_loss: 0.3974 - train_acc: 0.8235\n",
            "Ending Epoch 4/100 \t - train_loss: 0.3965 - train_acc: 0.8237 - val_loss: 0.4190 - val_acc: 0.8221\n",
            "Time taken for 1 epoch: 35.98520588874817 secs\n",
            "\n",
            "Starting Epoch 5/100\n",
            "  > Batch 50 \t\t - train_loss: 0.3899 - train_acc: 0.8192\n",
            "  > Batch 100 \t\t - train_loss: 0.3889 - train_acc: 0.8226\n",
            "  > Batch 150 \t\t - train_loss: 0.3894 - train_acc: 0.8242\n",
            "  > Batch 200 \t\t - train_loss: 0.3876 - train_acc: 0.8248\n",
            "Ending Epoch 5/100 \t Saving checkpoint for epoch 5 at ./checkpoints/train/ckpt-5\n",
            "- train_loss: 0.3867 - train_acc: 0.8249 - val_loss: 0.4091 - val_acc: 0.8239\n",
            "Time taken for 1 epoch: 36.02432608604431 secs\n",
            "\n",
            "Starting Epoch 6/100\n",
            "  > Batch 50 \t\t - train_loss: 0.3797 - train_acc: 0.8206\n",
            "  > Batch 100 \t\t - train_loss: 0.3789 - train_acc: 0.8239\n",
            "  > Batch 150 \t\t - train_loss: 0.3796 - train_acc: 0.8254\n",
            "  > Batch 200 \t\t - train_loss: 0.3785 - train_acc: 0.8258\n",
            "Ending Epoch 6/100 \t - train_loss: 0.3776 - train_acc: 0.8259 - val_loss: 0.4079 - val_acc: 0.8239\n",
            "Time taken for 1 epoch: 35.87545847892761 secs\n",
            "\n",
            "Starting Epoch 7/100\n",
            "  > Batch 50 \t\t - train_loss: 0.3717 - train_acc: 0.8217\n",
            "  > Batch 100 \t\t - train_loss: 0.3714 - train_acc: 0.8250\n",
            "  > Batch 150 \t\t - train_loss: 0.3719 - train_acc: 0.8265\n",
            "  > Batch 200 \t\t - train_loss: 0.3702 - train_acc: 0.8270\n",
            "Ending Epoch 7/100 \t - train_loss: 0.3693 - train_acc: 0.8272 - val_loss: 0.3988 - val_acc: 0.8255\n",
            "Time taken for 1 epoch: 35.87754440307617 secs\n",
            "\n",
            "Starting Epoch 8/100\n",
            "  > Batch 50 \t\t - train_loss: 0.3614 - train_acc: 0.8228\n",
            "  > Batch 100 \t\t - train_loss: 0.3618 - train_acc: 0.8263\n",
            "  > Batch 150 \t\t - train_loss: 0.3624 - train_acc: 0.8277\n",
            "  > Batch 200 \t\t - train_loss: 0.3606 - train_acc: 0.8283\n",
            "Ending Epoch 8/100 \t - train_loss: 0.3596 - train_acc: 0.8284 - val_loss: 0.3932 - val_acc: 0.8262\n",
            "Time taken for 1 epoch: 35.79929041862488 secs\n",
            "\n",
            "Starting Epoch 9/100\n",
            "  > Batch 50 \t\t - train_loss: 0.3521 - train_acc: 0.8244\n",
            "  > Batch 100 \t\t - train_loss: 0.3528 - train_acc: 0.8275\n",
            "  > Batch 150 \t\t - train_loss: 0.3531 - train_acc: 0.8289\n",
            "  > Batch 200 \t\t - train_loss: 0.3521 - train_acc: 0.8293\n",
            "Ending Epoch 9/100 \t - train_loss: 0.3509 - train_acc: 0.8295 - val_loss: 0.3900 - val_acc: 0.8268\n",
            "Time taken for 1 epoch: 35.72359108924866 secs\n",
            "\n",
            "Starting Epoch 10/100\n",
            "  > Batch 50 \t\t - train_loss: 0.3461 - train_acc: 0.8249\n",
            "  > Batch 100 \t\t - train_loss: 0.3459 - train_acc: 0.8283\n",
            "  > Batch 150 \t\t - train_loss: 0.3462 - train_acc: 0.8298\n",
            "  > Batch 200 \t\t - train_loss: 0.3446 - train_acc: 0.8302\n",
            "Ending Epoch 10/100 \t Saving checkpoint for epoch 10 at ./checkpoints/train/ckpt-6\n",
            "- train_loss: 0.3434 - train_acc: 0.8304 - val_loss: 0.3863 - val_acc: 0.8274\n",
            "Time taken for 1 epoch: 36.04425358772278 secs\n",
            "\n",
            "Starting Epoch 11/100\n",
            "  > Batch 50 \t\t - train_loss: 0.3376 - train_acc: 0.8263\n",
            "  > Batch 100 \t\t - train_loss: 0.3379 - train_acc: 0.8294\n",
            "  > Batch 150 \t\t - train_loss: 0.3380 - train_acc: 0.8310\n",
            "  > Batch 200 \t\t - train_loss: 0.3364 - train_acc: 0.8315\n",
            "Ending Epoch 11/100 \t - train_loss: 0.3351 - train_acc: 0.8316 - val_loss: 0.3801 - val_acc: 0.8286\n",
            "Time taken for 1 epoch: 35.969249963760376 secs\n",
            "\n",
            "Starting Epoch 12/100\n",
            "  > Batch 50 \t\t - train_loss: 0.3300 - train_acc: 0.8271\n",
            "  > Batch 100 \t\t - train_loss: 0.3305 - train_acc: 0.8304\n",
            "  > Batch 150 \t\t - train_loss: 0.3301 - train_acc: 0.8320\n",
            "  > Batch 200 \t\t - train_loss: 0.3289 - train_acc: 0.8324\n",
            "Ending Epoch 12/100 \t - train_loss: 0.3277 - train_acc: 0.8326 - val_loss: 0.3783 - val_acc: 0.8288\n",
            "Time taken for 1 epoch: 36.20357537269592 secs\n",
            "\n",
            "Starting Epoch 13/100\n",
            "  > Batch 50 \t\t - train_loss: 0.3220 - train_acc: 0.8279\n",
            "  > Batch 100 \t\t - train_loss: 0.3236 - train_acc: 0.8313\n",
            "  > Batch 150 \t\t - train_loss: 0.3238 - train_acc: 0.8327\n",
            "  > Batch 200 \t\t - train_loss: 0.3224 - train_acc: 0.8333\n",
            "Ending Epoch 13/100 \t - train_loss: 0.3210 - train_acc: 0.8335 - val_loss: 0.3728 - val_acc: 0.8299\n",
            "Time taken for 1 epoch: 36.056915283203125 secs\n",
            "\n",
            "Starting Epoch 14/100\n",
            "  > Batch 50 \t\t - train_loss: 0.3140 - train_acc: 0.8291\n",
            "  > Batch 100 \t\t - train_loss: 0.3151 - train_acc: 0.8323\n",
            "  > Batch 150 \t\t - train_loss: 0.3153 - train_acc: 0.8338\n",
            "  > Batch 200 \t\t - train_loss: 0.3134 - train_acc: 0.8344\n",
            "Ending Epoch 14/100 \t - train_loss: 0.3121 - train_acc: 0.8346 - val_loss: 0.3688 - val_acc: 0.8310\n",
            "Time taken for 1 epoch: 35.902812004089355 secs\n",
            "\n",
            "Starting Epoch 15/100\n",
            "  > Batch 50 \t\t - train_loss: 0.3077 - train_acc: 0.8302\n",
            "  > Batch 100 \t\t - train_loss: 0.3080 - train_acc: 0.8336\n",
            "  > Batch 150 \t\t - train_loss: 0.3081 - train_acc: 0.8350\n",
            "  > Batch 200 \t\t - train_loss: 0.3062 - train_acc: 0.8356\n",
            "Ending Epoch 15/100 \t Saving checkpoint for epoch 15 at ./checkpoints/train/ckpt-7\n",
            "- train_loss: 0.3048 - train_acc: 0.8357 - val_loss: 0.3630 - val_acc: 0.8319\n",
            "Time taken for 1 epoch: 36.1857488155365 secs\n",
            "\n",
            "Starting Epoch 16/100\n",
            "  > Batch 50 \t\t - train_loss: 0.2986 - train_acc: 0.8313\n",
            "  > Batch 100 \t\t - train_loss: 0.2991 - train_acc: 0.8346\n",
            "  > Batch 150 \t\t - train_loss: 0.2996 - train_acc: 0.8360\n",
            "  > Batch 200 \t\t - train_loss: 0.2984 - train_acc: 0.8365\n",
            "Ending Epoch 16/100 \t - train_loss: 0.2971 - train_acc: 0.8367 - val_loss: 0.3613 - val_acc: 0.8323\n",
            "Time taken for 1 epoch: 35.92274856567383 secs\n",
            "\n",
            "Starting Epoch 17/100\n",
            "  > Batch 50 \t\t - train_loss: 0.2920 - train_acc: 0.8321\n",
            "  > Batch 100 \t\t - train_loss: 0.2919 - train_acc: 0.8358\n",
            "  > Batch 150 \t\t - train_loss: 0.2921 - train_acc: 0.8373\n",
            "  > Batch 200 \t\t - train_loss: 0.2910 - train_acc: 0.8377\n",
            "Ending Epoch 17/100 \t - train_loss: 0.2896 - train_acc: 0.8379 - val_loss: 0.3620 - val_acc: 0.8323\n",
            "Time taken for 1 epoch: 35.908183574676514 secs\n",
            "\n",
            "Starting Epoch 18/100\n",
            "  > Batch 50 \t\t - train_loss: 0.2860 - train_acc: 0.8330\n",
            "  > Batch 100 \t\t - train_loss: 0.2858 - train_acc: 0.8365\n",
            "  > Batch 150 \t\t - train_loss: 0.2863 - train_acc: 0.8379\n",
            "  > Batch 200 \t\t - train_loss: 0.2849 - train_acc: 0.8386\n",
            "Ending Epoch 18/100 \t - train_loss: 0.2835 - train_acc: 0.8388 - val_loss: 0.3549 - val_acc: 0.8339\n",
            "Time taken for 1 epoch: 35.93516778945923 secs\n",
            "\n",
            "Starting Epoch 19/100\n",
            "  > Batch 50 \t\t - train_loss: 0.2778 - train_acc: 0.8343\n",
            "  > Batch 100 \t\t - train_loss: 0.2772 - train_acc: 0.8378\n",
            "  > Batch 150 \t\t - train_loss: 0.2777 - train_acc: 0.8393\n",
            "  > Batch 200 \t\t - train_loss: 0.2764 - train_acc: 0.8399\n",
            "Ending Epoch 19/100 \t - train_loss: 0.2753 - train_acc: 0.8400 - val_loss: 0.3520 - val_acc: 0.8345\n",
            "Time taken for 1 epoch: 35.91325616836548 secs\n",
            "\n",
            "Starting Epoch 20/100\n",
            "  > Batch 50 \t\t - train_loss: 0.2718 - train_acc: 0.8354\n",
            "  > Batch 100 \t\t - train_loss: 0.2715 - train_acc: 0.8389\n",
            "  > Batch 150 \t\t - train_loss: 0.2713 - train_acc: 0.8405\n",
            "  > Batch 200 \t\t - train_loss: 0.2699 - train_acc: 0.8410\n",
            "Ending Epoch 20/100 \t Saving checkpoint for epoch 20 at ./checkpoints/train/ckpt-8\n",
            "- train_loss: 0.2688 - train_acc: 0.8411 - val_loss: 0.3505 - val_acc: 0.8349\n",
            "Time taken for 1 epoch: 36.232810258865356 secs\n",
            "\n",
            "Starting Epoch 21/100\n",
            "  > Batch 50 \t\t - train_loss: 0.2658 - train_acc: 0.8356\n",
            "  > Batch 100 \t\t - train_loss: 0.2652 - train_acc: 0.8393\n",
            "  > Batch 150 \t\t - train_loss: 0.2652 - train_acc: 0.8410\n",
            "  > Batch 200 \t\t - train_loss: 0.2637 - train_acc: 0.8416\n",
            "Ending Epoch 21/100 \t - train_loss: 0.2624 - train_acc: 0.8418 - val_loss: 0.3480 - val_acc: 0.8350\n",
            "Time taken for 1 epoch: 35.9606146812439 secs\n",
            "\n",
            "Starting Epoch 22/100\n",
            "  > Batch 50 \t\t - train_loss: 0.2593 - train_acc: 0.8368\n",
            "  > Batch 100 \t\t - train_loss: 0.2591 - train_acc: 0.8404\n",
            "  > Batch 150 \t\t - train_loss: 0.2585 - train_acc: 0.8421\n",
            "  > Batch 200 \t\t - train_loss: 0.2570 - train_acc: 0.8427\n",
            "Ending Epoch 22/100 \t - train_loss: 0.2555 - train_acc: 0.8429 - val_loss: 0.3432 - val_acc: 0.8361\n",
            "Time taken for 1 epoch: 35.812992811203 secs\n",
            "\n",
            "Starting Epoch 23/100\n",
            "  > Batch 50 \t\t - train_loss: 0.2516 - train_acc: 0.8387\n",
            "  > Batch 100 \t\t - train_loss: 0.2510 - train_acc: 0.8419\n",
            "  > Batch 150 \t\t - train_loss: 0.2510 - train_acc: 0.8434\n",
            "  > Batch 200 \t\t - train_loss: 0.2494 - train_acc: 0.8440\n",
            "Ending Epoch 23/100 \t - train_loss: 0.2481 - train_acc: 0.8441 - val_loss: 0.3421 - val_acc: 0.8369\n",
            "Time taken for 1 epoch: 35.7457709312439 secs\n",
            "\n",
            "Starting Epoch 24/100\n",
            "  > Batch 50 \t\t - train_loss: 0.2446 - train_acc: 0.8391\n",
            "  > Batch 100 \t\t - train_loss: 0.2443 - train_acc: 0.8428\n",
            "  > Batch 150 \t\t - train_loss: 0.2444 - train_acc: 0.8445\n",
            "  > Batch 200 \t\t - train_loss: 0.2432 - train_acc: 0.8450\n",
            "Ending Epoch 24/100 \t - train_loss: 0.2418 - train_acc: 0.8452 - val_loss: 0.3399 - val_acc: 0.8375\n",
            "Time taken for 1 epoch: 35.78176712989807 secs\n",
            "\n",
            "Starting Epoch 25/100\n",
            "  > Batch 50 \t\t - train_loss: 0.2370 - train_acc: 0.8408\n",
            "  > Batch 100 \t\t - train_loss: 0.2378 - train_acc: 0.8441\n",
            "  > Batch 150 \t\t - train_loss: 0.2381 - train_acc: 0.8456\n",
            "  > Batch 200 \t\t - train_loss: 0.2367 - train_acc: 0.8461\n",
            "Ending Epoch 25/100 \t Saving checkpoint for epoch 25 at ./checkpoints/train/ckpt-9\n",
            "- train_loss: 0.2355 - train_acc: 0.8463 - val_loss: 0.3389 - val_acc: 0.8382\n",
            "Time taken for 1 epoch: 35.99763107299805 secs\n",
            "\n",
            "Starting Epoch 26/100\n",
            "  > Batch 50 \t\t - train_loss: 0.2323 - train_acc: 0.8414\n",
            "  > Batch 100 \t\t - train_loss: 0.2322 - train_acc: 0.8448\n",
            "  > Batch 150 \t\t - train_loss: 0.2328 - train_acc: 0.8462\n",
            "  > Batch 200 \t\t - train_loss: 0.2321 - train_acc: 0.8468\n",
            "Ending Epoch 26/100 \t - train_loss: 0.2309 - train_acc: 0.8469 - val_loss: 0.3392 - val_acc: 0.8384\n",
            "Time taken for 1 epoch: 35.735220193862915 secs\n",
            "\n",
            "Starting Epoch 27/100\n",
            "  > Batch 50 \t\t - train_loss: 0.2267 - train_acc: 0.8421\n",
            "  > Batch 100 \t\t - train_loss: 0.2261 - train_acc: 0.8458\n",
            "  > Batch 150 \t\t - train_loss: 0.2259 - train_acc: 0.8474\n",
            "  > Batch 200 \t\t - train_loss: 0.2252 - train_acc: 0.8479\n",
            "Ending Epoch 27/100 \t - train_loss: 0.2242 - train_acc: 0.8481 - val_loss: 0.3347 - val_acc: 0.8396\n",
            "Time taken for 1 epoch: 35.72712755203247 secs\n",
            "\n",
            "Starting Epoch 28/100\n",
            "  > Batch 50 \t\t - train_loss: 0.2185 - train_acc: 0.8437\n",
            "  > Batch 100 \t\t - train_loss: 0.2191 - train_acc: 0.8470\n",
            "  > Batch 150 \t\t - train_loss: 0.2195 - train_acc: 0.8485\n",
            "  > Batch 200 \t\t - train_loss: 0.2183 - train_acc: 0.8491\n",
            "Ending Epoch 28/100 \t - train_loss: 0.2172 - train_acc: 0.8492 - val_loss: 0.3328 - val_acc: 0.8399\n",
            "Time taken for 1 epoch: 35.75645613670349 secs\n",
            "\n",
            "Starting Epoch 29/100\n",
            "  > Batch 50 \t\t - train_loss: 0.2142 - train_acc: 0.8444\n",
            "  > Batch 100 \t\t - train_loss: 0.2144 - train_acc: 0.8477\n",
            "  > Batch 150 \t\t - train_loss: 0.2141 - train_acc: 0.8493\n",
            "  > Batch 200 \t\t - train_loss: 0.2134 - train_acc: 0.8497\n",
            "Ending Epoch 29/100 \t - train_loss: 0.2123 - train_acc: 0.8499 - val_loss: 0.3311 - val_acc: 0.8402\n",
            "Time taken for 1 epoch: 35.74796509742737 secs\n",
            "\n",
            "Starting Epoch 30/100\n",
            "  > Batch 50 \t\t - train_loss: 0.2091 - train_acc: 0.8451\n",
            "  > Batch 100 \t\t - train_loss: 0.2081 - train_acc: 0.8488\n",
            "  > Batch 150 \t\t - train_loss: 0.2077 - train_acc: 0.8505\n",
            "  > Batch 200 \t\t - train_loss: 0.2071 - train_acc: 0.8509\n",
            "Ending Epoch 30/100 \t Saving checkpoint for epoch 30 at ./checkpoints/train/ckpt-10\n",
            "- train_loss: 0.2059 - train_acc: 0.8511 - val_loss: 0.3340 - val_acc: 0.8400\n",
            "Time taken for 1 epoch: 36.17871928215027 secs\n",
            "\n",
            "Starting Epoch 31/100\n",
            "  > Batch 50 \t\t - train_loss: 0.2035 - train_acc: 0.8460\n",
            "  > Batch 100 \t\t - train_loss: 0.2031 - train_acc: 0.8497\n",
            "  > Batch 150 \t\t - train_loss: 0.2027 - train_acc: 0.8514\n",
            "  > Batch 200 \t\t - train_loss: 0.2016 - train_acc: 0.8519\n",
            "Ending Epoch 31/100 \t - train_loss: 0.2006 - train_acc: 0.8521 - val_loss: 0.3311 - val_acc: 0.8408\n",
            "Time taken for 1 epoch: 35.76446890830994 secs\n",
            "\n",
            "Starting Epoch 32/100\n",
            "  > Batch 50 \t\t - train_loss: 0.1983 - train_acc: 0.8470\n",
            "  > Batch 100 \t\t - train_loss: 0.1976 - train_acc: 0.8505\n",
            "  > Batch 150 \t\t - train_loss: 0.1971 - train_acc: 0.8522\n",
            "  > Batch 200 \t\t - train_loss: 0.1963 - train_acc: 0.8528\n",
            "Ending Epoch 32/100 \t - train_loss: 0.1952 - train_acc: 0.8530 - val_loss: 0.3283 - val_acc: 0.8419\n",
            "Time taken for 1 epoch: 35.75447177886963 secs\n",
            "\n",
            "Starting Epoch 33/100\n",
            "  > Batch 50 \t\t - train_loss: 0.1907 - train_acc: 0.8482\n",
            "  > Batch 100 \t\t - train_loss: 0.1914 - train_acc: 0.8517\n",
            "  > Batch 150 \t\t - train_loss: 0.1919 - train_acc: 0.8532\n",
            "  > Batch 200 \t\t - train_loss: 0.1913 - train_acc: 0.8537\n",
            "Ending Epoch 33/100 \t - train_loss: 0.1902 - train_acc: 0.8539 - val_loss: 0.3290 - val_acc: 0.8425\n",
            "Time taken for 1 epoch: 35.74541735649109 secs\n",
            "\n",
            "Starting Epoch 34/100\n",
            "  > Batch 50 \t\t - train_loss: 0.1873 - train_acc: 0.8490\n",
            "  > Batch 100 \t\t - train_loss: 0.1874 - train_acc: 0.8526\n",
            "  > Batch 150 \t\t - train_loss: 0.1871 - train_acc: 0.8542\n",
            "  > Batch 200 \t\t - train_loss: 0.1867 - train_acc: 0.8546\n",
            "Ending Epoch 34/100 \t - train_loss: 0.1857 - train_acc: 0.8548 - val_loss: 0.3252 - val_acc: 0.8432\n",
            "Time taken for 1 epoch: 35.734220027923584 secs\n",
            "\n",
            "Starting Epoch 35/100\n",
            "  > Batch 50 \t\t - train_loss: 0.1812 - train_acc: 0.8497\n",
            "  > Batch 100 \t\t - train_loss: 0.1827 - train_acc: 0.8532\n",
            "  > Batch 150 \t\t - train_loss: 0.1818 - train_acc: 0.8550\n",
            "  > Batch 200 \t\t - train_loss: 0.1811 - train_acc: 0.8555\n",
            "Ending Epoch 35/100 \t Saving checkpoint for epoch 35 at ./checkpoints/train/ckpt-11\n",
            "- train_loss: 0.1802 - train_acc: 0.8556 - val_loss: 0.3256 - val_acc: 0.8435\n",
            "Time taken for 1 epoch: 36.0880765914917 secs\n",
            "\n",
            "Starting Epoch 36/100\n",
            "  > Batch 50 \t\t - train_loss: 0.1762 - train_acc: 0.8505\n",
            "  > Batch 100 \t\t - train_loss: 0.1761 - train_acc: 0.8543\n",
            "  > Batch 150 \t\t - train_loss: 0.1760 - train_acc: 0.8560\n",
            "  > Batch 200 \t\t - train_loss: 0.1758 - train_acc: 0.8564\n",
            "Ending Epoch 36/100 \t - train_loss: 0.1749 - train_acc: 0.8566 - val_loss: 0.3247 - val_acc: 0.8441\n",
            "Time taken for 1 epoch: 35.744635343551636 secs\n",
            "\n",
            "Starting Epoch 37/100\n",
            "  > Batch 50 \t\t - train_loss: 0.1711 - train_acc: 0.8516\n",
            "  > Batch 100 \t\t - train_loss: 0.1715 - train_acc: 0.8552\n",
            "  > Batch 150 \t\t - train_loss: 0.1713 - train_acc: 0.8569\n",
            "  > Batch 200 \t\t - train_loss: 0.1709 - train_acc: 0.8573\n",
            "Ending Epoch 37/100 \t - train_loss: 0.1700 - train_acc: 0.8574 - val_loss: 0.3231 - val_acc: 0.8451\n",
            "Time taken for 1 epoch: 35.73521327972412 secs\n",
            "\n",
            "Starting Epoch 38/100\n",
            "  > Batch 50 \t\t - train_loss: 0.1658 - train_acc: 0.8524\n",
            "  > Batch 100 \t\t - train_loss: 0.1662 - train_acc: 0.8562\n",
            "  > Batch 150 \t\t - train_loss: 0.1666 - train_acc: 0.8578\n",
            "  > Batch 200 \t\t - train_loss: 0.1659 - train_acc: 0.8584\n",
            "Ending Epoch 38/100 \t - train_loss: 0.1650 - train_acc: 0.8585 - val_loss: 0.3229 - val_acc: 0.8456\n",
            "Time taken for 1 epoch: 35.69873404502869 secs\n",
            "\n",
            "Starting Epoch 39/100\n",
            "  > Batch 50 \t\t - train_loss: 0.1629 - train_acc: 0.8532\n",
            "  > Batch 100 \t\t - train_loss: 0.1626 - train_acc: 0.8568\n",
            "  > Batch 150 \t\t - train_loss: 0.1626 - train_acc: 0.8585\n",
            "  > Batch 200 \t\t - train_loss: 0.1623 - train_acc: 0.8590\n",
            "Ending Epoch 39/100 \t - train_loss: 0.1616 - train_acc: 0.8591 - val_loss: 0.3205 - val_acc: 0.8456\n",
            "Time taken for 1 epoch: 35.752081632614136 secs\n",
            "\n",
            "Starting Epoch 40/100\n",
            "  > Batch 50 \t\t - train_loss: 0.1588 - train_acc: 0.8544\n",
            "  > Batch 100 \t\t - train_loss: 0.1588 - train_acc: 0.8579\n",
            "  > Batch 150 \t\t - train_loss: 0.1586 - train_acc: 0.8594\n",
            "  > Batch 200 \t\t - train_loss: 0.1578 - train_acc: 0.8599\n",
            "Ending Epoch 40/100 \t Saving checkpoint for epoch 40 at ./checkpoints/train/ckpt-12\n",
            "- train_loss: 0.1570 - train_acc: 0.8600 - val_loss: 0.3199 - val_acc: 0.8466\n",
            "Time taken for 1 epoch: 36.19018816947937 secs\n",
            "\n",
            "Starting Epoch 41/100\n",
            "  > Batch 50 \t\t - train_loss: 0.1559 - train_acc: 0.8544\n",
            "  > Batch 100 \t\t - train_loss: 0.1546 - train_acc: 0.8582\n",
            "  > Batch 150 \t\t - train_loss: 0.1544 - train_acc: 0.8599\n",
            "  > Batch 200 \t\t - train_loss: 0.1541 - train_acc: 0.8605\n",
            "Ending Epoch 41/100 \t - train_loss: 0.1532 - train_acc: 0.8607 - val_loss: 0.3160 - val_acc: 0.8477\n",
            "Time taken for 1 epoch: 35.68000316619873 secs\n",
            "\n",
            "Starting Epoch 42/100\n",
            "  > Batch 50 \t\t - train_loss: 0.1513 - train_acc: 0.8554\n",
            "  > Batch 100 \t\t - train_loss: 0.1510 - train_acc: 0.8592\n",
            "  > Batch 150 \t\t - train_loss: 0.1505 - train_acc: 0.8610\n",
            "  > Batch 200 \t\t - train_loss: 0.1501 - train_acc: 0.8614\n",
            "Ending Epoch 42/100 \t - train_loss: 0.1493 - train_acc: 0.8616 - val_loss: 0.3156 - val_acc: 0.8482\n",
            "Time taken for 1 epoch: 35.6808762550354 secs\n",
            "\n",
            "Starting Epoch 43/100\n",
            "  > Batch 50 \t\t - train_loss: 0.1469 - train_acc: 0.8563\n",
            "  > Batch 100 \t\t - train_loss: 0.1465 - train_acc: 0.8601\n",
            "  > Batch 150 \t\t - train_loss: 0.1465 - train_acc: 0.8620\n",
            "  > Batch 200 \t\t - train_loss: 0.1459 - train_acc: 0.8624\n",
            "Ending Epoch 43/100 \t - train_loss: 0.1452 - train_acc: 0.8626 - val_loss: 0.3128 - val_acc: 0.8489\n",
            "Time taken for 1 epoch: 35.698970317840576 secs\n",
            "\n",
            "Starting Epoch 44/100\n",
            "  > Batch 50 \t\t - train_loss: 0.1417 - train_acc: 0.8572\n",
            "  > Batch 100 \t\t - train_loss: 0.1424 - train_acc: 0.8607\n",
            "  > Batch 150 \t\t - train_loss: 0.1431 - train_acc: 0.8624\n",
            "  > Batch 200 \t\t - train_loss: 0.1424 - train_acc: 0.8629\n",
            "Ending Epoch 44/100 \t - train_loss: 0.1416 - train_acc: 0.8631 - val_loss: 0.3109 - val_acc: 0.8496\n",
            "Time taken for 1 epoch: 35.64546012878418 secs\n",
            "\n",
            "Starting Epoch 45/100\n",
            "  > Batch 50 \t\t - train_loss: 0.1388 - train_acc: 0.8583\n",
            "  > Batch 100 \t\t - train_loss: 0.1380 - train_acc: 0.8621\n",
            "  > Batch 150 \t\t - train_loss: 0.1384 - train_acc: 0.8637\n",
            "  > Batch 200 \t\t - train_loss: 0.1382 - train_acc: 0.8641\n",
            "Ending Epoch 45/100 \t Saving checkpoint for epoch 45 at ./checkpoints/train/ckpt-13\n",
            "- train_loss: 0.1376 - train_acc: 0.8642 - val_loss: 0.3112 - val_acc: 0.8496\n",
            "Time taken for 1 epoch: 35.96836709976196 secs\n",
            "\n",
            "Starting Epoch 46/100\n",
            "  > Batch 50 \t\t - train_loss: 0.1356 - train_acc: 0.8587\n",
            "  > Batch 100 \t\t - train_loss: 0.1349 - train_acc: 0.8626\n",
            "  > Batch 150 \t\t - train_loss: 0.1351 - train_acc: 0.8642\n",
            "  > Batch 200 \t\t - train_loss: 0.1351 - train_acc: 0.8646\n",
            "Ending Epoch 46/100 \t - train_loss: 0.1344 - train_acc: 0.8647 - val_loss: 0.3118 - val_acc: 0.8500\n",
            "Time taken for 1 epoch: 35.70131468772888 secs\n",
            "\n",
            "Starting Epoch 47/100\n",
            "  > Batch 50 \t\t - train_loss: 0.1325 - train_acc: 0.8594\n",
            "  > Batch 100 \t\t - train_loss: 0.1321 - train_acc: 0.8633\n",
            "  > Batch 150 \t\t - train_loss: 0.1319 - train_acc: 0.8651\n",
            "  > Batch 200 \t\t - train_loss: 0.1317 - train_acc: 0.8655\n",
            "Ending Epoch 47/100 \t - train_loss: 0.1312 - train_acc: 0.8656 - val_loss: 0.3103 - val_acc: 0.8507\n",
            "Time taken for 1 epoch: 35.65843224525452 secs\n",
            "\n",
            "Starting Epoch 48/100\n",
            "  > Batch 50 \t\t - train_loss: 0.1309 - train_acc: 0.8597\n",
            "  > Batch 100 \t\t - train_loss: 0.1301 - train_acc: 0.8636\n",
            "  > Batch 150 \t\t - train_loss: 0.1296 - train_acc: 0.8654\n",
            "  > Batch 200 \t\t - train_loss: 0.1293 - train_acc: 0.8659\n",
            "Ending Epoch 48/100 \t - train_loss: 0.1286 - train_acc: 0.8660 - val_loss: 0.3125 - val_acc: 0.8511\n",
            "Time taken for 1 epoch: 35.72755765914917 secs\n",
            "\n",
            "Starting Epoch 49/100\n",
            "  > Batch 50 \t\t - train_loss: 0.1254 - train_acc: 0.8613\n",
            "  > Batch 100 \t\t - train_loss: 0.1250 - train_acc: 0.8649\n",
            "  > Batch 150 \t\t - train_loss: 0.1251 - train_acc: 0.8665\n",
            "  > Batch 200 \t\t - train_loss: 0.1251 - train_acc: 0.8669\n",
            "Ending Epoch 49/100 \t - train_loss: 0.1246 - train_acc: 0.8670 - val_loss: 0.3122 - val_acc: 0.8516\n",
            "Time taken for 1 epoch: 35.67228031158447 secs\n",
            "\n",
            "Starting Epoch 50/100\n",
            "  > Batch 50 \t\t - train_loss: 0.1212 - train_acc: 0.8624\n",
            "  > Batch 100 \t\t - train_loss: 0.1223 - train_acc: 0.8656\n",
            "  > Batch 150 \t\t - train_loss: 0.1224 - train_acc: 0.8672\n",
            "  > Batch 200 \t\t - train_loss: 0.1222 - train_acc: 0.8677\n",
            "Ending Epoch 50/100 \t Saving checkpoint for epoch 50 at ./checkpoints/train/ckpt-14\n",
            "- train_loss: 0.1215 - train_acc: 0.8678 - val_loss: 0.3142 - val_acc: 0.8517\n",
            "Time taken for 1 epoch: 35.96339964866638 secs\n",
            "\n",
            "Starting Epoch 51/100\n",
            "  > Batch 50 \t\t - train_loss: 0.1205 - train_acc: 0.8622\n",
            "  > Batch 100 \t\t - train_loss: 0.1193 - train_acc: 0.8662\n",
            "  > Batch 150 \t\t - train_loss: 0.1194 - train_acc: 0.8679\n",
            "  > Batch 200 \t\t - train_loss: 0.1192 - train_acc: 0.8683\n",
            "Ending Epoch 51/100 \t - train_loss: 0.1187 - train_acc: 0.8684 - val_loss: 0.3099 - val_acc: 0.8526\n",
            "Time taken for 1 epoch: 35.68192958831787 secs\n",
            "\n",
            "Starting Epoch 52/100\n",
            "  > Batch 50 \t\t - train_loss: 0.1182 - train_acc: 0.8630\n",
            "  > Batch 100 \t\t - train_loss: 0.1170 - train_acc: 0.8668\n",
            "  > Batch 150 \t\t - train_loss: 0.1168 - train_acc: 0.8684\n",
            "  > Batch 200 \t\t - train_loss: 0.1164 - train_acc: 0.8689\n",
            "Ending Epoch 52/100 \t - train_loss: 0.1158 - train_acc: 0.8690 - val_loss: 0.3085 - val_acc: 0.8531\n",
            "Time taken for 1 epoch: 35.59685301780701 secs\n",
            "\n",
            "Starting Epoch 53/100\n",
            "  > Batch 50 \t\t - train_loss: 0.1131 - train_acc: 0.8639\n",
            "  > Batch 100 \t\t - train_loss: 0.1136 - train_acc: 0.8675\n",
            "  > Batch 150 \t\t - train_loss: 0.1138 - train_acc: 0.8691\n",
            "  > Batch 200 \t\t - train_loss: 0.1134 - train_acc: 0.8695\n",
            "Ending Epoch 53/100 \t - train_loss: 0.1126 - train_acc: 0.8697 - val_loss: 0.3089 - val_acc: 0.8534\n",
            "Time taken for 1 epoch: 35.6818311214447 secs\n",
            "\n",
            "Starting Epoch 54/100\n",
            "  > Batch 50 \t\t - train_loss: 0.1114 - train_acc: 0.8643\n",
            "  > Batch 100 \t\t - train_loss: 0.1117 - train_acc: 0.8679\n",
            "  > Batch 150 \t\t - train_loss: 0.1112 - train_acc: 0.8697\n",
            "  > Batch 200 \t\t - train_loss: 0.1109 - train_acc: 0.8702\n",
            "Ending Epoch 54/100 \t - train_loss: 0.1103 - train_acc: 0.8703 - val_loss: 0.3079 - val_acc: 0.8536\n",
            "Time taken for 1 epoch: 35.64829969406128 secs\n",
            "\n",
            "Starting Epoch 55/100\n",
            "  > Batch 50 \t\t - train_loss: 0.1084 - train_acc: 0.8651\n",
            "  > Batch 100 \t\t - train_loss: 0.1078 - train_acc: 0.8689\n",
            "  > Batch 150 \t\t - train_loss: 0.1087 - train_acc: 0.8704\n",
            "  > Batch 200 \t\t - train_loss: 0.1084 - train_acc: 0.8709\n",
            "Ending Epoch 55/100 \t Saving checkpoint for epoch 55 at ./checkpoints/train/ckpt-15\n",
            "- train_loss: 0.1078 - train_acc: 0.8710 - val_loss: 0.3081 - val_acc: 0.8543\n",
            "Time taken for 1 epoch: 35.997052907943726 secs\n",
            "\n",
            "Starting Epoch 56/100\n",
            "  > Batch 50 \t\t - train_loss: 0.1046 - train_acc: 0.8659\n",
            "  > Batch 100 \t\t - train_loss: 0.1047 - train_acc: 0.8695\n",
            "  > Batch 150 \t\t - train_loss: 0.1053 - train_acc: 0.8711\n",
            "  > Batch 200 \t\t - train_loss: 0.1053 - train_acc: 0.8714\n",
            "Ending Epoch 56/100 \t - train_loss: 0.1048 - train_acc: 0.8716 - val_loss: 0.3083 - val_acc: 0.8548\n",
            "Time taken for 1 epoch: 35.785409450531006 secs\n",
            "\n",
            "Starting Epoch 57/100\n",
            "  > Batch 50 \t\t - train_loss: 0.1038 - train_acc: 0.8665\n",
            "  > Batch 100 \t\t - train_loss: 0.1041 - train_acc: 0.8699\n",
            "  > Batch 150 \t\t - train_loss: 0.1042 - train_acc: 0.8716\n",
            "  > Batch 200 \t\t - train_loss: 0.1039 - train_acc: 0.8720\n",
            "Ending Epoch 57/100 \t - train_loss: 0.1034 - train_acc: 0.8721 - val_loss: 0.3090 - val_acc: 0.8551\n",
            "Time taken for 1 epoch: 35.67794942855835 secs\n",
            "\n",
            "Starting Epoch 58/100\n",
            "  > Batch 50 \t\t - train_loss: 0.1016 - train_acc: 0.8667\n",
            "  > Batch 100 \t\t - train_loss: 0.1011 - train_acc: 0.8704\n",
            "  > Batch 150 \t\t - train_loss: 0.1013 - train_acc: 0.8720\n",
            "  > Batch 200 \t\t - train_loss: 0.1013 - train_acc: 0.8723\n",
            "Ending Epoch 58/100 \t - train_loss: 0.1007 - train_acc: 0.8725 - val_loss: 0.3134 - val_acc: 0.8544\n",
            "Time taken for 1 epoch: 35.719897747039795 secs\n",
            "\n",
            "Starting Epoch 59/100\n",
            "  > Batch 50 \t\t - train_loss: 0.0986 - train_acc: 0.8673\n",
            "  > Batch 100 \t\t - train_loss: 0.0990 - train_acc: 0.8709\n",
            "  > Batch 150 \t\t - train_loss: 0.0994 - train_acc: 0.8726\n",
            "  > Batch 200 \t\t - train_loss: 0.0990 - train_acc: 0.8730\n",
            "Ending Epoch 59/100 \t - train_loss: 0.0985 - train_acc: 0.8732 - val_loss: 0.3099 - val_acc: 0.8553\n",
            "Time taken for 1 epoch: 35.70602560043335 secs\n",
            "\n",
            "Starting Epoch 60/100\n",
            "  > Batch 50 \t\t - train_loss: 0.0972 - train_acc: 0.8676\n",
            "  > Batch 100 \t\t - train_loss: 0.0970 - train_acc: 0.8714\n",
            "  > Batch 150 \t\t - train_loss: 0.0967 - train_acc: 0.8733\n",
            "  > Batch 200 \t\t - train_loss: 0.0971 - train_acc: 0.8736\n",
            "Ending Epoch 60/100 \t Saving checkpoint for epoch 60 at ./checkpoints/train/ckpt-16\n",
            "- train_loss: 0.0967 - train_acc: 0.8737 - val_loss: 0.3111 - val_acc: 0.8557\n",
            "Time taken for 1 epoch: 36.05048727989197 secs\n",
            "\n",
            "Starting Epoch 61/100\n",
            "  > Batch 50 \t\t - train_loss: 0.0949 - train_acc: 0.8682\n",
            "  > Batch 100 \t\t - train_loss: 0.0955 - train_acc: 0.8718\n",
            "  > Batch 150 \t\t - train_loss: 0.0953 - train_acc: 0.8735\n",
            "  > Batch 200 \t\t - train_loss: 0.0948 - train_acc: 0.8741\n",
            "Ending Epoch 61/100 \t - train_loss: 0.0945 - train_acc: 0.8742 - val_loss: 0.3085 - val_acc: 0.8561\n",
            "Time taken for 1 epoch: 35.654123067855835 secs\n",
            "\n",
            "Starting Epoch 62/100\n",
            "  > Batch 50 \t\t - train_loss: 0.0931 - train_acc: 0.8687\n",
            "  > Batch 100 \t\t - train_loss: 0.0925 - train_acc: 0.8725\n",
            "  > Batch 150 \t\t - train_loss: 0.0927 - train_acc: 0.8743\n",
            "  > Batch 200 \t\t - train_loss: 0.0926 - train_acc: 0.8746\n",
            "Ending Epoch 62/100 \t - train_loss: 0.0919 - train_acc: 0.8748 - val_loss: 0.3089 - val_acc: 0.8565\n",
            "Time taken for 1 epoch: 35.64071583747864 secs\n",
            "\n",
            "Starting Epoch 63/100\n",
            "  > Batch 50 \t\t - train_loss: 0.0902 - train_acc: 0.8693\n",
            "  > Batch 100 \t\t - train_loss: 0.0912 - train_acc: 0.8729\n",
            "  > Batch 150 \t\t - train_loss: 0.0912 - train_acc: 0.8747\n",
            "  > Batch 200 \t\t - train_loss: 0.0909 - train_acc: 0.8751\n",
            "Ending Epoch 63/100 \t - train_loss: 0.0904 - train_acc: 0.8752 - val_loss: 0.3073 - val_acc: 0.8572\n",
            "Time taken for 1 epoch: 35.651779651641846 secs\n",
            "\n",
            "Starting Epoch 64/100\n",
            "  > Batch 50 \t\t - train_loss: 0.0885 - train_acc: 0.8700\n",
            "  > Batch 100 \t\t - train_loss: 0.0893 - train_acc: 0.8734\n",
            "  > Batch 150 \t\t - train_loss: 0.0892 - train_acc: 0.8750\n",
            "  > Batch 200 \t\t - train_loss: 0.0890 - train_acc: 0.8755\n",
            "Ending Epoch 64/100 \t - train_loss: 0.0886 - train_acc: 0.8756 - val_loss: 0.3091 - val_acc: 0.8571\n",
            "Time taken for 1 epoch: 35.59616470336914 secs\n",
            "\n",
            "Starting Epoch 65/100\n",
            "  > Batch 50 \t\t - train_loss: 0.0868 - train_acc: 0.8703\n",
            "  > Batch 100 \t\t - train_loss: 0.0866 - train_acc: 0.8741\n",
            "  > Batch 150 \t\t - train_loss: 0.0876 - train_acc: 0.8756\n",
            "  > Batch 200 \t\t - train_loss: 0.0874 - train_acc: 0.8760\n",
            "Ending Epoch 65/100 \t Saving checkpoint for epoch 65 at ./checkpoints/train/ckpt-17\n",
            "- train_loss: 0.0870 - train_acc: 0.8761 - val_loss: 0.3093 - val_acc: 0.8573\n",
            "Time taken for 1 epoch: 36.054983615875244 secs\n",
            "\n",
            "Starting Epoch 66/100\n",
            "  > Batch 50 \t\t - train_loss: 0.0850 - train_acc: 0.8708\n",
            "  > Batch 100 \t\t - train_loss: 0.0852 - train_acc: 0.8744\n",
            "  > Batch 150 \t\t - train_loss: 0.0859 - train_acc: 0.8760\n",
            "  > Batch 200 \t\t - train_loss: 0.0857 - train_acc: 0.8764\n",
            "Ending Epoch 66/100 \t - train_loss: 0.0854 - train_acc: 0.8766 - val_loss: 0.3123 - val_acc: 0.8576\n",
            "Time taken for 1 epoch: 35.625932693481445 secs\n",
            "\n",
            "Starting Epoch 67/100\n",
            "  > Batch 50 \t\t - train_loss: 0.0828 - train_acc: 0.8716\n",
            "  > Batch 100 \t\t - train_loss: 0.0830 - train_acc: 0.8752\n",
            "  > Batch 150 \t\t - train_loss: 0.0840 - train_acc: 0.8767\n",
            "  > Batch 200 \t\t - train_loss: 0.0837 - train_acc: 0.8771\n",
            "Ending Epoch 67/100 \t - train_loss: 0.0833 - train_acc: 0.8772 - val_loss: 0.3107 - val_acc: 0.8575\n",
            "Time taken for 1 epoch: 35.83174777030945 secs\n",
            "\n",
            "Starting Epoch 68/100\n",
            "  > Batch 50 \t\t - train_loss: 0.0819 - train_acc: 0.8717\n",
            "  > Batch 100 \t\t - train_loss: 0.0819 - train_acc: 0.8754\n",
            "  > Batch 150 \t\t - train_loss: 0.0821 - train_acc: 0.8770\n",
            "  > Batch 200 \t\t - train_loss: 0.0818 - train_acc: 0.8774\n",
            "Ending Epoch 68/100 \t - train_loss: 0.0814 - train_acc: 0.8776 - val_loss: 0.3122 - val_acc: 0.8580\n",
            "Time taken for 1 epoch: 35.76006269454956 secs\n",
            "\n",
            "Starting Epoch 69/100\n",
            "  > Batch 50 \t\t - train_loss: 0.0820 - train_acc: 0.8718\n",
            "  > Batch 100 \t\t - train_loss: 0.0822 - train_acc: 0.8754\n",
            "  > Batch 150 \t\t - train_loss: 0.0823 - train_acc: 0.8771\n",
            "  > Batch 200 \t\t - train_loss: 0.0818 - train_acc: 0.8776\n",
            "Ending Epoch 69/100 \t - train_loss: 0.0813 - train_acc: 0.8778 - val_loss: 0.3105 - val_acc: 0.8584\n",
            "Time taken for 1 epoch: 35.654600381851196 secs\n",
            "\n",
            "Starting Epoch 70/100\n",
            "  > Batch 50 \t\t - train_loss: 0.0803 - train_acc: 0.8720\n",
            "  > Batch 100 \t\t - train_loss: 0.0805 - train_acc: 0.8757\n",
            "  > Batch 150 \t\t - train_loss: 0.0806 - train_acc: 0.8775\n",
            "  > Batch 200 \t\t - train_loss: 0.0803 - train_acc: 0.8779\n",
            "Ending Epoch 70/100 \t Saving checkpoint for epoch 70 at ./checkpoints/train/ckpt-18\n",
            "- train_loss: 0.0798 - train_acc: 0.8780 - val_loss: 0.3092 - val_acc: 0.8586\n",
            "Time taken for 1 epoch: 35.952324867248535 secs\n",
            "\n",
            "Starting Epoch 71/100\n",
            "  > Batch 50 \t\t - train_loss: 0.0783 - train_acc: 0.8727\n",
            "  > Batch 100 \t\t - train_loss: 0.0787 - train_acc: 0.8763\n",
            "  > Batch 150 \t\t - train_loss: 0.0790 - train_acc: 0.8779\n",
            "  > Batch 200 \t\t - train_loss: 0.0786 - train_acc: 0.8784\n",
            "Ending Epoch 71/100 \t - train_loss: 0.0782 - train_acc: 0.8785 - val_loss: 0.3102 - val_acc: 0.8587\n",
            "Time taken for 1 epoch: 35.67149043083191 secs\n",
            "\n",
            "Starting Epoch 72/100\n",
            "  > Batch 50 \t\t - train_loss: 0.0755 - train_acc: 0.8733\n",
            "  > Batch 100 \t\t - train_loss: 0.0765 - train_acc: 0.8768\n",
            "  > Batch 150 \t\t - train_loss: 0.0771 - train_acc: 0.8784\n",
            "  > Batch 200 \t\t - train_loss: 0.0772 - train_acc: 0.8789\n",
            "Ending Epoch 72/100 \t - train_loss: 0.0769 - train_acc: 0.8790 - val_loss: 0.3125 - val_acc: 0.8592\n",
            "Time taken for 1 epoch: 35.65025973320007 secs\n",
            "\n",
            "Starting Epoch 73/100\n",
            "  > Batch 50 \t\t - train_loss: 0.0746 - train_acc: 0.8734\n",
            "  > Batch 100 \t\t - train_loss: 0.0747 - train_acc: 0.8771\n",
            "  > Batch 150 \t\t - train_loss: 0.0752 - train_acc: 0.8787\n",
            "  > Batch 200 \t\t - train_loss: 0.0751 - train_acc: 0.8791\n",
            "Ending Epoch 73/100 \t - train_loss: 0.0747 - train_acc: 0.8793 - val_loss: 0.3129 - val_acc: 0.8592\n",
            "Time taken for 1 epoch: 35.658944606781006 secs\n",
            "\n",
            "Starting Epoch 74/100\n",
            "  > Batch 50 \t\t - train_loss: 0.0727 - train_acc: 0.8741\n",
            "  > Batch 100 \t\t - train_loss: 0.0732 - train_acc: 0.8775\n",
            "  > Batch 150 \t\t - train_loss: 0.0738 - train_acc: 0.8792\n",
            "  > Batch 200 \t\t - train_loss: 0.0738 - train_acc: 0.8796\n",
            "Ending Epoch 74/100 \t - train_loss: 0.0735 - train_acc: 0.8796 - val_loss: 0.3090 - val_acc: 0.8600\n",
            "Time taken for 1 epoch: 35.77017855644226 secs\n",
            "\n",
            "Starting Epoch 75/100\n",
            "  > Batch 50 \t\t - train_loss: 0.0715 - train_acc: 0.8744\n",
            "  > Batch 100 \t\t - train_loss: 0.0720 - train_acc: 0.8780\n",
            "  > Batch 150 \t\t - train_loss: 0.0726 - train_acc: 0.8797\n",
            "  > Batch 200 \t\t - train_loss: 0.0727 - train_acc: 0.8800\n",
            "Ending Epoch 75/100 \t Saving checkpoint for epoch 75 at ./checkpoints/train/ckpt-19\n",
            "- train_loss: 0.0724 - train_acc: 0.8801 - val_loss: 0.3124 - val_acc: 0.8593\n",
            "Time taken for 1 epoch: 35.945040702819824 secs\n",
            "\n",
            "Starting Epoch 76/100\n",
            "  > Batch 50 \t\t - train_loss: 0.0716 - train_acc: 0.8744\n",
            "  > Batch 100 \t\t - train_loss: 0.0715 - train_acc: 0.8782\n",
            "  > Batch 150 \t\t - train_loss: 0.0717 - train_acc: 0.8799\n",
            "  > Batch 200 \t\t - train_loss: 0.0716 - train_acc: 0.8804\n",
            "Ending Epoch 76/100 \t - train_loss: 0.0713 - train_acc: 0.8805 - val_loss: 0.3123 - val_acc: 0.8603\n",
            "Time taken for 1 epoch: 35.69775938987732 secs\n",
            "\n",
            "Starting Epoch 77/100\n",
            "  > Batch 50 \t\t - train_loss: 0.0698 - train_acc: 0.8749\n",
            "  > Batch 100 \t\t - train_loss: 0.0705 - train_acc: 0.8784\n",
            "  > Batch 150 \t\t - train_loss: 0.0704 - train_acc: 0.8802\n",
            "  > Batch 200 \t\t - train_loss: 0.0703 - train_acc: 0.8806\n",
            "Ending Epoch 77/100 \t - train_loss: 0.0699 - train_acc: 0.8807 - val_loss: 0.3107 - val_acc: 0.8605\n",
            "Time taken for 1 epoch: 35.64293050765991 secs\n",
            "\n",
            "Starting Epoch 78/100\n",
            "  > Batch 50 \t\t - train_loss: 0.0672 - train_acc: 0.8757\n",
            "  > Batch 100 \t\t - train_loss: 0.0682 - train_acc: 0.8791\n",
            "  > Batch 150 \t\t - train_loss: 0.0687 - train_acc: 0.8807\n",
            "  > Batch 200 \t\t - train_loss: 0.0688 - train_acc: 0.8810\n",
            "Ending Epoch 78/100 \t - train_loss: 0.0684 - train_acc: 0.8812 - val_loss: 0.3127 - val_acc: 0.8607\n",
            "Time taken for 1 epoch: 35.65719962120056 secs\n",
            "\n",
            "Starting Epoch 79/100\n",
            "  > Batch 50 \t\t - train_loss: 0.0680 - train_acc: 0.8755\n",
            "  > Batch 100 \t\t - train_loss: 0.0679 - train_acc: 0.8792\n",
            "  > Batch 150 \t\t - train_loss: 0.0685 - train_acc: 0.8808\n",
            "  > Batch 200 \t\t - train_loss: 0.0682 - train_acc: 0.8812\n",
            "Ending Epoch 79/100 \t - train_loss: 0.0678 - train_acc: 0.8813 - val_loss: 0.3143 - val_acc: 0.8609\n",
            "Time taken for 1 epoch: 35.62097716331482 secs\n",
            "\n",
            "Starting Epoch 80/100\n",
            "  > Batch 50 \t\t - train_loss: 0.0653 - train_acc: 0.8761\n",
            "  > Batch 100 \t\t - train_loss: 0.0657 - train_acc: 0.8797\n",
            "  > Batch 150 \t\t - train_loss: 0.0667 - train_acc: 0.8812\n",
            "  > Batch 200 \t\t - train_loss: 0.0671 - train_acc: 0.8815\n",
            "Ending Epoch 80/100 \t Saving checkpoint for epoch 80 at ./checkpoints/train/ckpt-20\n",
            "- train_loss: 0.0668 - train_acc: 0.8816 - val_loss: 0.3139 - val_acc: 0.8610\n",
            "Time taken for 1 epoch: 35.9643075466156 secs\n",
            "\n",
            "Starting Epoch 81/100\n",
            "  > Batch 50 \t\t - train_loss: 0.0654 - train_acc: 0.8763\n",
            "  > Batch 100 \t\t - train_loss: 0.0649 - train_acc: 0.8801\n",
            "  > Batch 150 \t\t - train_loss: 0.0655 - train_acc: 0.8817\n",
            "  > Batch 200 \t\t - train_loss: 0.0653 - train_acc: 0.8821\n",
            "Ending Epoch 81/100 \t - train_loss: 0.0650 - train_acc: 0.8822 - val_loss: 0.3133 - val_acc: 0.8610\n",
            "Time taken for 1 epoch: 35.64766335487366 secs\n",
            "\n",
            "Starting Epoch 82/100\n",
            "  > Batch 50 \t\t - train_loss: 0.0631 - train_acc: 0.8765\n",
            "  > Batch 100 \t\t - train_loss: 0.0642 - train_acc: 0.8800\n",
            "  > Batch 150 \t\t - train_loss: 0.0643 - train_acc: 0.8818\n",
            "  > Batch 200 \t\t - train_loss: 0.0644 - train_acc: 0.8822\n",
            "Ending Epoch 82/100 \t - train_loss: 0.0641 - train_acc: 0.8823 - val_loss: 0.3127 - val_acc: 0.8613\n",
            "Time taken for 1 epoch: 35.68485379219055 secs\n",
            "\n",
            "Starting Epoch 83/100\n",
            "  > Batch 50 \t\t - train_loss: 0.0621 - train_acc: 0.8771\n",
            "  > Batch 100 \t\t - train_loss: 0.0625 - train_acc: 0.8806\n",
            "  > Batch 150 \t\t - train_loss: 0.0629 - train_acc: 0.8823\n",
            "  > Batch 200 \t\t - train_loss: 0.0635 - train_acc: 0.8825\n",
            "Ending Epoch 83/100 \t - train_loss: 0.0632 - train_acc: 0.8826 - val_loss: 0.3140 - val_acc: 0.8615\n",
            "Time taken for 1 epoch: 35.64443898200989 secs\n",
            "\n",
            "Starting Epoch 84/100\n",
            "  > Batch 50 \t\t - train_loss: 0.0624 - train_acc: 0.8771\n",
            "  > Batch 100 \t\t - train_loss: 0.0622 - train_acc: 0.8809\n",
            "  > Batch 150 \t\t - train_loss: 0.0620 - train_acc: 0.8825\n",
            "  > Batch 200 \t\t - train_loss: 0.0624 - train_acc: 0.8828\n",
            "Ending Epoch 84/100 \t - train_loss: 0.0622 - train_acc: 0.8829 - val_loss: 0.3156 - val_acc: 0.8616\n",
            "Time taken for 1 epoch: 35.62784695625305 secs\n",
            "\n",
            "Starting Epoch 85/100\n",
            "  > Batch 50 \t\t - train_loss: 0.0619 - train_acc: 0.8771\n",
            "  > Batch 100 \t\t - train_loss: 0.0622 - train_acc: 0.8807\n",
            "  > Batch 150 \t\t - train_loss: 0.0624 - train_acc: 0.8824\n",
            "  > Batch 200 \t\t - train_loss: 0.0626 - train_acc: 0.8827\n",
            "Ending Epoch 85/100 \t Saving checkpoint for epoch 85 at ./checkpoints/train/ckpt-21\n",
            "- train_loss: 0.0623 - train_acc: 0.8828 - val_loss: 0.3130 - val_acc: 0.8620\n",
            "Time taken for 1 epoch: 35.99485635757446 secs\n",
            "\n",
            "Starting Epoch 86/100\n",
            "  > Batch 50 \t\t - train_loss: 0.0607 - train_acc: 0.8773\n",
            "  > Batch 100 \t\t - train_loss: 0.0605 - train_acc: 0.8811\n",
            "  > Batch 150 \t\t - train_loss: 0.0605 - train_acc: 0.8829\n",
            "  > Batch 200 \t\t - train_loss: 0.0606 - train_acc: 0.8832\n",
            "Ending Epoch 86/100 \t - train_loss: 0.0603 - train_acc: 0.8833 - val_loss: 0.3150 - val_acc: 0.8624\n",
            "Time taken for 1 epoch: 35.66675639152527 secs\n",
            "\n",
            "Starting Epoch 87/100\n",
            "  > Batch 50 \t\t - train_loss: 0.0592 - train_acc: 0.8779\n",
            "  > Batch 100 \t\t - train_loss: 0.0600 - train_acc: 0.8813\n",
            "  > Batch 150 \t\t - train_loss: 0.0600 - train_acc: 0.8830\n",
            "  > Batch 200 \t\t - train_loss: 0.0599 - train_acc: 0.8835\n",
            "Ending Epoch 87/100 \t - train_loss: 0.0597 - train_acc: 0.8836 - val_loss: 0.3147 - val_acc: 0.8622\n",
            "Time taken for 1 epoch: 35.65254235267639 secs\n",
            "\n",
            "Starting Epoch 88/100\n",
            "  > Batch 50 \t\t - train_loss: 0.0588 - train_acc: 0.8779\n",
            "  > Batch 100 \t\t - train_loss: 0.0591 - train_acc: 0.8815\n",
            "  > Batch 150 \t\t - train_loss: 0.0593 - train_acc: 0.8831\n",
            "  > Batch 200 \t\t - train_loss: 0.0591 - train_acc: 0.8836\n",
            "Ending Epoch 88/100 \t - train_loss: 0.0588 - train_acc: 0.8837 - val_loss: 0.3169 - val_acc: 0.8621\n",
            "Time taken for 1 epoch: 35.65522813796997 secs\n",
            "\n",
            "Starting Epoch 89/100\n",
            "  > Batch 50 \t\t - train_loss: 0.0578 - train_acc: 0.8781\n",
            "  > Batch 100 \t\t - train_loss: 0.0581 - train_acc: 0.8818\n",
            "  > Batch 150 \t\t - train_loss: 0.0583 - train_acc: 0.8836\n",
            "  > Batch 200 \t\t - train_loss: 0.0585 - train_acc: 0.8839\n",
            "Ending Epoch 89/100 \t - train_loss: 0.0582 - train_acc: 0.8840 - val_loss: 0.3168 - val_acc: 0.8623\n",
            "Time taken for 1 epoch: 35.695671796798706 secs\n",
            "\n",
            "Starting Epoch 90/100\n",
            "  > Batch 50 \t\t - train_loss: 0.0558 - train_acc: 0.8787\n",
            "  > Batch 100 \t\t - train_loss: 0.0565 - train_acc: 0.8822\n",
            "  > Batch 150 \t\t - train_loss: 0.0573 - train_acc: 0.8837\n",
            "  > Batch 200 \t\t - train_loss: 0.0572 - train_acc: 0.8841\n",
            "Ending Epoch 90/100 \t Saving checkpoint for epoch 90 at ./checkpoints/train/ckpt-22\n",
            "- train_loss: 0.0571 - train_acc: 0.8842 - val_loss: 0.3166 - val_acc: 0.8621\n",
            "Time taken for 1 epoch: 35.98992419242859 secs\n",
            "\n",
            "Starting Epoch 91/100\n",
            "  > Batch 50 \t\t - train_loss: 0.0572 - train_acc: 0.8783\n",
            "  > Batch 100 \t\t - train_loss: 0.0571 - train_acc: 0.8821\n",
            "  > Batch 150 \t\t - train_loss: 0.0574 - train_acc: 0.8838\n",
            "  > Batch 200 \t\t - train_loss: 0.0572 - train_acc: 0.8842\n",
            "Ending Epoch 91/100 \t - train_loss: 0.0570 - train_acc: 0.8843 - val_loss: 0.3156 - val_acc: 0.8628\n",
            "Time taken for 1 epoch: 35.778550148010254 secs\n",
            "\n",
            "Starting Epoch 92/100\n",
            "  > Batch 50 \t\t - train_loss: 0.0567 - train_acc: 0.8786\n",
            "  > Batch 100 \t\t - train_loss: 0.0559 - train_acc: 0.8825\n",
            "  > Batch 150 \t\t - train_loss: 0.0557 - train_acc: 0.8843\n",
            "  > Batch 200 \t\t - train_loss: 0.0556 - train_acc: 0.8847\n",
            "Ending Epoch 92/100 \t - train_loss: 0.0554 - train_acc: 0.8848 - val_loss: 0.3156 - val_acc: 0.8625\n",
            "Time taken for 1 epoch: 35.68380117416382 secs\n",
            "\n",
            "Starting Epoch 93/100\n",
            "  > Batch 50 \t\t - train_loss: 0.0544 - train_acc: 0.8792\n",
            "  > Batch 100 \t\t - train_loss: 0.0550 - train_acc: 0.8827\n",
            "  > Batch 150 \t\t - train_loss: 0.0554 - train_acc: 0.8843\n",
            "  > Batch 200 \t\t - train_loss: 0.0554 - train_acc: 0.8847\n",
            "Ending Epoch 93/100 \t - train_loss: 0.0553 - train_acc: 0.8847 - val_loss: 0.3152 - val_acc: 0.8631\n",
            "Time taken for 1 epoch: 35.60875058174133 secs\n",
            "\n",
            "Starting Epoch 94/100\n",
            "  > Batch 50 \t\t - train_loss: 0.0555 - train_acc: 0.8789\n",
            "  > Batch 100 \t\t - train_loss: 0.0554 - train_acc: 0.8826\n",
            "  > Batch 150 \t\t - train_loss: 0.0554 - train_acc: 0.8843\n",
            "  > Batch 200 \t\t - train_loss: 0.0552 - train_acc: 0.8848\n",
            "Ending Epoch 94/100 \t - train_loss: 0.0548 - train_acc: 0.8849 - val_loss: 0.3169 - val_acc: 0.8627\n",
            "Time taken for 1 epoch: 35.631863594055176 secs\n",
            "\n",
            "Starting Epoch 95/100\n",
            "  > Batch 50 \t\t - train_loss: 0.0532 - train_acc: 0.8794\n",
            "  > Batch 100 \t\t - train_loss: 0.0537 - train_acc: 0.8831\n",
            "  > Batch 150 \t\t - train_loss: 0.0542 - train_acc: 0.8848\n",
            "  > Batch 200 \t\t - train_loss: 0.0539 - train_acc: 0.8852\n",
            "Ending Epoch 95/100 \t Saving checkpoint for epoch 95 at ./checkpoints/train/ckpt-23\n",
            "- train_loss: 0.0536 - train_acc: 0.8854 - val_loss: 0.3174 - val_acc: 0.8629\n",
            "Time taken for 1 epoch: 35.92965579032898 secs\n",
            "\n",
            "Starting Epoch 96/100\n",
            "  > Batch 50 \t\t - train_loss: 0.0513 - train_acc: 0.8800\n",
            "  > Batch 100 \t\t - train_loss: 0.0519 - train_acc: 0.8835\n",
            "  > Batch 150 \t\t - train_loss: 0.0527 - train_acc: 0.8851\n",
            "  > Batch 200 \t\t - train_loss: 0.0525 - train_acc: 0.8855\n",
            "Ending Epoch 96/100 \t - train_loss: 0.0523 - train_acc: 0.8856 - val_loss: 0.3181 - val_acc: 0.8636\n",
            "Time taken for 1 epoch: 35.62456703186035 secs\n",
            "\n",
            "Starting Epoch 97/100\n",
            "  > Batch 50 \t\t - train_loss: 0.0526 - train_acc: 0.8796\n",
            "  > Batch 100 \t\t - train_loss: 0.0526 - train_acc: 0.8834\n",
            "  > Batch 150 \t\t - train_loss: 0.0527 - train_acc: 0.8851\n",
            "  > Batch 200 \t\t - train_loss: 0.0526 - train_acc: 0.8856\n",
            "Ending Epoch 97/100 \t - train_loss: 0.0526 - train_acc: 0.8856 - val_loss: 0.3168 - val_acc: 0.8629\n",
            "Time taken for 1 epoch: 35.575995683670044 secs\n",
            "\n",
            "Starting Epoch 98/100\n",
            "  > Batch 50 \t\t - train_loss: 0.0508 - train_acc: 0.8798\n",
            "  > Batch 100 \t\t - train_loss: 0.0511 - train_acc: 0.8836\n",
            "  > Batch 150 \t\t - train_loss: 0.0515 - train_acc: 0.8852\n",
            "  > Batch 200 \t\t - train_loss: 0.0514 - train_acc: 0.8857\n",
            "Ending Epoch 98/100 \t - train_loss: 0.0512 - train_acc: 0.8858 - val_loss: 0.3158 - val_acc: 0.8635\n",
            "Time taken for 1 epoch: 35.60836720466614 secs\n",
            "\n",
            "Starting Epoch 99/100\n",
            "  > Batch 50 \t\t - train_loss: 0.0506 - train_acc: 0.8799\n",
            "  > Batch 100 \t\t - train_loss: 0.0505 - train_acc: 0.8838\n",
            "  > Batch 150 \t\t - train_loss: 0.0505 - train_acc: 0.8857\n",
            "  > Batch 200 \t\t - train_loss: 0.0504 - train_acc: 0.8861\n",
            "Ending Epoch 99/100 \t - train_loss: 0.0502 - train_acc: 0.8862 - val_loss: 0.3204 - val_acc: 0.8636\n",
            "Time taken for 1 epoch: 35.648024797439575 secs\n",
            "\n",
            "Starting Epoch 100/100\n",
            "  > Batch 50 \t\t - train_loss: 0.0493 - train_acc: 0.8804\n",
            "  > Batch 100 \t\t - train_loss: 0.0497 - train_acc: 0.8842\n",
            "  > Batch 150 \t\t - train_loss: 0.0500 - train_acc: 0.8858\n",
            "  > Batch 200 \t\t - train_loss: 0.0503 - train_acc: 0.8861\n",
            "Ending Epoch 100/100 \t Saving checkpoint for epoch 100 at ./checkpoints/train/ckpt-24\n",
            "- train_loss: 0.0501 - train_acc: 0.8862 - val_loss: 0.3173 - val_acc: 0.8641\n",
            "Time taken for 1 epoch: 36.015398263931274 secs\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ho2IsCzZeUdJ"
      },
      "source": [
        "from tensorflow.nn import softmax\r\n",
        "\r\n",
        "newline_token = tokenizer.encode('\\n')[0]\r\n",
        "\r\n",
        "def evaluate(inp_list, max_length=35, temperature_factor=1, verbose=False):\r\n",
        "  # the input is the tokenized string obtained from the given input list\r\n",
        "  input = tokenizer.encode('\\n'.join(inp_list))\r\n",
        "\r\n",
        "  # the encoder input is surrounded by a start and an end token\r\n",
        "  encoder_input = tf.expand_dims([tokenizer.vocab_size] + input + [tokenizer.vocab_size + 1], 0)\r\n",
        "\r\n",
        "  # the decoder input is the same sentence preceded by a start token\r\n",
        "  decoder_input = tf.expand_dims([tokenizer.vocab_size] + input, 0)\r\n",
        "\r\n",
        "  # the final output of the evaluation (initially, this is an empty list)\r\n",
        "  output = []\r\n",
        "\r\n",
        "  # we repeat the process to get the entire verse (until the end token or the newline token is predicted)\r\n",
        "  for i in range(max_length):\r\n",
        "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(encoder_input, decoder_input)  \r\n",
        "    logits, attention_weights = transformer(\r\n",
        "        encoder_input, decoder_input, False,\r\n",
        "        enc_padding_mask, combined_mask, dec_padding_mask\r\n",
        "    )\r\n",
        "\r\n",
        "    # we get the probabilities for the decoded token (the last one)\r\n",
        "    probabilities = softmax(logits[0, -1, :tokenizer.vocab_size]).numpy()\r\n",
        "\r\n",
        "    # we take a subset of possible tokens whose probability is at least 1/temperature_factor of the maximal one\r\n",
        "    indices = np.arange(tokenizer.vocab_size)[probabilities >= probabilities.max() / temperature_factor]\r\n",
        "\r\n",
        "    # we renormalize this subset using, again, a softmax activation\r\n",
        "    probabilities = softmax(probabilities[probabilities >= probabilities.max() / temperature_factor]).numpy()\r\n",
        "    \r\n",
        "    # the id is randomly chosen among the indices according to the computed probabilities\r\n",
        "    predicted_id = np.random.choice(indices, size=1, p=probabilities)[0]\r\n",
        "    \r\n",
        "    # if the token coincides with the nd token or the newline token, the generation is interrupted\r\n",
        "    if predicted_id == newline_token or predicted_id >= tokenizer.vocab_size:\r\n",
        "      break\r\n",
        "\r\n",
        "    # otherwise the token is appended both to the new decoder input and to the final output\r\n",
        "    decoder_input = tf.concat([decoder_input, [[predicted_id]]], axis=-1)\r\n",
        "    output.append(predicted_id)\r\n",
        "\r\n",
        "    if verbose:\r\n",
        "      print(tokenizer.decode([predicted_id]), end='')\r\n",
        "\r\n",
        "  return output, attention_weights"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v966QeR8Z6CB"
      },
      "source": [
        "string = \"-start-non vi dispiaccia se vi lece dirci-end-\\n-start-sa la man destra giace alcuna foce-end-\\n\"\r\n",
        "\r\n",
        "def generate(\r\n",
        "    input_string=string, # first three tercets of the comedy\r\n",
        "    max_iterations=400,\r\n",
        "    temperature_factor=1.0\r\n",
        "):\r\n",
        "  # at the beginning, the generated string is the encoding of the input string (plus a newline character)\r\n",
        "  generated_string = input_string\r\n",
        "\r\n",
        "  for i in range(max_iterations):\r\n",
        "    # the input list is made up of the last 'seq_length' verses (-1 for the last blank verse to be filled)\r\n",
        "    input_list = generated_string.split('\\n')[-seq_length-1:]\r\n",
        "\r\n",
        "    # the generated verse is then decoded\r\n",
        "    target, _ = evaluate(input_list)\r\n",
        "    generated_verse = tokenizer.decode(target)\r\n",
        "\r\n",
        "    generated_string += generated_verse + '\\n'\r\n",
        "  \r\n",
        "  # we finally return the decoded (and unmarked) string, excluding the input provided by the user\r\n",
        "  return generated_string[len(input_string):]"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1w3vpSsedLem"
      },
      "source": [
        "generated_canto = generate(temperature_factor=0.75)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q89qQR0ar2xK"
      },
      "source": [
        "generated_canto = re.sub(\r\n",
        "  '-start-=end_terzine=-end-',\r\n",
        "  '',\r\n",
        "  generated_canto\r\n",
        ")\r\n",
        "\r\n",
        "generated_canto = re.sub(\r\n",
        "  '-start-',\r\n",
        "  \"\",\r\n",
        "  generated_canto\r\n",
        ")\r\n",
        "\r\n",
        "generated_canto = re.sub(\r\n",
        "  '-end-',\r\n",
        "  \"\",\r\n",
        "  generated_canto\r\n",
        ")   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIldtg--d79e"
      },
      "source": [
        "print(generated_canto)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hE8Ny4jMg7ri"
      },
      "source": [
        "from metrics import eval\r\n",
        "\r\n",
        "eval(generated_canto, divine_comedy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJpLjt09hFXS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d08a98f8-6e94-4f59-dbd1-0db741fb9a61"
      },
      "source": [
        "def ngrams_plagiarism(generated_text, original_text, n=4):\r\n",
        "    # the tokenizer is used to remove non-alphanumeric symbols\r\n",
        "    tokenizer = tfds.deprecated.text.Tokenizer()\r\n",
        "    original_text = tokenizer.join(tokenizer.tokenize(original_text.lower()))\r\n",
        "    generated_text_tokens = tokenizer.tokenize(generated_text.lower())\r\n",
        " \r\n",
        "    total_ngrams = len(generated_text_tokens) - n + 1\r\n",
        "    plagiarism_counter = 0\r\n",
        " \r\n",
        "    for i in range(total_ngrams):\r\n",
        "        ngram = tokenizer.join(generated_text_tokens[i:i+n])\r\n",
        "        plagiarism_counter += 1 if ngram in original_text else 0\r\n",
        "    return 1 - (plagiarism_counter / total_ngrams)\r\n",
        " \r\n",
        "ngrams_plagiarism(generated_canto, divine_comedy)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9953445065176909"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKyxLKEXFmc2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}